2023-05-21 18:41:47 [INFO] Experiment for mlp test begins at 2023/05/21 18:41:47
2023-05-21 18:41:47 [INFO] Config file contents:
2023-05-21 18:41:47 [INFO] name: mlp test
2023-05-21 18:41:47 [INFO] activation: relu
2023-05-21 18:41:47 [INFO] activation_last_layer: relu
2023-05-21 18:41:47 [INFO] loss_function: mse
2023-05-21 18:41:47 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 18:41:47 [INFO] metric: mae
2023-05-21 18:41:47 [INFO] shuffle: True
2023-05-21 18:41:47 [INFO] epochs: 100
2023-05-21 18:41:47 [INFO] batch_size: 512
2023-05-21 18:41:47 [INFO] verbose: 2
2023-05-21 18:41:47 [INFO] TensorBoard_log_path: logs
2023-05-21 18:41:47 [INFO] TensorBoard_hist_freq: 1
2023-05-21 18:41:47 [INFO] EarlyStopping_monitor: loss
2023-05-21 18:41:47 [INFO] EarlyStopping_patience: 15
2023-05-21 18:41:47 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 18:41:47 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 18:41:47 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 18:41:47 [INFO] log_path: logs/training.log
2023-05-21 18:41:47 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 18:41:47 [INFO] Training in progress
2023-05-21 18:45:00 [INFO] Experiment for mlp test begins at 2023/05/21 18:45:00
2023-05-21 18:45:00 [INFO] Config file contents:
2023-05-21 18:45:00 [INFO] name: mlp test
2023-05-21 18:45:00 [INFO] activation: relu
2023-05-21 18:45:00 [INFO] activation_last_layer: relu
2023-05-21 18:45:00 [INFO] loss_function: mse
2023-05-21 18:45:00 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 18:45:00 [INFO] metric: mae
2023-05-21 18:45:00 [INFO] shuffle: True
2023-05-21 18:45:00 [INFO] epochs: 100
2023-05-21 18:45:00 [INFO] batch_size: 512
2023-05-21 18:45:00 [INFO] verbose: 2
2023-05-21 18:45:00 [INFO] TensorBoard_log_path: logs
2023-05-21 18:45:00 [INFO] TensorBoard_hist_freq: 1
2023-05-21 18:45:00 [INFO] EarlyStopping_monitor: loss
2023-05-21 18:45:00 [INFO] EarlyStopping_patience: 15
2023-05-21 18:45:00 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 18:45:00 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 18:45:00 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 18:45:00 [INFO] log_path: logs/training.log
2023-05-21 18:45:00 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 18:45:01 [INFO] Training in progress
2023-05-21 18:45:01 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 18:45:02 [DEBUG] Epoch 0 - {'loss': '8457864871936.000000', 'mae': '2325366.000000', 'lr': '0.001000'} 
2023-05-21 18:45:04 [DEBUG] Epoch 1 - {'loss': '8218217021440.000000', 'mae': '2306628.000000', 'lr': '0.001000'} 
2023-05-21 18:45:05 [DEBUG] Epoch 2 - {'loss': '7471596830720.000000', 'mae': '2134575.250000', 'lr': '0.001000'} 
2023-05-21 18:45:06 [DEBUG] Epoch 3 - {'loss': '207044493312.000000', 'mae': '327623.718750', 'lr': '0.001000'} 
2023-05-21 18:45:07 [DEBUG] Epoch 4 - {'loss': '206270087168.000000', 'mae': '326914.968750', 'lr': '0.001000'} 
2023-05-21 18:45:08 [DEBUG] Epoch 5 - {'loss': '206140948480.000000', 'mae': '326850.125000', 'lr': '0.001000'} 
2023-05-21 18:45:09 [DEBUG] Epoch 6 - {'loss': '206147518464.000000', 'mae': '326916.750000', 'lr': '0.001000'} 
2023-05-21 18:45:10 [DEBUG] Epoch 7 - {'loss': '206145683456.000000', 'mae': '326951.843750', 'lr': '0.001000'} 
2023-05-21 18:45:12 [DEBUG] Epoch 8 - {'loss': '206070448128.000000', 'mae': '326967.343750', 'lr': '0.001000'} 
2023-05-21 18:45:13 [DEBUG] Epoch 9 - {'loss': '206134804480.000000', 'mae': '326936.906250', 'lr': '0.001000'} 
2023-05-21 18:45:14 [DEBUG] Epoch 10 - {'loss': '206095056896.000000', 'mae': '326956.093750', 'lr': '0.001000'} 
2023-05-21 18:45:15 [DEBUG] Epoch 11 - {'loss': '206083211264.000000', 'mae': '326976.906250', 'lr': '0.001000'} 
2023-05-21 18:45:16 [DEBUG] Epoch 12 - {'loss': '205947289600.000000', 'mae': '326886.437500', 'lr': '0.000500'} 
2023-05-21 18:45:17 [DEBUG] Epoch 13 - {'loss': '205948239872.000000', 'mae': '326907.656250', 'lr': '0.000500'} 
2023-05-21 18:45:18 [DEBUG] Epoch 14 - {'loss': '206067728384.000000', 'mae': '326955.687500', 'lr': '0.000500'} 
2023-05-21 18:45:19 [DEBUG] Epoch 15 - {'loss': '206173274112.000000', 'mae': '327060.375000', 'lr': '0.000500'} 
2023-05-21 18:45:20 [DEBUG] Epoch 16 - {'loss': '205974945792.000000', 'mae': '326889.687500', 'lr': '0.000250'} 
2023-05-21 18:45:22 [DEBUG] Epoch 17 - {'loss': '205913735168.000000', 'mae': '326803.250000', 'lr': '0.000250'} 
2023-05-21 18:45:23 [DEBUG] Epoch 18 - {'loss': '206014627840.000000', 'mae': '326931.031250', 'lr': '0.000250'} 
2023-05-21 18:45:24 [DEBUG] Epoch 19 - {'loss': '205920550912.000000', 'mae': '326937.656250', 'lr': '0.000250'} 
2023-05-21 18:45:25 [DEBUG] Epoch 20 - {'loss': '205956186112.000000', 'mae': '326855.375000', 'lr': '0.000250'} 
2023-05-21 18:45:26 [DEBUG] Epoch 21 - {'loss': '205999013888.000000', 'mae': '326900.593750', 'lr': '0.000125'} 
2023-05-21 18:45:27 [DEBUG] Epoch 22 - {'loss': '205949124608.000000', 'mae': '326922.718750', 'lr': '0.000125'} 
2023-05-21 18:45:28 [DEBUG] Epoch 23 - {'loss': '205915848704.000000', 'mae': '326885.281250', 'lr': '0.000125'} 
2023-05-21 18:45:29 [DEBUG] Epoch 24 - {'loss': '205997948928.000000', 'mae': '326897.531250', 'lr': '0.000063'} 
2023-05-21 18:45:30 [DEBUG] Epoch 25 - {'loss': '206039433216.000000', 'mae': '326966.156250', 'lr': '0.000063'} 
2023-05-21 18:45:32 [DEBUG] Epoch 26 - {'loss': '205891223552.000000', 'mae': '326803.750000', 'lr': '0.000063'} 
2023-05-21 18:45:33 [DEBUG] Epoch 27 - {'loss': '206053556224.000000', 'mae': '326991.656250', 'lr': '0.000063'} 
2023-05-21 18:45:34 [DEBUG] Epoch 28 - {'loss': '205933019136.000000', 'mae': '326841.375000', 'lr': '0.000063'} 
2023-05-21 18:45:35 [DEBUG] Epoch 29 - {'loss': '205932118016.000000', 'mae': '326818.156250', 'lr': '0.000063'} 
2023-05-21 18:45:36 [DEBUG] Epoch 30 - {'loss': '205948387328.000000', 'mae': '326818.156250', 'lr': '0.000031'} 
2023-05-21 18:45:37 [DEBUG] Epoch 31 - {'loss': '205919485952.000000', 'mae': '326864.812500', 'lr': '0.000031'} 
2023-05-21 18:45:38 [DEBUG] Epoch 32 - {'loss': '205852917760.000000', 'mae': '326806.312500', 'lr': '0.000031'} 
2023-05-21 18:45:39 [DEBUG] Epoch 33 - {'loss': '205897678848.000000', 'mae': '326873.062500', 'lr': '0.000031'} 
2023-05-21 18:45:41 [DEBUG] Epoch 34 - {'loss': '205943422976.000000', 'mae': '326878.437500', 'lr': '0.000031'} 
2023-05-21 18:45:42 [DEBUG] Epoch 35 - {'loss': '206135427072.000000', 'mae': '327038.031250', 'lr': '0.000031'} 
2023-05-21 18:45:43 [DEBUG] Epoch 36 - {'loss': '206013710336.000000', 'mae': '326876.781250', 'lr': '0.000016'} 
2023-05-21 18:45:44 [DEBUG] Epoch 37 - {'loss': '205943816192.000000', 'mae': '326884.125000', 'lr': '0.000016'} 
2023-05-21 18:45:45 [DEBUG] Epoch 38 - {'loss': '205872955392.000000', 'mae': '326806.968750', 'lr': '0.000016'} 
2023-05-21 18:45:46 [DEBUG] Epoch 39 - {'loss': '206004551680.000000', 'mae': '326877.437500', 'lr': '0.000008'} 
2023-05-21 18:45:47 [DEBUG] Epoch 40 - {'loss': '205890027520.000000', 'mae': '326770.375000', 'lr': '0.000008'} 
2023-05-21 18:45:48 [DEBUG] Epoch 41 - {'loss': '205946847232.000000', 'mae': '326856.062500', 'lr': '0.000008'} 
2023-05-21 18:45:49 [DEBUG] Epoch 42 - {'loss': '205922451456.000000', 'mae': '326803.687500', 'lr': '0.000004'} 
2023-05-21 18:45:50 [DEBUG] Epoch 43 - {'loss': '206031028224.000000', 'mae': '326960.500000', 'lr': '0.000004'} 
2023-05-21 18:45:52 [DEBUG] Epoch 44 - {'loss': '205932249088.000000', 'mae': '326840.843750', 'lr': '0.000004'} 
2023-05-21 18:45:53 [DEBUG] Epoch 45 - {'loss': '205981728768.000000', 'mae': '326875.687500', 'lr': '0.000002'} 
2023-05-21 18:45:54 [DEBUG] Epoch 46 - {'loss': '205842612224.000000', 'mae': '326774.343750', 'lr': '0.000002'} 
2023-05-21 18:45:55 [DEBUG] Epoch 47 - {'loss': '206079475712.000000', 'mae': '327014.250000', 'lr': '0.000002'} 
2023-05-21 18:45:56 [DEBUG] Epoch 48 - {'loss': '205969571840.000000', 'mae': '326904.937500', 'lr': '0.000002'} 
2023-05-21 18:45:57 [DEBUG] Epoch 49 - {'loss': '205952630784.000000', 'mae': '326945.593750', 'lr': '0.000002'} 
2023-05-21 18:45:58 [DEBUG] Epoch 50 - {'loss': '206027620352.000000', 'mae': '326874.968750', 'lr': '0.000001'} 
2023-05-21 18:45:59 [DEBUG] Epoch 51 - {'loss': '205993672704.000000', 'mae': '326849.718750', 'lr': '0.000001'} 
2023-05-21 18:46:01 [DEBUG] Epoch 52 - {'loss': '205949157376.000000', 'mae': '326865.093750', 'lr': '0.000001'} 
2023-05-21 18:46:02 [DEBUG] Epoch 53 - {'loss': '206021427200.000000', 'mae': '326901.343750', 'lr': '0.000000'} 
2023-05-21 18:46:03 [DEBUG] Epoch 54 - {'loss': '205985497088.000000', 'mae': '326930.406250', 'lr': '0.000000'} 
2023-05-21 18:46:04 [DEBUG] Epoch 55 - {'loss': '206172667904.000000', 'mae': '327035.312500', 'lr': '0.000000'} 
2023-05-21 18:46:05 [DEBUG] Epoch 56 - {'loss': '205916176384.000000', 'mae': '326819.093750', 'lr': '0.000000'} 
2023-05-21 18:46:06 [DEBUG] Epoch 57 - {'loss': '205893287936.000000', 'mae': '326790.937500', 'lr': '0.000000'} 
2023-05-21 18:46:07 [DEBUG] Epoch 58 - {'loss': '205915537408.000000', 'mae': '326788.281250', 'lr': '0.000000'} 
2023-05-21 18:46:08 [DEBUG] Epoch 59 - {'loss': '206029225984.000000', 'mae': '326892.093750', 'lr': '0.000000'} 
2023-05-21 18:46:09 [DEBUG] Epoch 60 - {'loss': '206041661440.000000', 'mae': '326972.187500', 'lr': '0.000000'} 
2023-05-21 18:46:11 [DEBUG] Epoch 61 - {'loss': '205912473600.000000', 'mae': '326821.312500', 'lr': '0.000000'} 
2023-05-21 18:46:11 [INFO] Training finished, elapsed time: 69.94 seconds
2023-05-21 18:46:11 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 18:57:54 [INFO] Experiment for mlp test begins at 2023/05/21 18:57:54
2023-05-21 18:57:54 [INFO] Config file contents:
2023-05-21 18:57:54 [INFO] name: mlp test
2023-05-21 18:57:54 [INFO] activation: relu
2023-05-21 18:57:54 [INFO] activation_last_layer: relu
2023-05-21 18:57:54 [INFO] loss_function: mse
2023-05-21 18:57:54 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 18:57:54 [INFO] metric: mae
2023-05-21 18:57:54 [INFO] shuffle: True
2023-05-21 18:57:54 [INFO] epochs: 10
2023-05-21 18:57:54 [INFO] batch_size: 512
2023-05-21 18:57:54 [INFO] verbose: 2
2023-05-21 18:57:54 [INFO] TensorBoard_log_path: logs
2023-05-21 18:57:54 [INFO] TensorBoard_hist_freq: 1
2023-05-21 18:57:54 [INFO] EarlyStopping_monitor: loss
2023-05-21 18:57:54 [INFO] EarlyStopping_patience: 15
2023-05-21 18:57:54 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 18:57:54 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 18:57:54 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 18:57:54 [INFO] log_path: logs/training.log
2023-05-21 18:57:54 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 18:57:54 [INFO] Training in progress
2023-05-21 18:57:55 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-21 18:57:56 [DEBUG] Epoch 0 - {'loss': '4335533555712.000000', 'mae': '1723912.375000', 'lr': '0.001000'} 
2023-05-21 18:57:57 [DEBUG] Epoch 1 - {'loss': '3868831776768.000000', 'mae': '1656936.500000', 'lr': '0.001000'} 
2023-05-21 18:57:58 [DEBUG] Epoch 2 - {'loss': '3868660858880.000000', 'mae': '1656908.625000', 'lr': '0.001000'} 
2023-05-21 18:57:59 [DEBUG] Epoch 3 - {'loss': '3868202369024.000000', 'mae': '1656823.500000', 'lr': '0.001000'} 
2023-05-21 18:58:01 [DEBUG] Epoch 4 - {'loss': '3868272885760.000000', 'mae': '1656839.375000', 'lr': '0.001000'} 
2023-05-21 18:58:02 [DEBUG] Epoch 5 - {'loss': '3868237234176.000000', 'mae': '1656865.875000', 'lr': '0.001000'} 
2023-05-21 18:58:03 [DEBUG] Epoch 6 - {'loss': '3868709093376.000000', 'mae': '1656883.000000', 'lr': '0.001000'} 
2023-05-21 18:58:04 [DEBUG] Epoch 7 - {'loss': '3868212330496.000000', 'mae': '1656823.000000', 'lr': '0.000500'} 
2023-05-21 18:58:05 [DEBUG] Epoch 8 - {'loss': '3868106686464.000000', 'mae': '1656884.625000', 'lr': '0.000500'} 
2023-05-21 18:58:06 [DEBUG] Epoch 9 - {'loss': '3868308799488.000000', 'mae': '1656840.625000', 'lr': '0.000500'} 
2023-05-21 18:59:16 [INFO] Experiment for mlp test begins at 2023/05/21 18:59:16
2023-05-21 18:59:16 [INFO] Config file contents:
2023-05-21 18:59:16 [INFO] name: mlp test
2023-05-21 18:59:16 [INFO] activation: relu
2023-05-21 18:59:16 [INFO] activation_last_layer: relu
2023-05-21 18:59:16 [INFO] loss_function: mse
2023-05-21 18:59:16 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 18:59:16 [INFO] metric: mae
2023-05-21 18:59:16 [INFO] shuffle: True
2023-05-21 18:59:16 [INFO] epochs: 10
2023-05-21 18:59:16 [INFO] batch_size: 512
2023-05-21 18:59:16 [INFO] verbose: 2
2023-05-21 18:59:16 [INFO] TensorBoard_log_path: logs
2023-05-21 18:59:16 [INFO] TensorBoard_hist_freq: 1
2023-05-21 18:59:16 [INFO] EarlyStopping_monitor: loss
2023-05-21 18:59:16 [INFO] EarlyStopping_patience: 15
2023-05-21 18:59:16 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 18:59:16 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 18:59:16 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 18:59:16 [INFO] log_path: logs/training.log
2023-05-21 18:59:16 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 18:59:16 [INFO] Training in progress
2023-05-21 18:59:16 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-21 18:59:18 [DEBUG] Epoch 0 - {'loss': '8383446384640.000000', 'mae': '2319874.750000', 'lr': '0.001000'} 
2023-05-21 18:59:19 [DEBUG] Epoch 1 - {'loss': '8218489126912.000000', 'mae': '2306936.000000', 'lr': '0.001000'} 
2023-05-21 18:59:20 [DEBUG] Epoch 2 - {'loss': '8218274168832.000000', 'mae': '2306913.250000', 'lr': '0.001000'} 
2023-05-21 18:59:21 [DEBUG] Epoch 3 - {'loss': '8217971130368.000000', 'mae': '2306886.250000', 'lr': '0.001000'} 
2023-05-21 18:59:22 [DEBUG] Epoch 4 - {'loss': '8218537361408.000000', 'mae': '2306946.250000', 'lr': '0.001000'} 
2023-05-21 18:59:23 [DEBUG] Epoch 5 - {'loss': '8218321354752.000000', 'mae': '2306923.000000', 'lr': '0.001000'} 
2023-05-21 18:59:24 [DEBUG] Epoch 6 - {'loss': '8218342326272.000000', 'mae': '2306891.750000', 'lr': '0.001000'} 
2023-05-21 18:59:25 [DEBUG] Epoch 7 - {'loss': '8218426736640.000000', 'mae': '2306899.750000', 'lr': '0.000500'} 
2023-05-21 18:59:27 [DEBUG] Epoch 8 - {'loss': '8218038763520.000000', 'mae': '2306892.250000', 'lr': '0.000500'} 
2023-05-21 18:59:28 [DEBUG] Epoch 9 - {'loss': '8218250575872.000000', 'mae': '2306939.750000', 'lr': '0.000500'} 
2023-05-21 19:27:41 [INFO] Experiment for mlp test begins at 2023/05/21 19:27:41
2023-05-21 19:27:41 [INFO] Config file contents:
2023-05-21 19:27:41 [INFO] name: mlp test
2023-05-21 19:27:41 [INFO] activation: relu
2023-05-21 19:27:41 [INFO] activation_last_layer: relu
2023-05-21 19:27:41 [INFO] loss_function: mse
2023-05-21 19:27:41 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:27:41 [INFO] metric: mae
2023-05-21 19:27:41 [INFO] shuffle: True
2023-05-21 19:27:41 [INFO] epochs: 10
2023-05-21 19:27:41 [INFO] batch_size: 512
2023-05-21 19:27:41 [INFO] verbose: 2
2023-05-21 19:27:41 [INFO] TensorBoard_log_path: logs
2023-05-21 19:27:41 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:27:41 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:27:41 [INFO] EarlyStopping_patience: 15
2023-05-21 19:27:41 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:27:41 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:27:41 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:27:41 [INFO] log_path: logs/training.log
2023-05-21 19:27:41 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:27:41 [INFO] Training in progress
2023-05-21 19:27:41 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-21 19:27:43 [DEBUG] Epoch 0 - {'loss': '1235635666944.000000', 'mae': '503329.812500', 'lr': '0.001000'} 
2023-05-21 19:27:44 [DEBUG] Epoch 1 - {'loss': '171722326016.000000', 'mae': '301329.531250', 'lr': '0.001000'} 
2023-05-21 19:27:45 [DEBUG] Epoch 2 - {'loss': '170301177856.000000', 'mae': '300153.093750', 'lr': '0.001000'} 
2023-05-21 19:27:46 [DEBUG] Epoch 3 - {'loss': '170022076416.000000', 'mae': '299878.843750', 'lr': '0.001000'} 
2023-05-21 19:27:47 [DEBUG] Epoch 4 - {'loss': '169820454912.000000', 'mae': '299598.843750', 'lr': '0.001000'} 
2023-05-21 19:27:48 [DEBUG] Epoch 5 - {'loss': '169830023168.000000', 'mae': '299706.812500', 'lr': '0.001000'} 
2023-05-21 19:27:49 [DEBUG] Epoch 6 - {'loss': '169859645440.000000', 'mae': '299734.125000', 'lr': '0.001000'} 
2023-05-21 19:27:50 [DEBUG] Epoch 7 - {'loss': '169918889984.000000', 'mae': '299801.156250', 'lr': '0.001000'} 
2023-05-21 19:27:52 [DEBUG] Epoch 8 - {'loss': '169733619712.000000', 'mae': '299602.875000', 'lr': '0.000500'} 
2023-05-21 19:27:53 [DEBUG] Epoch 9 - {'loss': '169811640320.000000', 'mae': '299720.312500', 'lr': '0.000500'} 
2023-05-21 19:27:53 [INFO] Training finished, elapsed time: 12.05 seconds
2023-05-21 19:27:53 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:29:36 [INFO] Experiment for mlp test begins at 2023/05/21 19:29:36
2023-05-21 19:29:36 [INFO] Config file contents:
2023-05-21 19:29:36 [INFO] name: mlp test
2023-05-21 19:29:36 [INFO] activation: relu
2023-05-21 19:29:36 [INFO] activation_last_layer: relu
2023-05-21 19:29:36 [INFO] loss_function: mse
2023-05-21 19:29:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:29:36 [INFO] metric: mae
2023-05-21 19:29:36 [INFO] shuffle: True
2023-05-21 19:29:36 [INFO] epochs: 10
2023-05-21 19:29:36 [INFO] batch_size: 512
2023-05-21 19:29:36 [INFO] verbose: 2
2023-05-21 19:29:36 [INFO] TensorBoard_log_path: logs
2023-05-21 19:29:36 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:29:36 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:29:36 [INFO] EarlyStopping_patience: 15
2023-05-21 19:29:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:29:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:29:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:29:36 [INFO] log_path: logs/training.log
2023-05-21 19:29:36 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:29:36 [INFO] Training in progress
2023-05-21 19:29:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-21 19:29:38 [DEBUG] Epoch 0 - {'loss': '438901604352.000000', 'mae': '380461.906250', 'lr': '0.001000'} 
2023-05-21 19:29:39 [DEBUG] Epoch 1 - {'loss': '170232446976.000000', 'mae': '300003.562500', 'lr': '0.001000'} 
2023-05-21 19:29:40 [DEBUG] Epoch 2 - {'loss': '170126032896.000000', 'mae': '299941.875000', 'lr': '0.001000'} 
2023-05-21 19:29:41 [DEBUG] Epoch 3 - {'loss': '169822339072.000000', 'mae': '299700.625000', 'lr': '0.001000'} 
2023-05-21 19:29:43 [DEBUG] Epoch 4 - {'loss': '169875062784.000000', 'mae': '299710.156250', 'lr': '0.001000'} 
2023-05-21 19:29:44 [DEBUG] Epoch 5 - {'loss': '169922428928.000000', 'mae': '299796.125000', 'lr': '0.001000'} 
2023-05-21 19:29:45 [DEBUG] Epoch 6 - {'loss': '169745694720.000000', 'mae': '299647.125000', 'lr': '0.001000'} 
2023-05-21 19:29:46 [DEBUG] Epoch 7 - {'loss': '169826877440.000000', 'mae': '299732.125000', 'lr': '0.001000'} 
2023-05-21 19:29:47 [DEBUG] Epoch 8 - {'loss': '169965092864.000000', 'mae': '299800.343750', 'lr': '0.001000'} 
2023-05-21 19:29:48 [DEBUG] Epoch 9 - {'loss': '169832660992.000000', 'mae': '299673.406250', 'lr': '0.001000'} 
2023-05-21 19:29:48 [INFO] Training finished, elapsed time: 12.16 seconds
2023-05-21 19:29:48 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:38:37 [INFO] Experiment for mlp test begins at 2023/05/21 19:38:37
2023-05-21 19:38:37 [INFO] Config file contents:
2023-05-21 19:38:37 [INFO] name: mlp test
2023-05-21 19:38:37 [INFO] activation: relu
2023-05-21 19:38:37 [INFO] activation_last_layer: relu
2023-05-21 19:38:37 [INFO] loss_function: mse
2023-05-21 19:38:37 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:38:37 [INFO] metric: mae
2023-05-21 19:38:37 [INFO] shuffle: True
2023-05-21 19:38:37 [INFO] epochs: 10
2023-05-21 19:38:37 [INFO] batch_size: 512
2023-05-21 19:38:37 [INFO] verbose: 2
2023-05-21 19:38:37 [INFO] TensorBoard_log_path: logs
2023-05-21 19:38:37 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:38:37 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:38:37 [INFO] EarlyStopping_patience: 15
2023-05-21 19:38:37 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:38:37 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:38:37 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:38:37 [INFO] log_path: logs/training.log
2023-05-21 19:38:37 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:38:37 [INFO] Training in progress
2023-05-21 19:38:38 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 19:38:39 [DEBUG] Epoch 0 - {'loss': '4358261440512.000000', 'mae': '1472522.875000', 'lr': '0.001000'} 
2023-05-21 19:38:40 [DEBUG] Epoch 1 - {'loss': '123276075008.000000', 'mae': '256410.640625', 'lr': '0.001000'} 
2023-05-21 19:38:41 [DEBUG] Epoch 2 - {'loss': '117958385664.000000', 'mae': '251607.625000', 'lr': '0.001000'} 
2023-05-21 19:38:42 [DEBUG] Epoch 3 - {'loss': '116920000512.000000', 'mae': '250617.296875', 'lr': '0.001000'} 
2023-05-21 19:38:43 [DEBUG] Epoch 4 - {'loss': '116352163840.000000', 'mae': '249934.578125', 'lr': '0.001000'} 
2023-05-21 19:38:45 [DEBUG] Epoch 5 - {'loss': '116388511744.000000', 'mae': '249974.140625', 'lr': '0.001000'} 
2023-05-21 19:38:46 [DEBUG] Epoch 6 - {'loss': '116109148160.000000', 'mae': '249584.984375', 'lr': '0.001000'} 
2023-05-21 19:38:47 [DEBUG] Epoch 7 - {'loss': '116106788864.000000', 'mae': '249604.609375', 'lr': '0.001000'} 
2023-05-21 19:38:48 [DEBUG] Epoch 8 - {'loss': '115853606912.000000', 'mae': '249324.968750', 'lr': '0.001000'} 
2023-05-21 19:38:49 [DEBUG] Epoch 9 - {'loss': '115786375168.000000', 'mae': '249231.312500', 'lr': '0.001000'} 
2023-05-21 19:38:49 [INFO] Training finished, elapsed time: 12.23 seconds
2023-05-21 19:38:49 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:40:02 [INFO] Experiment for mlp test begins at 2023/05/21 19:40:02
2023-05-21 19:40:02 [INFO] Config file contents:
2023-05-21 19:40:02 [INFO] name: mlp test
2023-05-21 19:40:02 [INFO] activation: relu
2023-05-21 19:40:02 [INFO] activation_last_layer: relu
2023-05-21 19:40:02 [INFO] loss_function: mse
2023-05-21 19:40:02 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:40:02 [INFO] metric: mae
2023-05-21 19:40:02 [INFO] shuffle: True
2023-05-21 19:40:02 [INFO] epochs: 2
2023-05-21 19:40:02 [INFO] batch_size: 512
2023-05-21 19:40:02 [INFO] verbose: 2
2023-05-21 19:40:02 [INFO] TensorBoard_log_path: logs
2023-05-21 19:40:02 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:40:02 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:40:02 [INFO] EarlyStopping_patience: 15
2023-05-21 19:40:02 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:40:02 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:40:02 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:40:02 [INFO] log_path: logs/training.log
2023-05-21 19:40:02 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:40:02 [INFO] Training in progress
2023-05-21 19:40:03 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 19:40:04 [DEBUG] Epoch 0 - {'loss': '1111151738880.000000', 'mae': '521822.250000', 'lr': '0.001000'} 
2023-05-21 19:40:05 [DEBUG] Epoch 1 - {'loss': '116868128768.000000', 'mae': '250626.328125', 'lr': '0.001000'} 
2023-05-21 19:40:05 [INFO] Training finished, elapsed time: 3.10 seconds
2023-05-21 19:40:05 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:42:06 [INFO] Experiment for mlp test begins at 2023/05/21 19:42:06
2023-05-21 19:42:06 [INFO] Config file contents:
2023-05-21 19:42:06 [INFO] name: mlp test
2023-05-21 19:42:06 [INFO] activation: relu
2023-05-21 19:42:06 [INFO] activation_last_layer: relu
2023-05-21 19:42:06 [INFO] loss_function: mse
2023-05-21 19:42:06 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:42:06 [INFO] metric: mae
2023-05-21 19:42:06 [INFO] shuffle: True
2023-05-21 19:42:06 [INFO] epochs: 2
2023-05-21 19:42:06 [INFO] batch_size: 512
2023-05-21 19:42:06 [INFO] verbose: 2
2023-05-21 19:42:06 [INFO] TensorBoard_log_path: logs
2023-05-21 19:42:06 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:42:06 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:42:06 [INFO] EarlyStopping_patience: 15
2023-05-21 19:42:06 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:42:06 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:42:06 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:42:06 [INFO] log_path: logs/training.log
2023-05-21 19:42:06 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:42:07 [INFO] Training in progress
2023-05-21 19:42:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 19:42:08 [DEBUG] Epoch 0 - {'loss': '18184635678720.000000', 'mae': '2885997.250000', 'lr': '0.001000'} 
2023-05-21 19:42:10 [DEBUG] Epoch 1 - {'loss': '16776747286528.000000', 'mae': '2787598.250000', 'lr': '0.001000'} 
2023-05-21 19:42:10 [INFO] Training finished, elapsed time: 3.10 seconds
2023-05-21 19:42:10 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:46:12 [INFO] Experiment for mlp test begins at 2023/05/21 19:46:12
2023-05-21 19:46:12 [INFO] Config file contents:
2023-05-21 19:46:12 [INFO] name: mlp test
2023-05-21 19:46:12 [INFO] activation: relu
2023-05-21 19:46:12 [INFO] activation_last_layer: relu
2023-05-21 19:46:12 [INFO] loss_function: mse
2023-05-21 19:46:12 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:46:12 [INFO] metric: mae
2023-05-21 19:46:12 [INFO] shuffle: True
2023-05-21 19:46:12 [INFO] epochs: 2
2023-05-21 19:46:12 [INFO] batch_size: 512
2023-05-21 19:46:12 [INFO] verbose: 2
2023-05-21 19:46:12 [INFO] TensorBoard_log_path: logs
2023-05-21 19:46:12 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:46:12 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:46:12 [INFO] EarlyStopping_patience: 15
2023-05-21 19:46:12 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:46:12 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:46:12 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:46:12 [INFO] log_path: logs/training.log
2023-05-21 19:46:12 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:46:12 [INFO] Training in progress
2023-05-21 19:46:12 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-21 19:46:14 [DEBUG] Epoch 0 - {'loss': '596275560448.000000', 'mae': '399564.968750', 'lr': '0.001000'} 
2023-05-21 19:46:15 [DEBUG] Epoch 1 - {'loss': '206628225024.000000', 'mae': '327590.625000', 'lr': '0.001000'} 
2023-05-21 19:46:15 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-21 19:46:15 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:47:17 [INFO] Experiment for mlp test begins at 2023/05/21 19:47:17
2023-05-21 19:47:17 [INFO] Config file contents:
2023-05-21 19:47:17 [INFO] name: mlp test
2023-05-21 19:47:17 [INFO] activation: relu
2023-05-21 19:47:17 [INFO] activation_last_layer: relu
2023-05-21 19:47:17 [INFO] loss_function: mse
2023-05-21 19:47:17 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:47:17 [INFO] metric: mae
2023-05-21 19:47:17 [INFO] shuffle: True
2023-05-21 19:47:17 [INFO] epochs: 2
2023-05-21 19:47:17 [INFO] batch_size: 512
2023-05-21 19:47:17 [INFO] verbose: 2
2023-05-21 19:47:17 [INFO] TensorBoard_log_path: logs
2023-05-21 19:47:17 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:47:17 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:47:17 [INFO] EarlyStopping_patience: 15
2023-05-21 19:47:17 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:47:17 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:47:17 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:47:17 [INFO] log_path: logs/training.log
2023-05-21 19:47:17 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:47:17 [INFO] Training in progress
2023-05-21 19:47:18 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-21 19:47:19 [DEBUG] Epoch 0 - {'loss': '670939676672.000000', 'mae': '391735.750000', 'lr': '0.001000'} 
2023-05-21 19:47:20 [DEBUG] Epoch 1 - {'loss': '117719154688.000000', 'mae': '251043.578125', 'lr': '0.001000'} 
2023-05-21 19:47:20 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-21 19:47:20 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:51:48 [INFO] Experiment for mlp test begins at 2023/05/21 19:51:48
2023-05-21 19:51:48 [INFO] Config file contents:
2023-05-21 19:51:48 [INFO] name: mlp test
2023-05-21 19:51:48 [INFO] activation: relu
2023-05-21 19:51:48 [INFO] activation_last_layer: relu
2023-05-21 19:51:48 [INFO] loss_function: mse
2023-05-21 19:51:48 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:51:48 [INFO] metric: mae
2023-05-21 19:51:48 [INFO] shuffle: True
2023-05-21 19:51:48 [INFO] epochs: 2
2023-05-21 19:51:48 [INFO] batch_size: 512
2023-05-21 19:51:48 [INFO] verbose: 2
2023-05-21 19:51:48 [INFO] TensorBoard_log_path: logs
2023-05-21 19:51:48 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:51:48 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:51:48 [INFO] EarlyStopping_patience: 15
2023-05-21 19:51:48 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:51:48 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:51:48 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:51:48 [INFO] log_path: logs/training.log
2023-05-21 19:51:48 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:51:48 [INFO] Training in progress
2023-05-21 19:51:49 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 19:51:50 [DEBUG] Epoch 0 - {'loss': '749334036480.000000', 'mae': '410440.812500', 'lr': '0.001000'} 
2023-05-21 19:51:51 [DEBUG] Epoch 1 - {'loss': '117051965440.000000', 'mae': '250765.484375', 'lr': '0.001000'} 
2023-05-21 19:51:51 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-21 19:51:51 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:58:44 [INFO] Experiment for mlp test begins at 2023/05/21 19:58:44
2023-05-21 19:58:44 [INFO] Config file contents:
2023-05-21 19:58:44 [INFO] name: mlp test
2023-05-21 19:58:44 [INFO] activation: relu
2023-05-21 19:58:44 [INFO] activation_last_layer: relu
2023-05-21 19:58:44 [INFO] loss_function: mse
2023-05-21 19:58:44 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:58:44 [INFO] metric: mae
2023-05-21 19:58:44 [INFO] shuffle: True
2023-05-21 19:58:44 [INFO] epochs: 2
2023-05-21 19:58:44 [INFO] batch_size: 512
2023-05-21 19:58:44 [INFO] verbose: 2
2023-05-21 19:58:44 [INFO] TensorBoard_log_path: logs
2023-05-21 19:58:44 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:58:44 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:58:44 [INFO] EarlyStopping_patience: 15
2023-05-21 19:58:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:58:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:58:44 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:58:44 [INFO] log_path: logs/training.log
2023-05-21 19:58:44 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:58:44 [INFO] Training in progress
2023-05-21 19:58:45 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-21 19:58:46 [DEBUG] Epoch 0 - {'loss': '11468305072128.000000', 'mae': '2036391.500000', 'lr': '0.001000'} 
2023-05-21 19:58:47 [DEBUG] Epoch 1 - {'loss': '121561366528.000000', 'mae': '254737.765625', 'lr': '0.001000'} 
2023-05-21 19:58:47 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-21 19:58:47 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 19:59:09 [INFO] Experiment for mlp test begins at 2023/05/21 19:59:09
2023-05-21 19:59:09 [INFO] Config file contents:
2023-05-21 19:59:09 [INFO] name: mlp test
2023-05-21 19:59:09 [INFO] activation: relu
2023-05-21 19:59:09 [INFO] activation_last_layer: relu
2023-05-21 19:59:09 [INFO] loss_function: mse
2023-05-21 19:59:09 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 19:59:09 [INFO] metric: mae
2023-05-21 19:59:09 [INFO] shuffle: True
2023-05-21 19:59:09 [INFO] epochs: 2
2023-05-21 19:59:09 [INFO] batch_size: 512
2023-05-21 19:59:09 [INFO] verbose: 2
2023-05-21 19:59:09 [INFO] TensorBoard_log_path: logs
2023-05-21 19:59:09 [INFO] TensorBoard_hist_freq: 1
2023-05-21 19:59:09 [INFO] EarlyStopping_monitor: loss
2023-05-21 19:59:09 [INFO] EarlyStopping_patience: 15
2023-05-21 19:59:09 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 19:59:09 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 19:59:09 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 19:59:09 [INFO] log_path: logs/training.log
2023-05-21 19:59:09 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 19:59:09 [INFO] Training in progress
2023-05-21 19:59:10 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-21 19:59:11 [DEBUG] Epoch 0 - {'loss': '8702832672768.000000', 'mae': '2339456.750000', 'lr': '0.001000'} 
2023-05-21 19:59:12 [DEBUG] Epoch 1 - {'loss': '8218271023104.000000', 'mae': '2306614.500000', 'lr': '0.001000'} 
2023-05-21 19:59:12 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-21 19:59:12 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 20:10:06 [INFO] Experiment for mlp test begins at 2023/05/21 20:10:06
2023-05-21 20:10:06 [INFO] Config file contents:
2023-05-21 20:10:06 [INFO] name: mlp test
2023-05-21 20:10:06 [INFO] activation: relu
2023-05-21 20:10:06 [INFO] activation_last_layer: relu
2023-05-21 20:10:06 [INFO] loss_function: mse
2023-05-21 20:10:06 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 20:10:06 [INFO] metric: mae
2023-05-21 20:10:06 [INFO] shuffle: True
2023-05-21 20:10:06 [INFO] epochs: 2
2023-05-21 20:10:06 [INFO] batch_size: 512
2023-05-21 20:10:06 [INFO] verbose: 2
2023-05-21 20:10:06 [INFO] TensorBoard_log_path: logs
2023-05-21 20:10:06 [INFO] TensorBoard_hist_freq: 1
2023-05-21 20:10:06 [INFO] EarlyStopping_monitor: loss
2023-05-21 20:10:06 [INFO] EarlyStopping_patience: 15
2023-05-21 20:10:06 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 20:10:06 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 20:10:06 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 20:10:06 [INFO] log_path: logs/training.log
2023-05-21 20:10:06 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 20:10:06 [INFO] Training in progress
2023-05-21 20:10:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 20:10:08 [DEBUG] Epoch 0 - {'loss': '7486094966784.000000', 'mae': '1975311.250000', 'lr': '0.001000'} 
2023-05-21 20:10:09 [DEBUG] Epoch 1 - {'loss': '3869606936576.000000', 'mae': '1657054.625000', 'lr': '0.001000'} 
2023-05-21 20:10:09 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-21 20:10:09 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 20:10:50 [INFO] Experiment for mlp test begins at 2023/05/21 20:10:50
2023-05-21 20:10:50 [INFO] Config file contents:
2023-05-21 20:10:50 [INFO] name: mlp test
2023-05-21 20:10:50 [INFO] activation: relu
2023-05-21 20:10:50 [INFO] activation_last_layer: relu
2023-05-21 20:10:50 [INFO] loss_function: mse
2023-05-21 20:10:50 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 20:10:50 [INFO] metric: mae
2023-05-21 20:10:50 [INFO] shuffle: True
2023-05-21 20:10:50 [INFO] epochs: 2
2023-05-21 20:10:50 [INFO] batch_size: 512
2023-05-21 20:10:50 [INFO] verbose: 2
2023-05-21 20:10:50 [INFO] TensorBoard_log_path: logs
2023-05-21 20:10:50 [INFO] TensorBoard_hist_freq: 1
2023-05-21 20:10:50 [INFO] EarlyStopping_monitor: loss
2023-05-21 20:10:50 [INFO] EarlyStopping_patience: 15
2023-05-21 20:10:50 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 20:10:50 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 20:10:50 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 20:10:50 [INFO] log_path: logs/training.log
2023-05-21 20:10:50 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 20:10:50 [INFO] Training in progress
2023-05-21 20:10:51 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-21 20:10:52 [DEBUG] Epoch 0 - {'loss': '650871177216.000000', 'mae': '411382.937500', 'lr': '0.001000'} 
2023-05-21 20:10:53 [DEBUG] Epoch 1 - {'loss': '204558041088.000000', 'mae': '326848.781250', 'lr': '0.001000'} 
2023-05-21 20:10:53 [INFO] Training finished, elapsed time: 3.11 seconds
2023-05-21 20:10:53 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-21 20:13:39 [INFO] Experiment for mlp test begins at 2023/05/21 20:13:39
2023-05-21 20:13:39 [INFO] Config file contents:
2023-05-21 20:13:39 [INFO] name: mlp test
2023-05-21 20:13:39 [INFO] activation: relu
2023-05-21 20:13:39 [INFO] activation_last_layer: relu
2023-05-21 20:13:39 [INFO] loss_function: mse
2023-05-21 20:13:39 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-21 20:13:39 [INFO] metric: mae
2023-05-21 20:13:39 [INFO] shuffle: True
2023-05-21 20:13:39 [INFO] epochs: 2
2023-05-21 20:13:39 [INFO] batch_size: 512
2023-05-21 20:13:39 [INFO] verbose: 2
2023-05-21 20:13:39 [INFO] TensorBoard_log_path: logs
2023-05-21 20:13:39 [INFO] TensorBoard_hist_freq: 1
2023-05-21 20:13:39 [INFO] EarlyStopping_monitor: loss
2023-05-21 20:13:39 [INFO] EarlyStopping_patience: 15
2023-05-21 20:13:39 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-21 20:13:39 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-21 20:13:39 [INFO] ReduceLROnPlateau_patience: 3
2023-05-21 20:13:39 [INFO] log_path: logs/training.log
2023-05-21 20:13:39 [INFO] save_path: models/pretrain_mlp.h5
2023-05-21 20:13:39 [INFO] Training in progress
2023-05-21 20:13:40 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-21 20:13:41 [DEBUG] Epoch 0 - {'loss': '917759000576.000000', 'mae': '470558.531250', 'lr': '0.001000'} 
2023-05-21 20:13:42 [DEBUG] Epoch 1 - {'loss': '152704925696.000000', 'mae': '277496.843750', 'lr': '0.001000'} 
2023-05-21 20:13:42 [INFO] Training finished, elapsed time: 3.07 seconds
2023-05-21 20:13:42 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-22 19:30:45 [INFO] Experiment for mlp test begins at 2023/05/22 19:30:45
2023-05-22 19:30:45 [INFO] Config file contents:
2023-05-22 19:30:45 [INFO] name: mlp test
2023-05-22 19:30:45 [INFO] activation: relu
2023-05-22 19:30:45 [INFO] activation_last_layer: relu
2023-05-22 19:30:45 [INFO] loss_function: mse
2023-05-22 19:30:45 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-22 19:30:45 [INFO] metric: mae
2023-05-22 19:30:45 [INFO] shuffle: True
2023-05-22 19:30:45 [INFO] epochs: 2
2023-05-22 19:30:45 [INFO] batch_size: 512
2023-05-22 19:30:45 [INFO] verbose: 2
2023-05-22 19:30:45 [INFO] TensorBoard_log_path: logs
2023-05-22 19:30:45 [INFO] TensorBoard_hist_freq: 1
2023-05-22 19:30:45 [INFO] EarlyStopping_monitor: loss
2023-05-22 19:30:45 [INFO] EarlyStopping_patience: 15
2023-05-22 19:30:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-22 19:30:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-22 19:30:45 [INFO] ReduceLROnPlateau_patience: 3
2023-05-22 19:30:45 [INFO] log_path: logs/training.log
2023-05-22 19:30:45 [INFO] save_path: models/pretrain_mlp.h5
2023-05-22 19:30:45 [INFO] Training in progress
2023-05-22 19:30:46 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-22 19:30:47 [DEBUG] Epoch 0 - {'loss': '591957786624.000000', 'mae': '395528.343750', 'lr': '0.001000'} 
2023-05-22 19:30:48 [DEBUG] Epoch 1 - {'loss': '206272987136.000000', 'mae': '327072.968750', 'lr': '0.001000'} 
2023-05-22 19:30:48 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-22 19:30:48 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-22 21:01:52 [INFO] Experiment for mlp test begins at 2023/05/22 21:01:52
2023-05-22 21:01:52 [INFO] Config file contents:
2023-05-22 21:01:52 [INFO] name: mlp test
2023-05-22 21:01:52 [INFO] activation: relu
2023-05-22 21:01:52 [INFO] activation_last_layer: relu
2023-05-22 21:01:52 [INFO] loss_function: mse
2023-05-22 21:01:52 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-22 21:01:52 [INFO] metric: mae
2023-05-22 21:01:52 [INFO] shuffle: True
2023-05-22 21:01:52 [INFO] epochs: 2
2023-05-22 21:01:52 [INFO] batch_size: 512
2023-05-22 21:01:52 [INFO] verbose: 2
2023-05-22 21:01:52 [INFO] TensorBoard_log_path: logs
2023-05-22 21:01:52 [INFO] TensorBoard_hist_freq: 1
2023-05-22 21:01:52 [INFO] EarlyStopping_monitor: loss
2023-05-22 21:01:52 [INFO] EarlyStopping_patience: 15
2023-05-22 21:01:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-22 21:01:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-22 21:01:52 [INFO] ReduceLROnPlateau_patience: 3
2023-05-22 21:01:52 [INFO] log_path: logs/training.log
2023-05-22 21:01:52 [INFO] save_path: models/pretrain_mlp.h5
2023-05-22 21:01:52 [INFO] Training in progress
2023-05-22 21:01:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-22 21:01:54 [DEBUG] Epoch 0 - {'loss': '407959437312.000000', 'mae': '376082.687500', 'lr': '0.001000'} 
2023-05-22 21:01:55 [DEBUG] Epoch 1 - {'loss': '206149763072.000000', 'mae': '326961.312500', 'lr': '0.001000'} 
2023-05-22 21:01:55 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-22 21:01:55 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-22 21:03:56 [INFO] Experiment for mlp test begins at 2023/05/22 21:03:56
2023-05-22 21:03:56 [INFO] Config file contents:
2023-05-22 21:03:56 [INFO] name: mlp test
2023-05-22 21:03:56 [INFO] activation: relu
2023-05-22 21:03:56 [INFO] activation_last_layer: relu
2023-05-22 21:03:56 [INFO] loss_function: mse
2023-05-22 21:03:56 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-22 21:03:56 [INFO] metric: mae
2023-05-22 21:03:56 [INFO] shuffle: True
2023-05-22 21:03:56 [INFO] epochs: 2
2023-05-22 21:03:56 [INFO] batch_size: 512
2023-05-22 21:03:56 [INFO] verbose: 2
2023-05-22 21:03:56 [INFO] TensorBoard_log_path: logs
2023-05-22 21:03:56 [INFO] TensorBoard_hist_freq: 1
2023-05-22 21:03:56 [INFO] EarlyStopping_monitor: loss
2023-05-22 21:03:56 [INFO] EarlyStopping_patience: 15
2023-05-22 21:03:56 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-22 21:03:56 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-22 21:03:56 [INFO] ReduceLROnPlateau_patience: 3
2023-05-22 21:03:56 [INFO] log_path: logs/training.log
2023-05-22 21:03:56 [INFO] save_path: models/pretrain_mlp.h5
2023-05-22 21:03:56 [INFO] Training in progress
2023-05-22 21:03:57 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-22 21:03:58 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-22 21:03:59 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-22 21:03:59 [INFO] Training finished, elapsed time: 3.13 seconds
2023-05-22 21:03:59 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-22 21:06:21 [INFO] Experiment for mlp test begins at 2023/05/22 21:06:21
2023-05-22 21:06:21 [INFO] Config file contents:
2023-05-22 21:06:21 [INFO] name: mlp test
2023-05-22 21:06:21 [INFO] activation: relu
2023-05-22 21:06:21 [INFO] activation_last_layer: relu
2023-05-22 21:06:21 [INFO] loss_function: mse
2023-05-22 21:06:21 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-22 21:06:21 [INFO] metric: mae
2023-05-22 21:06:21 [INFO] shuffle: True
2023-05-22 21:06:21 [INFO] epochs: 2
2023-05-22 21:06:21 [INFO] batch_size: 512
2023-05-22 21:06:21 [INFO] verbose: 2
2023-05-22 21:06:21 [INFO] TensorBoard_log_path: logs
2023-05-22 21:06:21 [INFO] TensorBoard_hist_freq: 1
2023-05-22 21:06:21 [INFO] EarlyStopping_monitor: loss
2023-05-22 21:06:21 [INFO] EarlyStopping_patience: 15
2023-05-22 21:06:21 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-22 21:06:21 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-22 21:06:21 [INFO] ReduceLROnPlateau_patience: 3
2023-05-22 21:06:21 [INFO] log_path: logs/training.log
2023-05-22 21:06:21 [INFO] save_path: models/pretrain_mlp.h5
2023-05-22 21:06:21 [INFO] Training in progress
2023-05-22 21:06:22 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-22 21:06:23 [DEBUG] Epoch 0 - {'loss': '34595451961344.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-22 21:06:24 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-22 21:06:24 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-22 21:06:24 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-22 21:10:52 [INFO] Experiment for mlp test begins at 2023/05/22 21:10:52
2023-05-22 21:10:52 [INFO] Config file contents:
2023-05-22 21:10:52 [INFO] name: mlp test
2023-05-22 21:10:52 [INFO] activation: relu
2023-05-22 21:10:52 [INFO] activation_last_layer: relu
2023-05-22 21:10:52 [INFO] loss_function: mse
2023-05-22 21:10:52 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-22 21:10:52 [INFO] metric: mae
2023-05-22 21:10:52 [INFO] shuffle: True
2023-05-22 21:10:52 [INFO] epochs: 2
2023-05-22 21:10:52 [INFO] batch_size: 512
2023-05-22 21:10:52 [INFO] verbose: 2
2023-05-22 21:10:52 [INFO] TensorBoard_log_path: logs
2023-05-22 21:10:52 [INFO] TensorBoard_hist_freq: 1
2023-05-22 21:10:52 [INFO] EarlyStopping_monitor: loss
2023-05-22 21:10:52 [INFO] EarlyStopping_patience: 15
2023-05-22 21:10:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-22 21:10:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-22 21:10:52 [INFO] ReduceLROnPlateau_patience: 3
2023-05-22 21:10:52 [INFO] log_path: logs/training.log
2023-05-22 21:10:52 [INFO] save_path: models/pretrain_mlp.h5
2023-05-22 21:10:52 [INFO] Training in progress
2023-05-22 21:10:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-22 21:10:54 [DEBUG] Epoch 0 - {'loss': '34595420504064.000000', 'mae': '3707967.250000', 'lr': '0.001000'} 
2023-05-22 21:10:55 [DEBUG] Epoch 1 - {'loss': '34595451961344.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-22 21:10:55 [INFO] Training finished, elapsed time: 3.07 seconds
2023-05-22 21:10:55 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:00:51 [INFO] Experiment for mlp test begins at 2023/05/24 02:00:51
2023-05-24 02:00:51 [INFO] Config file contents:
2023-05-24 02:00:51 [INFO] name: mlp test
2023-05-24 02:00:51 [INFO] activation: relu
2023-05-24 02:00:51 [INFO] activation_last_layer: relu
2023-05-24 02:00:51 [INFO] loss_function: mse
2023-05-24 02:00:51 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:00:51 [INFO] metric: mae
2023-05-24 02:00:51 [INFO] shuffle: True
2023-05-24 02:00:51 [INFO] epochs: 2
2023-05-24 02:00:51 [INFO] batch_size: 512
2023-05-24 02:00:51 [INFO] verbose: 2
2023-05-24 02:00:51 [INFO] TensorBoard_log_path: logs
2023-05-24 02:00:51 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:00:51 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:00:51 [INFO] EarlyStopping_patience: 15
2023-05-24 02:00:51 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:00:51 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:00:51 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:00:51 [INFO] log_path: logs/training.log
2023-05-24 02:00:51 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:00:51 [INFO] Training in progress
2023-05-24 02:00:51 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 02:00:53 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-24 02:00:54 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707967.000000', 'lr': '0.001000'} 
2023-05-24 02:00:54 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-24 02:00:54 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:08:44 [INFO] Experiment for mlp test begins at 2023/05/24 02:08:44
2023-05-24 02:08:44 [INFO] Config file contents:
2023-05-24 02:08:44 [INFO] name: mlp test
2023-05-24 02:08:44 [INFO] activation: relu
2023-05-24 02:08:44 [INFO] activation_last_layer: relu
2023-05-24 02:08:44 [INFO] loss_function: mse
2023-05-24 02:08:44 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:08:44 [INFO] metric: mae
2023-05-24 02:08:44 [INFO] shuffle: True
2023-05-24 02:08:44 [INFO] epochs: 2
2023-05-24 02:08:44 [INFO] batch_size: 512
2023-05-24 02:08:44 [INFO] verbose: 2
2023-05-24 02:08:44 [INFO] TensorBoard_log_path: logs
2023-05-24 02:08:44 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:08:44 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:08:44 [INFO] EarlyStopping_patience: 15
2023-05-24 02:08:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:08:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:08:44 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:08:44 [INFO] log_path: logs/training.log
2023-05-24 02:08:44 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:08:44 [INFO] Training in progress
2023-05-24 02:08:45 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 02:08:46 [DEBUG] Epoch 0 - {'loss': '34595445669888.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 02:08:47 [DEBUG] Epoch 1 - {'loss': '34595420504064.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-24 02:08:47 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 02:08:47 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:10:37 [INFO] Experiment for mlp test begins at 2023/05/24 02:10:37
2023-05-24 02:10:37 [INFO] Config file contents:
2023-05-24 02:10:37 [INFO] name: mlp test
2023-05-24 02:10:37 [INFO] activation: relu
2023-05-24 02:10:37 [INFO] activation_last_layer: relu
2023-05-24 02:10:37 [INFO] loss_function: mse
2023-05-24 02:10:37 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:10:37 [INFO] metric: mae
2023-05-24 02:10:37 [INFO] shuffle: True
2023-05-24 02:10:37 [INFO] epochs: 2
2023-05-24 02:10:37 [INFO] batch_size: 512
2023-05-24 02:10:37 [INFO] verbose: 2
2023-05-24 02:10:37 [INFO] TensorBoard_log_path: logs
2023-05-24 02:10:37 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:10:37 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:10:37 [INFO] EarlyStopping_patience: 15
2023-05-24 02:10:37 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:10:37 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:10:37 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:10:37 [INFO] log_path: logs/training.log
2023-05-24 02:10:37 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:10:37 [INFO] Training in progress
2023-05-24 02:10:38 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 02:10:39 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 02:10:40 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-24 02:10:40 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-24 02:10:40 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:19:59 [INFO] Experiment for mlp test begins at 2023/05/24 02:19:59
2023-05-24 02:19:59 [INFO] Config file contents:
2023-05-24 02:19:59 [INFO] name: mlp test
2023-05-24 02:19:59 [INFO] activation: relu
2023-05-24 02:19:59 [INFO] activation_last_layer: relu
2023-05-24 02:19:59 [INFO] loss_function: mse
2023-05-24 02:19:59 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:19:59 [INFO] metric: mae
2023-05-24 02:19:59 [INFO] shuffle: True
2023-05-24 02:19:59 [INFO] epochs: 2
2023-05-24 02:19:59 [INFO] batch_size: 512
2023-05-24 02:19:59 [INFO] verbose: 2
2023-05-24 02:19:59 [INFO] TensorBoard_log_path: logs
2023-05-24 02:19:59 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:19:59 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:19:59 [INFO] EarlyStopping_patience: 15
2023-05-24 02:19:59 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:19:59 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:19:59 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:19:59 [INFO] log_path: logs/training.log
2023-05-24 02:19:59 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:19:59 [INFO] Training in progress
2023-05-24 02:20:00 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 02:20:01 [DEBUG] Epoch 0 - {'loss': '34595451961344.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 02:20:02 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707967.250000', 'lr': '0.001000'} 
2023-05-24 02:20:02 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 02:20:02 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:36:03 [INFO] Experiment for mlp test begins at 2023/05/24 02:36:03
2023-05-24 02:36:03 [INFO] Config file contents:
2023-05-24 02:36:03 [INFO] name: mlp test
2023-05-24 02:36:03 [INFO] activation: relu
2023-05-24 02:36:03 [INFO] activation_last_layer: relu
2023-05-24 02:36:03 [INFO] loss_function: mse
2023-05-24 02:36:03 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:36:03 [INFO] metric: mae
2023-05-24 02:36:03 [INFO] shuffle: True
2023-05-24 02:36:03 [INFO] epochs: 2
2023-05-24 02:36:03 [INFO] batch_size: 512
2023-05-24 02:36:03 [INFO] verbose: 2
2023-05-24 02:36:03 [INFO] TensorBoard_log_path: logs
2023-05-24 02:36:03 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:36:03 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:36:03 [INFO] EarlyStopping_patience: 15
2023-05-24 02:36:03 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:36:03 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:36:03 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:36:03 [INFO] log_path: logs/training.log
2023-05-24 02:36:03 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:36:03 [INFO] Training in progress
2023-05-24 02:36:03 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 02:36:05 [DEBUG] Epoch 0 - {'loss': '34595451961344.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 02:36:06 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 02:36:06 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 02:36:06 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:38:49 [INFO] Experiment for mlp test begins at 2023/05/24 02:38:49
2023-05-24 02:38:49 [INFO] Config file contents:
2023-05-24 02:38:49 [INFO] name: mlp test
2023-05-24 02:38:49 [INFO] activation: relu
2023-05-24 02:38:49 [INFO] activation_last_layer: relu
2023-05-24 02:38:49 [INFO] loss_function: mse
2023-05-24 02:38:49 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:38:49 [INFO] metric: mae
2023-05-24 02:38:49 [INFO] shuffle: True
2023-05-24 02:38:49 [INFO] epochs: 2
2023-05-24 02:38:49 [INFO] batch_size: 512
2023-05-24 02:38:49 [INFO] verbose: 2
2023-05-24 02:38:49 [INFO] TensorBoard_log_path: logs
2023-05-24 02:38:49 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:38:49 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:38:49 [INFO] EarlyStopping_patience: 15
2023-05-24 02:38:49 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:38:49 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:38:49 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:38:49 [INFO] log_path: logs/training.log
2023-05-24 02:38:49 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:38:49 [INFO] Training in progress
2023-05-24 02:38:50 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 02:38:51 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 02:38:53 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 02:38:53 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-24 02:38:53 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:40:59 [INFO] Experiment for mlp test begins at 2023/05/24 02:40:59
2023-05-24 02:40:59 [INFO] Config file contents:
2023-05-24 02:40:59 [INFO] name: mlp test
2023-05-24 02:40:59 [INFO] activation: relu
2023-05-24 02:40:59 [INFO] activation_last_layer: relu
2023-05-24 02:40:59 [INFO] loss_function: mse
2023-05-24 02:40:59 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:40:59 [INFO] metric: mae
2023-05-24 02:40:59 [INFO] shuffle: True
2023-05-24 02:40:59 [INFO] epochs: 2
2023-05-24 02:40:59 [INFO] batch_size: 512
2023-05-24 02:40:59 [INFO] verbose: 2
2023-05-24 02:40:59 [INFO] TensorBoard_log_path: logs
2023-05-24 02:40:59 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:40:59 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:40:59 [INFO] EarlyStopping_patience: 15
2023-05-24 02:40:59 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:40:59 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:40:59 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:40:59 [INFO] log_path: logs/training.log
2023-05-24 02:40:59 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:40:59 [INFO] Training in progress
2023-05-24 02:41:00 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.
2023-05-24 02:41:01 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 02:41:02 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-24 02:41:02 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-24 02:41:02 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:51:14 [INFO] Experiment for mlp test begins at 2023/05/24 02:51:14
2023-05-24 02:51:14 [INFO] Config file contents:
2023-05-24 02:51:14 [INFO] name: mlp test
2023-05-24 02:51:14 [INFO] activation: relu
2023-05-24 02:51:14 [INFO] activation_last_layer: relu
2023-05-24 02:51:14 [INFO] loss_function: mse
2023-05-24 02:51:14 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:51:14 [INFO] metric: mae
2023-05-24 02:51:14 [INFO] shuffle: True
2023-05-24 02:51:14 [INFO] epochs: 2
2023-05-24 02:51:14 [INFO] batch_size: 512
2023-05-24 02:51:14 [INFO] verbose: 2
2023-05-24 02:51:14 [INFO] TensorBoard_log_path: logs
2023-05-24 02:51:14 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:51:14 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:51:14 [INFO] EarlyStopping_patience: 15
2023-05-24 02:51:14 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:51:14 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:51:14 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:51:14 [INFO] log_path: logs/training.log
2023-05-24 02:51:14 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:51:14 [INFO] Training in progress
2023-05-24 02:51:15 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 02:51:16 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 02:51:17 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 02:51:17 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-24 02:51:17 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:52:34 [INFO] Experiment for mlp test begins at 2023/05/24 02:52:34
2023-05-24 02:52:34 [INFO] Config file contents:
2023-05-24 02:52:34 [INFO] name: mlp test
2023-05-24 02:52:34 [INFO] activation: relu
2023-05-24 02:52:34 [INFO] activation_last_layer: relu
2023-05-24 02:52:34 [INFO] loss_function: mse
2023-05-24 02:52:34 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:52:34 [INFO] metric: mae
2023-05-24 02:52:34 [INFO] shuffle: True
2023-05-24 02:52:34 [INFO] epochs: 2
2023-05-24 02:52:34 [INFO] batch_size: 512
2023-05-24 02:52:34 [INFO] verbose: 2
2023-05-24 02:52:34 [INFO] TensorBoard_log_path: logs
2023-05-24 02:52:34 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:52:34 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:52:34 [INFO] EarlyStopping_patience: 15
2023-05-24 02:52:34 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:52:34 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:52:34 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:52:34 [INFO] log_path: logs/training.log
2023-05-24 02:52:34 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:52:34 [INFO] Training in progress
2023-05-24 02:52:35 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 02:52:36 [DEBUG] Epoch 0 - {'loss': '34595430989824.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-24 02:52:37 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-24 02:52:37 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-24 02:52:37 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:54:08 [INFO] Experiment for mlp test begins at 2023/05/24 02:54:08
2023-05-24 02:54:08 [INFO] Config file contents:
2023-05-24 02:54:08 [INFO] name: mlp test
2023-05-24 02:54:08 [INFO] activation: relu
2023-05-24 02:54:08 [INFO] activation_last_layer: relu
2023-05-24 02:54:08 [INFO] loss_function: mse
2023-05-24 02:54:08 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:54:08 [INFO] metric: mae
2023-05-24 02:54:08 [INFO] shuffle: True
2023-05-24 02:54:08 [INFO] epochs: 2
2023-05-24 02:54:08 [INFO] batch_size: 512
2023-05-24 02:54:08 [INFO] verbose: 2
2023-05-24 02:54:08 [INFO] TensorBoard_log_path: logs
2023-05-24 02:54:08 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:54:08 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:54:08 [INFO] EarlyStopping_patience: 15
2023-05-24 02:54:08 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:54:08 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:54:08 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:54:08 [INFO] log_path: logs/training.log
2023-05-24 02:54:08 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:54:09 [INFO] Training in progress
2023-05-24 02:54:09 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 02:54:10 [DEBUG] Epoch 0 - {'loss': '34595449864192.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-24 02:54:12 [DEBUG] Epoch 1 - {'loss': '34595456155648.000000', 'mae': '3707967.000000', 'lr': '0.001000'} 
2023-05-24 02:54:12 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-24 02:54:12 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:56:14 [INFO] Experiment for mlp test begins at 2023/05/24 02:56:14
2023-05-24 02:56:14 [INFO] Config file contents:
2023-05-24 02:56:14 [INFO] name: mlp test
2023-05-24 02:56:14 [INFO] activation: relu
2023-05-24 02:56:14 [INFO] activation_last_layer: relu
2023-05-24 02:56:14 [INFO] loss_function: mse
2023-05-24 02:56:14 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:56:14 [INFO] metric: mae
2023-05-24 02:56:14 [INFO] shuffle: True
2023-05-24 02:56:14 [INFO] epochs: 2
2023-05-24 02:56:14 [INFO] batch_size: 512
2023-05-24 02:56:14 [INFO] verbose: 2
2023-05-24 02:56:14 [INFO] TensorBoard_log_path: logs
2023-05-24 02:56:14 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:56:14 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:56:14 [INFO] EarlyStopping_patience: 15
2023-05-24 02:56:14 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:56:14 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:56:14 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:56:14 [INFO] log_path: logs/training.log
2023-05-24 02:56:14 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:56:14 [INFO] Training in progress
2023-05-24 02:56:15 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 02:56:16 [DEBUG] Epoch 0 - {'loss': '34595433086976.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 02:56:17 [DEBUG] Epoch 1 - {'loss': '34595441475584.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 02:56:17 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-24 02:56:17 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:57:49 [INFO] Experiment for mlp test begins at 2023/05/24 02:57:49
2023-05-24 02:57:49 [INFO] Config file contents:
2023-05-24 02:57:49 [INFO] name: mlp test
2023-05-24 02:57:49 [INFO] activation: relu
2023-05-24 02:57:49 [INFO] activation_last_layer: relu
2023-05-24 02:57:49 [INFO] loss_function: mse
2023-05-24 02:57:49 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:57:49 [INFO] metric: mae
2023-05-24 02:57:49 [INFO] shuffle: True
2023-05-24 02:57:49 [INFO] epochs: 2
2023-05-24 02:57:49 [INFO] batch_size: 512
2023-05-24 02:57:49 [INFO] verbose: 2
2023-05-24 02:57:49 [INFO] TensorBoard_log_path: logs
2023-05-24 02:57:49 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:57:49 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:57:49 [INFO] EarlyStopping_patience: 15
2023-05-24 02:57:49 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:57:49 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:57:49 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:57:49 [INFO] log_path: logs/training.log
2023-05-24 02:57:49 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:57:49 [INFO] Training in progress
2023-05-24 02:57:50 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-24 02:57:51 [DEBUG] Epoch 0 - {'loss': '34595449864192.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 02:57:53 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707971.000000', 'lr': '0.001000'} 
2023-05-24 02:57:53 [INFO] Training finished, elapsed time: 3.11 seconds
2023-05-24 02:57:53 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:58:36 [INFO] Experiment for mlp test begins at 2023/05/24 02:58:36
2023-05-24 02:58:36 [INFO] Config file contents:
2023-05-24 02:58:36 [INFO] name: mlp test
2023-05-24 02:58:36 [INFO] activation: relu
2023-05-24 02:58:36 [INFO] activation_last_layer: relu
2023-05-24 02:58:36 [INFO] loss_function: mse
2023-05-24 02:58:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:58:36 [INFO] metric: mae
2023-05-24 02:58:36 [INFO] shuffle: True
2023-05-24 02:58:36 [INFO] epochs: 2
2023-05-24 02:58:36 [INFO] batch_size: 512
2023-05-24 02:58:36 [INFO] verbose: 2
2023-05-24 02:58:36 [INFO] TensorBoard_log_path: logs
2023-05-24 02:58:36 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:58:36 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:58:36 [INFO] EarlyStopping_patience: 15
2023-05-24 02:58:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:58:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:58:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:58:36 [INFO] log_path: logs/training.log
2023-05-24 02:58:36 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:58:36 [INFO] Training in progress
2023-05-24 02:58:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 02:58:38 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 02:58:39 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 02:58:39 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-24 02:58:39 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 02:59:10 [INFO] Experiment for mlp test begins at 2023/05/24 02:59:10
2023-05-24 02:59:10 [INFO] Config file contents:
2023-05-24 02:59:10 [INFO] name: mlp test
2023-05-24 02:59:10 [INFO] activation: relu
2023-05-24 02:59:10 [INFO] activation_last_layer: relu
2023-05-24 02:59:10 [INFO] loss_function: mse
2023-05-24 02:59:10 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 02:59:10 [INFO] metric: mae
2023-05-24 02:59:10 [INFO] shuffle: True
2023-05-24 02:59:10 [INFO] epochs: 2
2023-05-24 02:59:10 [INFO] batch_size: 512
2023-05-24 02:59:10 [INFO] verbose: 2
2023-05-24 02:59:10 [INFO] TensorBoard_log_path: logs
2023-05-24 02:59:10 [INFO] TensorBoard_hist_freq: 1
2023-05-24 02:59:10 [INFO] EarlyStopping_monitor: loss
2023-05-24 02:59:10 [INFO] EarlyStopping_patience: 15
2023-05-24 02:59:10 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 02:59:10 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 02:59:10 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 02:59:10 [INFO] log_path: logs/training.log
2023-05-24 02:59:10 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 02:59:10 [INFO] Training in progress
2023-05-24 02:59:10 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 02:59:12 [DEBUG] Epoch 0 - {'loss': '34595445669888.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-24 02:59:13 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 02:59:13 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-24 02:59:13 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 03:01:00 [INFO] Experiment for mlp test begins at 2023/05/24 03:01:00
2023-05-24 03:01:00 [INFO] Config file contents:
2023-05-24 03:01:00 [INFO] name: mlp test
2023-05-24 03:01:00 [INFO] activation: relu
2023-05-24 03:01:00 [INFO] activation_last_layer: relu
2023-05-24 03:01:00 [INFO] loss_function: mse
2023-05-24 03:01:00 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 03:01:00 [INFO] metric: mae
2023-05-24 03:01:00 [INFO] shuffle: True
2023-05-24 03:01:00 [INFO] epochs: 2
2023-05-24 03:01:00 [INFO] batch_size: 512
2023-05-24 03:01:00 [INFO] verbose: 2
2023-05-24 03:01:00 [INFO] TensorBoard_log_path: logs
2023-05-24 03:01:00 [INFO] TensorBoard_hist_freq: 1
2023-05-24 03:01:00 [INFO] EarlyStopping_monitor: loss
2023-05-24 03:01:00 [INFO] EarlyStopping_patience: 15
2023-05-24 03:01:00 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 03:01:00 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 03:01:00 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 03:01:00 [INFO] log_path: logs/training.log
2023-05-24 03:01:00 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 03:01:00 [INFO] Training in progress
2023-05-24 03:01:00 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 03:01:02 [DEBUG] Epoch 0 - {'loss': '34595460349952.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-24 03:01:03 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 03:01:03 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-24 03:01:03 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 03:01:25 [INFO] Experiment for mlp test begins at 2023/05/24 03:01:25
2023-05-24 03:01:25 [INFO] Config file contents:
2023-05-24 03:01:25 [INFO] name: mlp test
2023-05-24 03:01:25 [INFO] activation: relu
2023-05-24 03:01:25 [INFO] activation_last_layer: relu
2023-05-24 03:01:25 [INFO] loss_function: mse
2023-05-24 03:01:25 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 03:01:25 [INFO] metric: mae
2023-05-24 03:01:25 [INFO] shuffle: True
2023-05-24 03:01:25 [INFO] epochs: 2
2023-05-24 03:01:25 [INFO] batch_size: 512
2023-05-24 03:01:25 [INFO] verbose: 2
2023-05-24 03:01:25 [INFO] TensorBoard_log_path: logs
2023-05-24 03:01:25 [INFO] TensorBoard_hist_freq: 1
2023-05-24 03:01:25 [INFO] EarlyStopping_monitor: loss
2023-05-24 03:01:25 [INFO] EarlyStopping_patience: 15
2023-05-24 03:01:25 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 03:01:25 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 03:01:25 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 03:01:25 [INFO] log_path: logs/training.log
2023-05-24 03:01:25 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 03:01:25 [INFO] Training in progress
2023-05-24 03:01:26 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 03:01:27 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-24 03:01:28 [DEBUG] Epoch 1 - {'loss': '34595433086976.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 03:01:28 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 03:01:28 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 03:02:17 [INFO] Experiment for mlp test begins at 2023/05/24 03:02:17
2023-05-24 03:02:17 [INFO] Config file contents:
2023-05-24 03:02:17 [INFO] name: mlp test
2023-05-24 03:02:17 [INFO] activation: relu
2023-05-24 03:02:17 [INFO] activation_last_layer: relu
2023-05-24 03:02:17 [INFO] loss_function: mse
2023-05-24 03:02:17 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 03:02:17 [INFO] metric: mae
2023-05-24 03:02:17 [INFO] shuffle: True
2023-05-24 03:02:17 [INFO] epochs: 2
2023-05-24 03:02:17 [INFO] batch_size: 512
2023-05-24 03:02:17 [INFO] verbose: 2
2023-05-24 03:02:17 [INFO] TensorBoard_log_path: logs
2023-05-24 03:02:17 [INFO] TensorBoard_hist_freq: 1
2023-05-24 03:02:17 [INFO] EarlyStopping_monitor: loss
2023-05-24 03:02:17 [INFO] EarlyStopping_patience: 15
2023-05-24 03:02:17 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 03:02:17 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 03:02:17 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 03:02:17 [INFO] log_path: logs/training.log
2023-05-24 03:02:17 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 03:02:17 [INFO] Training in progress
2023-05-24 03:02:18 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 03:02:19 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 03:02:20 [DEBUG] Epoch 1 - {'loss': '34595433086976.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 03:02:20 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 03:02:20 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 22:01:39 [INFO] Experiment for mlp test begins at 2023/05/24 22:01:39
2023-05-24 22:01:39 [INFO] Config file contents:
2023-05-24 22:01:39 [INFO] name: mlp test
2023-05-24 22:01:39 [INFO] activation: relu
2023-05-24 22:01:39 [INFO] activation_last_layer: relu
2023-05-24 22:01:39 [INFO] loss_function: mse
2023-05-24 22:01:39 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 22:01:39 [INFO] metric: mae
2023-05-24 22:01:39 [INFO] shuffle: True
2023-05-24 22:01:39 [INFO] epochs: 2
2023-05-24 22:01:39 [INFO] batch_size: 512
2023-05-24 22:01:39 [INFO] verbose: 2
2023-05-24 22:01:39 [INFO] TensorBoard_log_path: logs
2023-05-24 22:01:39 [INFO] TensorBoard_hist_freq: 1
2023-05-24 22:01:39 [INFO] EarlyStopping_monitor: loss
2023-05-24 22:01:39 [INFO] EarlyStopping_patience: 15
2023-05-24 22:01:39 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 22:01:39 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 22:01:39 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 22:01:39 [INFO] log_path: logs/training.log
2023-05-24 22:01:39 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 22:01:39 [INFO] Training in progress
2023-05-24 22:01:40 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 22:01:41 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-24 22:01:42 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-24 22:01:42 [INFO] Training finished, elapsed time: 3.12 seconds
2023-05-24 22:01:42 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 22:03:53 [INFO] Experiment for mlp test begins at 2023/05/24 22:03:53
2023-05-24 22:03:53 [INFO] Config file contents:
2023-05-24 22:03:53 [INFO] name: mlp test
2023-05-24 22:03:53 [INFO] activation: relu
2023-05-24 22:03:53 [INFO] activation_last_layer: relu
2023-05-24 22:03:53 [INFO] loss_function: mse
2023-05-24 22:03:53 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 22:03:53 [INFO] metric: mae
2023-05-24 22:03:53 [INFO] shuffle: True
2023-05-24 22:03:53 [INFO] epochs: 2
2023-05-24 22:03:53 [INFO] batch_size: 512
2023-05-24 22:03:53 [INFO] verbose: 2
2023-05-24 22:03:53 [INFO] TensorBoard_log_path: logs
2023-05-24 22:03:53 [INFO] TensorBoard_hist_freq: 1
2023-05-24 22:03:53 [INFO] EarlyStopping_monitor: loss
2023-05-24 22:03:53 [INFO] EarlyStopping_patience: 15
2023-05-24 22:03:53 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 22:03:53 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 22:03:53 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 22:03:53 [INFO] log_path: logs/training.log
2023-05-24 22:03:53 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 22:03:53 [INFO] Training in progress
2023-05-24 22:03:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 22:03:55 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 22:03:56 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-24 22:03:56 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 22:03:56 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:02:19 [INFO] Experiment for mlp test begins at 2023/05/24 23:02:19
2023-05-24 23:02:19 [INFO] Config file contents:
2023-05-24 23:02:19 [INFO] name: mlp test
2023-05-24 23:02:19 [INFO] activation: relu
2023-05-24 23:02:19 [INFO] activation_last_layer: relu
2023-05-24 23:02:19 [INFO] loss_function: mse
2023-05-24 23:02:19 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:02:19 [INFO] metric: mae
2023-05-24 23:02:19 [INFO] shuffle: True
2023-05-24 23:02:19 [INFO] epochs: 2
2023-05-24 23:02:19 [INFO] batch_size: 512
2023-05-24 23:02:19 [INFO] verbose: 2
2023-05-24 23:02:19 [INFO] TensorBoard_log_path: logs
2023-05-24 23:02:19 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:02:19 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:02:19 [INFO] EarlyStopping_patience: 15
2023-05-24 23:02:19 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:02:19 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:02:19 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:02:19 [INFO] log_path: logs/training.log
2023-05-24 23:02:19 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:02:19 [INFO] Training in progress
2023-05-24 23:02:20 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:02:21 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707970.250000', 'lr': '0.001000'} 
2023-05-24 23:02:22 [DEBUG] Epoch 1 - {'loss': '34595441475584.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 23:02:22 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-24 23:02:22 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:03:04 [INFO] Experiment for mlp test begins at 2023/05/24 23:03:04
2023-05-24 23:03:04 [INFO] Config file contents:
2023-05-24 23:03:04 [INFO] name: mlp test
2023-05-24 23:03:04 [INFO] activation: relu
2023-05-24 23:03:04 [INFO] activation_last_layer: relu
2023-05-24 23:03:04 [INFO] loss_function: mse
2023-05-24 23:03:04 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:03:04 [INFO] metric: mae
2023-05-24 23:03:04 [INFO] shuffle: True
2023-05-24 23:03:04 [INFO] epochs: 2
2023-05-24 23:03:04 [INFO] batch_size: 512
2023-05-24 23:03:04 [INFO] verbose: 2
2023-05-24 23:03:04 [INFO] TensorBoard_log_path: logs
2023-05-24 23:03:04 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:03:04 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:03:04 [INFO] EarlyStopping_patience: 15
2023-05-24 23:03:04 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:03:04 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:03:04 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:03:04 [INFO] log_path: logs/training.log
2023-05-24 23:03:04 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:03:04 [INFO] Training in progress
2023-05-24 23:03:04 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:03:06 [DEBUG] Epoch 0 - {'loss': '34595449864192.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:03:07 [DEBUG] Epoch 1 - {'loss': '34595439378432.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 23:03:07 [INFO] Training finished, elapsed time: 3.11 seconds
2023-05-24 23:03:07 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:05:13 [INFO] Experiment for mlp test begins at 2023/05/24 23:05:13
2023-05-24 23:05:13 [INFO] Config file contents:
2023-05-24 23:05:13 [INFO] name: mlp test
2023-05-24 23:05:13 [INFO] activation: relu
2023-05-24 23:05:13 [INFO] activation_last_layer: relu
2023-05-24 23:05:13 [INFO] loss_function: mse
2023-05-24 23:05:13 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:05:13 [INFO] metric: mae
2023-05-24 23:05:13 [INFO] shuffle: True
2023-05-24 23:05:13 [INFO] epochs: 2
2023-05-24 23:05:13 [INFO] batch_size: 512
2023-05-24 23:05:13 [INFO] verbose: 2
2023-05-24 23:05:13 [INFO] TensorBoard_log_path: logs
2023-05-24 23:05:13 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:05:13 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:05:13 [INFO] EarlyStopping_patience: 15
2023-05-24 23:05:13 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:05:13 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:05:13 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:05:13 [INFO] log_path: logs/training.log
2023-05-24 23:05:13 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:05:13 [INFO] Training in progress
2023-05-24 23:05:14 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:05:15 [DEBUG] Epoch 0 - {'loss': '34595445669888.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:05:16 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 23:05:16 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-24 23:05:16 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:34:42 [INFO] Experiment for mlp test begins at 2023/05/24 23:34:42
2023-05-24 23:34:42 [INFO] Config file contents:
2023-05-24 23:34:42 [INFO] name: mlp test
2023-05-24 23:34:42 [INFO] activation: relu
2023-05-24 23:34:42 [INFO] activation_last_layer: relu
2023-05-24 23:34:42 [INFO] loss_function: mse
2023-05-24 23:34:42 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:34:42 [INFO] metric: mae
2023-05-24 23:34:42 [INFO] shuffle: True
2023-05-24 23:34:42 [INFO] epochs: 2
2023-05-24 23:34:42 [INFO] batch_size: 512
2023-05-24 23:34:42 [INFO] verbose: 2
2023-05-24 23:34:42 [INFO] TensorBoard_log_path: logs
2023-05-24 23:34:42 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:34:42 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:34:42 [INFO] EarlyStopping_patience: 15
2023-05-24 23:34:42 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:34:42 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:34:42 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:34:42 [INFO] log_path: logs/training.log
2023-05-24 23:34:42 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:34:42 [INFO] Training in progress
2023-05-24 23:34:43 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0045s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-24 23:34:44 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-24 23:34:46 [DEBUG] Epoch 1 - {'loss': '34595449864192.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 23:34:46 [INFO] Training finished, elapsed time: 3.07 seconds
2023-05-24 23:34:46 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:38:03 [INFO] Experiment for mlp test begins at 2023/05/24 23:38:03
2023-05-24 23:38:03 [INFO] Config file contents:
2023-05-24 23:38:03 [INFO] name: mlp test
2023-05-24 23:38:03 [INFO] activation: relu
2023-05-24 23:38:03 [INFO] activation_last_layer: relu
2023-05-24 23:38:03 [INFO] loss_function: mse
2023-05-24 23:38:03 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:38:03 [INFO] metric: mae
2023-05-24 23:38:03 [INFO] shuffle: True
2023-05-24 23:38:03 [INFO] epochs: 2
2023-05-24 23:38:03 [INFO] batch_size: 512
2023-05-24 23:38:03 [INFO] verbose: 2
2023-05-24 23:38:03 [INFO] TensorBoard_log_path: logs
2023-05-24 23:38:03 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:38:03 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:38:03 [INFO] EarlyStopping_patience: 15
2023-05-24 23:38:03 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:38:03 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:38:03 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:38:03 [INFO] log_path: logs/training.log
2023-05-24 23:38:03 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:38:03 [INFO] Training in progress
2023-05-24 23:38:04 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 23:38:05 [DEBUG] Epoch 0 - {'loss': '34595449864192.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 23:38:06 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 23:38:06 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 23:38:06 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:38:38 [INFO] Experiment for mlp test begins at 2023/05/24 23:38:38
2023-05-24 23:38:38 [INFO] Config file contents:
2023-05-24 23:38:38 [INFO] name: mlp test
2023-05-24 23:38:38 [INFO] activation: relu
2023-05-24 23:38:38 [INFO] activation_last_layer: relu
2023-05-24 23:38:38 [INFO] loss_function: mse
2023-05-24 23:38:38 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:38:38 [INFO] metric: mae
2023-05-24 23:38:38 [INFO] shuffle: True
2023-05-24 23:38:38 [INFO] epochs: 2
2023-05-24 23:38:38 [INFO] batch_size: 512
2023-05-24 23:38:38 [INFO] verbose: 2
2023-05-24 23:38:38 [INFO] TensorBoard_log_path: logs
2023-05-24 23:38:38 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:38:38 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:38:38 [INFO] EarlyStopping_patience: 15
2023-05-24 23:38:38 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:38:38 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:38:38 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:38:38 [INFO] log_path: logs/training.log
2023-05-24 23:38:38 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:38:38 [INFO] Training in progress
2023-05-24 23:38:38 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:38:40 [DEBUG] Epoch 0 - {'loss': '34595456155648.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:38:41 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:38:41 [INFO] Training finished, elapsed time: 3.07 seconds
2023-05-24 23:38:41 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:42:36 [INFO] Experiment for mlp test begins at 2023/05/24 23:42:36
2023-05-24 23:42:36 [INFO] Config file contents:
2023-05-24 23:42:36 [INFO] name: mlp test
2023-05-24 23:42:36 [INFO] activation: relu
2023-05-24 23:42:36 [INFO] activation_last_layer: relu
2023-05-24 23:42:36 [INFO] loss_function: mse
2023-05-24 23:42:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:42:36 [INFO] metric: mae
2023-05-24 23:42:36 [INFO] shuffle: True
2023-05-24 23:42:36 [INFO] epochs: 2
2023-05-24 23:42:36 [INFO] batch_size: 512
2023-05-24 23:42:36 [INFO] verbose: 2
2023-05-24 23:42:36 [INFO] TensorBoard_log_path: logs
2023-05-24 23:42:36 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:42:36 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:42:36 [INFO] EarlyStopping_patience: 15
2023-05-24 23:42:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:42:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:42:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:42:36 [INFO] log_path: logs/training.log
2023-05-24 23:42:36 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:42:36 [INFO] Training in progress
2023-05-24 23:42:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:42:38 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 23:42:39 [DEBUG] Epoch 1 - {'loss': '34595451961344.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-24 23:42:39 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-24 23:42:39 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:43:53 [INFO] Experiment for mlp test begins at 2023/05/24 23:43:53
2023-05-24 23:43:53 [INFO] Config file contents:
2023-05-24 23:43:53 [INFO] name: mlp test
2023-05-24 23:43:53 [INFO] activation: relu
2023-05-24 23:43:53 [INFO] activation_last_layer: relu
2023-05-24 23:43:53 [INFO] loss_function: mse
2023-05-24 23:43:53 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:43:53 [INFO] metric: mae
2023-05-24 23:43:53 [INFO] shuffle: True
2023-05-24 23:43:53 [INFO] epochs: 2
2023-05-24 23:43:53 [INFO] batch_size: 512
2023-05-24 23:43:53 [INFO] verbose: 2
2023-05-24 23:43:53 [INFO] TensorBoard_log_path: logs
2023-05-24 23:43:53 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:43:53 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:43:53 [INFO] EarlyStopping_patience: 15
2023-05-24 23:43:53 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:43:53 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:43:53 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:43:53 [INFO] log_path: logs/training.log
2023-05-24 23:43:53 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:43:53 [INFO] Training in progress
2023-05-24 23:43:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 23:43:55 [DEBUG] Epoch 0 - {'loss': '34595451961344.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-24 23:43:56 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:43:56 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 23:43:56 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:44:37 [INFO] Experiment for mlp test begins at 2023/05/24 23:44:37
2023-05-24 23:44:37 [INFO] Config file contents:
2023-05-24 23:44:37 [INFO] name: mlp test
2023-05-24 23:44:37 [INFO] activation: relu
2023-05-24 23:44:37 [INFO] activation_last_layer: relu
2023-05-24 23:44:37 [INFO] loss_function: mse
2023-05-24 23:44:37 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:44:37 [INFO] metric: mae
2023-05-24 23:44:37 [INFO] shuffle: True
2023-05-24 23:44:37 [INFO] epochs: 2
2023-05-24 23:44:37 [INFO] batch_size: 512
2023-05-24 23:44:37 [INFO] verbose: 2
2023-05-24 23:44:37 [INFO] TensorBoard_log_path: logs
2023-05-24 23:44:37 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:44:37 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:44:37 [INFO] EarlyStopping_patience: 15
2023-05-24 23:44:37 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:44:37 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:44:37 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:44:37 [INFO] log_path: logs/training.log
2023-05-24 23:44:37 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:44:37 [INFO] Training in progress
2023-05-24 23:44:38 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 23:44:39 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707967.000000', 'lr': '0.001000'} 
2023-05-24 23:44:40 [DEBUG] Epoch 1 - {'loss': '34595439378432.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-24 23:44:40 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-24 23:44:40 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:46:35 [INFO] Experiment for mlp test begins at 2023/05/24 23:46:35
2023-05-24 23:46:35 [INFO] Config file contents:
2023-05-24 23:46:35 [INFO] name: mlp test
2023-05-24 23:46:35 [INFO] activation: relu
2023-05-24 23:46:35 [INFO] activation_last_layer: relu
2023-05-24 23:46:35 [INFO] loss_function: mse
2023-05-24 23:46:35 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:46:35 [INFO] metric: mae
2023-05-24 23:46:35 [INFO] shuffle: True
2023-05-24 23:46:35 [INFO] epochs: 2
2023-05-24 23:46:35 [INFO] batch_size: 512
2023-05-24 23:46:35 [INFO] verbose: 2
2023-05-24 23:46:35 [INFO] TensorBoard_log_path: logs
2023-05-24 23:46:35 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:46:35 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:46:35 [INFO] EarlyStopping_patience: 15
2023-05-24 23:46:35 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:46:35 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:46:35 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:46:35 [INFO] log_path: logs/training.log
2023-05-24 23:46:35 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:46:36 [INFO] Training in progress
2023-05-24 23:46:36 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-24 23:46:38 [DEBUG] Epoch 0 - {'loss': '34595445669888.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-24 23:46:39 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-24 23:46:39 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-24 23:46:39 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:49:29 [INFO] Experiment for mlp test begins at 2023/05/24 23:49:29
2023-05-24 23:49:29 [INFO] Config file contents:
2023-05-24 23:49:29 [INFO] name: mlp test
2023-05-24 23:49:29 [INFO] activation: relu
2023-05-24 23:49:29 [INFO] activation_last_layer: relu
2023-05-24 23:49:29 [INFO] loss_function: mse
2023-05-24 23:49:29 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:49:29 [INFO] metric: mae
2023-05-24 23:49:29 [INFO] shuffle: True
2023-05-24 23:49:29 [INFO] epochs: 2
2023-05-24 23:49:29 [INFO] batch_size: 512
2023-05-24 23:49:29 [INFO] verbose: 2
2023-05-24 23:49:29 [INFO] TensorBoard_log_path: logs
2023-05-24 23:49:29 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:49:29 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:49:29 [INFO] EarlyStopping_patience: 15
2023-05-24 23:49:29 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:49:29 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:49:29 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:49:29 [INFO] log_path: logs/training.log
2023-05-24 23:49:29 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:49:29 [INFO] Training in progress
2023-05-24 23:49:29 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-24 23:49:31 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:49:32 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-24 23:49:32 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-24 23:49:32 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:53:39 [INFO] Experiment for mlp test begins at 2023/05/24 23:53:39
2023-05-24 23:53:39 [INFO] Config file contents:
2023-05-24 23:53:39 [INFO] name: mlp test
2023-05-24 23:53:39 [INFO] activation: relu
2023-05-24 23:53:39 [INFO] activation_last_layer: relu
2023-05-24 23:53:39 [INFO] loss_function: mse
2023-05-24 23:53:39 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:53:39 [INFO] metric: mae
2023-05-24 23:53:39 [INFO] shuffle: True
2023-05-24 23:53:39 [INFO] epochs: 2
2023-05-24 23:53:39 [INFO] batch_size: 512
2023-05-24 23:53:39 [INFO] verbose: 2
2023-05-24 23:53:39 [INFO] TensorBoard_log_path: logs
2023-05-24 23:53:39 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:53:39 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:53:39 [INFO] EarlyStopping_patience: 15
2023-05-24 23:53:39 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:53:39 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:53:39 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:53:39 [INFO] log_path: logs/training.log
2023-05-24 23:53:39 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:53:39 [INFO] Training in progress
2023-05-24 23:53:40 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:53:41 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-24 23:53:42 [DEBUG] Epoch 1 - {'loss': '34595451961344.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-24 23:53:42 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-24 23:53:42 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-24 23:58:36 [INFO] Experiment for mlp test begins at 2023/05/24 23:58:36
2023-05-24 23:58:36 [INFO] Config file contents:
2023-05-24 23:58:36 [INFO] name: mlp test
2023-05-24 23:58:36 [INFO] activation: relu
2023-05-24 23:58:36 [INFO] activation_last_layer: relu
2023-05-24 23:58:36 [INFO] loss_function: mse
2023-05-24 23:58:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-24 23:58:36 [INFO] metric: mae
2023-05-24 23:58:36 [INFO] shuffle: True
2023-05-24 23:58:36 [INFO] epochs: 2
2023-05-24 23:58:36 [INFO] batch_size: 512
2023-05-24 23:58:36 [INFO] verbose: 2
2023-05-24 23:58:36 [INFO] TensorBoard_log_path: logs
2023-05-24 23:58:36 [INFO] TensorBoard_hist_freq: 1
2023-05-24 23:58:36 [INFO] EarlyStopping_monitor: loss
2023-05-24 23:58:36 [INFO] EarlyStopping_patience: 15
2023-05-24 23:58:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-24 23:58:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-24 23:58:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-24 23:58:36 [INFO] log_path: logs/training.log
2023-05-24 23:58:36 [INFO] save_path: models/pretrain_mlp.h5
2023-05-24 23:58:36 [INFO] Training in progress
2023-05-24 23:58:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-24 23:58:38 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:58:39 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-24 23:58:39 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-24 23:58:39 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 00:06:36 [INFO] Experiment for mlp test begins at 2023/05/25 00:06:36
2023-05-25 00:06:36 [INFO] Config file contents:
2023-05-25 00:06:36 [INFO] name: mlp test
2023-05-25 00:06:36 [INFO] activation: relu
2023-05-25 00:06:36 [INFO] activation_last_layer: relu
2023-05-25 00:06:36 [INFO] loss_function: mse
2023-05-25 00:06:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 00:06:36 [INFO] metric: mae
2023-05-25 00:06:36 [INFO] shuffle: True
2023-05-25 00:06:36 [INFO] epochs: 2
2023-05-25 00:06:36 [INFO] batch_size: 512
2023-05-25 00:06:36 [INFO] verbose: 2
2023-05-25 00:06:36 [INFO] TensorBoard_log_path: logs
2023-05-25 00:06:36 [INFO] TensorBoard_hist_freq: 1
2023-05-25 00:06:36 [INFO] EarlyStopping_monitor: loss
2023-05-25 00:06:36 [INFO] EarlyStopping_patience: 15
2023-05-25 00:06:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 00:06:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 00:06:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 00:06:36 [INFO] log_path: logs/training.log
2023-05-25 00:06:36 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 00:06:36 [INFO] Training in progress
2023-05-25 00:06:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 00:06:38 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-25 00:06:39 [DEBUG] Epoch 1 - {'loss': '34595439378432.000000', 'mae': '3707967.500000', 'lr': '0.001000'} 
2023-05-25 00:06:39 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-25 00:06:39 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 00:08:18 [INFO] Experiment for mlp test begins at 2023/05/25 00:08:18
2023-05-25 00:08:18 [INFO] Config file contents:
2023-05-25 00:08:18 [INFO] name: mlp test
2023-05-25 00:08:18 [INFO] activation: relu
2023-05-25 00:08:18 [INFO] activation_last_layer: relu
2023-05-25 00:08:18 [INFO] loss_function: mse
2023-05-25 00:08:18 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 00:08:18 [INFO] metric: mae
2023-05-25 00:08:18 [INFO] shuffle: True
2023-05-25 00:08:18 [INFO] epochs: 2
2023-05-25 00:08:18 [INFO] batch_size: 512
2023-05-25 00:08:18 [INFO] verbose: 2
2023-05-25 00:08:18 [INFO] TensorBoard_log_path: logs
2023-05-25 00:08:18 [INFO] TensorBoard_hist_freq: 1
2023-05-25 00:08:18 [INFO] EarlyStopping_monitor: loss
2023-05-25 00:08:18 [INFO] EarlyStopping_patience: 15
2023-05-25 00:08:18 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 00:08:18 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 00:08:18 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 00:08:18 [INFO] log_path: logs/training.log
2023-05-25 00:08:18 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 00:08:18 [INFO] Training in progress
2023-05-25 00:08:19 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 00:08:20 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 00:08:21 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707971.000000', 'lr': '0.001000'} 
2023-05-25 00:08:21 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-25 00:08:21 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 00:09:03 [INFO] Experiment for mlp test begins at 2023/05/25 00:09:03
2023-05-25 00:09:03 [INFO] Config file contents:
2023-05-25 00:09:03 [INFO] name: mlp test
2023-05-25 00:09:03 [INFO] activation: relu
2023-05-25 00:09:03 [INFO] activation_last_layer: relu
2023-05-25 00:09:03 [INFO] loss_function: mse
2023-05-25 00:09:03 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 00:09:03 [INFO] metric: mae
2023-05-25 00:09:03 [INFO] shuffle: True
2023-05-25 00:09:03 [INFO] epochs: 2
2023-05-25 00:09:03 [INFO] batch_size: 512
2023-05-25 00:09:03 [INFO] verbose: 2
2023-05-25 00:09:03 [INFO] TensorBoard_log_path: logs
2023-05-25 00:09:03 [INFO] TensorBoard_hist_freq: 1
2023-05-25 00:09:03 [INFO] EarlyStopping_monitor: loss
2023-05-25 00:09:03 [INFO] EarlyStopping_patience: 15
2023-05-25 00:09:03 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 00:09:03 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 00:09:03 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 00:09:03 [INFO] log_path: logs/training.log
2023-05-25 00:09:03 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 00:09:03 [INFO] Training in progress
2023-05-25 00:09:04 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 00:09:05 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 00:09:06 [DEBUG] Epoch 1 - {'loss': '34595439378432.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 00:09:06 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-25 00:09:06 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 00:09:45 [INFO] Experiment for mlp test begins at 2023/05/25 00:09:45
2023-05-25 00:09:45 [INFO] Config file contents:
2023-05-25 00:09:45 [INFO] name: mlp test
2023-05-25 00:09:45 [INFO] activation: relu
2023-05-25 00:09:45 [INFO] activation_last_layer: relu
2023-05-25 00:09:45 [INFO] loss_function: mse
2023-05-25 00:09:45 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 00:09:45 [INFO] metric: mae
2023-05-25 00:09:45 [INFO] shuffle: True
2023-05-25 00:09:45 [INFO] epochs: 2
2023-05-25 00:09:45 [INFO] batch_size: 512
2023-05-25 00:09:45 [INFO] verbose: 2
2023-05-25 00:09:45 [INFO] TensorBoard_log_path: logs
2023-05-25 00:09:45 [INFO] TensorBoard_hist_freq: 1
2023-05-25 00:09:45 [INFO] EarlyStopping_monitor: loss
2023-05-25 00:09:45 [INFO] EarlyStopping_patience: 15
2023-05-25 00:09:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 00:09:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 00:09:45 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 00:09:45 [INFO] log_path: logs/training.log
2023-05-25 00:09:45 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 00:09:45 [INFO] Training in progress
2023-05-25 00:09:46 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 00:09:47 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 00:09:48 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 00:09:48 [INFO] Training finished, elapsed time: 3.02 seconds
2023-05-25 00:09:48 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:01:29 [INFO] Experiment for mlp test begins at 2023/05/25 04:01:29
2023-05-25 04:01:29 [INFO] Config file contents:
2023-05-25 04:01:29 [INFO] name: mlp test
2023-05-25 04:01:29 [INFO] activation: relu
2023-05-25 04:01:29 [INFO] activation_last_layer: relu
2023-05-25 04:01:29 [INFO] loss_function: mse
2023-05-25 04:01:29 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:01:29 [INFO] metric: mae
2023-05-25 04:01:29 [INFO] shuffle: True
2023-05-25 04:01:29 [INFO] epochs: 2
2023-05-25 04:01:29 [INFO] batch_size: 512
2023-05-25 04:01:29 [INFO] verbose: 2
2023-05-25 04:01:29 [INFO] TensorBoard_log_path: logs
2023-05-25 04:01:29 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:01:29 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:01:29 [INFO] EarlyStopping_patience: 15
2023-05-25 04:01:29 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:01:29 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:01:29 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:01:29 [INFO] log_path: logs/training.log
2023-05-25 04:01:29 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:01:29 [INFO] Training in progress
2023-05-25 04:01:30 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-25 04:01:31 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 04:01:33 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-25 04:01:33 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-25 04:01:33 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:06:00 [INFO] Experiment for mlp test begins at 2023/05/25 04:06:00
2023-05-25 04:06:00 [INFO] Config file contents:
2023-05-25 04:06:00 [INFO] name: mlp test
2023-05-25 04:06:00 [INFO] activation: relu
2023-05-25 04:06:00 [INFO] activation_last_layer: relu
2023-05-25 04:06:00 [INFO] loss_function: mse
2023-05-25 04:06:00 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:06:00 [INFO] metric: mae
2023-05-25 04:06:00 [INFO] shuffle: True
2023-05-25 04:06:00 [INFO] epochs: 2
2023-05-25 04:06:00 [INFO] batch_size: 512
2023-05-25 04:06:00 [INFO] verbose: 2
2023-05-25 04:06:00 [INFO] TensorBoard_log_path: logs
2023-05-25 04:06:00 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:06:00 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:06:00 [INFO] EarlyStopping_patience: 15
2023-05-25 04:06:00 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:06:00 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:06:00 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:06:00 [INFO] log_path: logs/training.log
2023-05-25 04:06:00 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:06:00 [INFO] Training in progress
2023-05-25 04:06:01 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.
2023-05-25 04:06:02 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 04:06:03 [DEBUG] Epoch 1 - {'loss': '34595433086976.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-25 04:06:03 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-25 04:06:03 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:09:41 [INFO] Experiment for mlp test begins at 2023/05/25 04:09:41
2023-05-25 04:09:41 [INFO] Config file contents:
2023-05-25 04:09:41 [INFO] name: mlp test
2023-05-25 04:09:41 [INFO] activation: relu
2023-05-25 04:09:41 [INFO] activation_last_layer: relu
2023-05-25 04:09:41 [INFO] loss_function: mse
2023-05-25 04:09:41 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:09:41 [INFO] metric: mae
2023-05-25 04:09:41 [INFO] shuffle: True
2023-05-25 04:09:41 [INFO] epochs: 2
2023-05-25 04:09:41 [INFO] batch_size: 512
2023-05-25 04:09:41 [INFO] verbose: 2
2023-05-25 04:09:41 [INFO] TensorBoard_log_path: logs
2023-05-25 04:09:41 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:09:41 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:09:41 [INFO] EarlyStopping_patience: 15
2023-05-25 04:09:41 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:09:41 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:09:41 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:09:41 [INFO] log_path: logs/training.log
2023-05-25 04:09:41 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:09:41 [INFO] Training in progress
2023-05-25 04:09:42 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:09:43 [DEBUG] Epoch 0 - {'loss': '34595460349952.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 04:09:44 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 04:09:44 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-25 04:09:44 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:10:27 [INFO] Experiment for mlp test begins at 2023/05/25 04:10:27
2023-05-25 04:10:27 [INFO] Config file contents:
2023-05-25 04:10:27 [INFO] name: mlp test
2023-05-25 04:10:27 [INFO] activation: relu
2023-05-25 04:10:27 [INFO] activation_last_layer: relu
2023-05-25 04:10:27 [INFO] loss_function: mse
2023-05-25 04:10:27 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:10:27 [INFO] metric: mae
2023-05-25 04:10:27 [INFO] shuffle: True
2023-05-25 04:10:27 [INFO] epochs: 2
2023-05-25 04:10:27 [INFO] batch_size: 512
2023-05-25 04:10:27 [INFO] verbose: 2
2023-05-25 04:10:27 [INFO] TensorBoard_log_path: logs
2023-05-25 04:10:27 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:10:27 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:10:27 [INFO] EarlyStopping_patience: 15
2023-05-25 04:10:27 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:10:27 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:10:27 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:10:27 [INFO] log_path: logs/training.log
2023-05-25 04:10:27 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:10:27 [INFO] Training in progress
2023-05-25 04:10:28 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 04:10:29 [DEBUG] Epoch 0 - {'loss': '34595424698368.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-25 04:10:30 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 04:10:30 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 04:10:30 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:15:46 [INFO] Experiment for mlp test begins at 2023/05/25 04:15:46
2023-05-25 04:15:46 [INFO] Config file contents:
2023-05-25 04:15:46 [INFO] name: mlp test
2023-05-25 04:15:46 [INFO] activation: relu
2023-05-25 04:15:46 [INFO] activation_last_layer: relu
2023-05-25 04:15:46 [INFO] loss_function: mse
2023-05-25 04:15:46 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:15:46 [INFO] metric: mae
2023-05-25 04:15:46 [INFO] shuffle: True
2023-05-25 04:15:46 [INFO] epochs: 2
2023-05-25 04:15:46 [INFO] batch_size: 512
2023-05-25 04:15:46 [INFO] verbose: 2
2023-05-25 04:15:46 [INFO] TensorBoard_log_path: logs
2023-05-25 04:15:46 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:15:46 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:15:46 [INFO] EarlyStopping_patience: 15
2023-05-25 04:15:46 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:15:46 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:15:46 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:15:46 [INFO] log_path: logs/training.log
2023-05-25 04:15:46 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:15:46 [INFO] Training in progress
2023-05-25 04:15:47 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 04:15:48 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707967.000000', 'lr': '0.001000'} 
2023-05-25 04:15:49 [DEBUG] Epoch 1 - {'loss': '34595424698368.000000', 'mae': '3707967.250000', 'lr': '0.001000'} 
2023-05-25 04:15:49 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 04:15:49 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:16:08 [INFO] Experiment for mlp test begins at 2023/05/25 04:16:08
2023-05-25 04:16:08 [INFO] Config file contents:
2023-05-25 04:16:08 [INFO] name: mlp test
2023-05-25 04:16:08 [INFO] activation: relu
2023-05-25 04:16:08 [INFO] activation_last_layer: relu
2023-05-25 04:16:08 [INFO] loss_function: mse
2023-05-25 04:16:08 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:16:08 [INFO] metric: mae
2023-05-25 04:16:08 [INFO] shuffle: True
2023-05-25 04:16:08 [INFO] epochs: 2
2023-05-25 04:16:08 [INFO] batch_size: 512
2023-05-25 04:16:08 [INFO] verbose: 2
2023-05-25 04:16:08 [INFO] TensorBoard_log_path: logs
2023-05-25 04:16:08 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:16:08 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:16:08 [INFO] EarlyStopping_patience: 15
2023-05-25 04:16:08 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:16:08 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:16:08 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:16:08 [INFO] log_path: logs/training.log
2023-05-25 04:16:08 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:16:08 [INFO] Training in progress
2023-05-25 04:16:09 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:16:10 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707970.250000', 'lr': '0.001000'} 
2023-05-25 04:16:11 [DEBUG] Epoch 1 - {'loss': '34595441475584.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-25 04:16:11 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-25 04:16:11 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:16:31 [INFO] Experiment for mlp test begins at 2023/05/25 04:16:31
2023-05-25 04:16:31 [INFO] Config file contents:
2023-05-25 04:16:31 [INFO] name: mlp test
2023-05-25 04:16:31 [INFO] activation: relu
2023-05-25 04:16:31 [INFO] activation_last_layer: relu
2023-05-25 04:16:31 [INFO] loss_function: mse
2023-05-25 04:16:31 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:16:31 [INFO] metric: mae
2023-05-25 04:16:31 [INFO] shuffle: True
2023-05-25 04:16:31 [INFO] epochs: 2
2023-05-25 04:16:31 [INFO] batch_size: 512
2023-05-25 04:16:31 [INFO] verbose: 2
2023-05-25 04:16:31 [INFO] TensorBoard_log_path: logs
2023-05-25 04:16:31 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:16:31 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:16:31 [INFO] EarlyStopping_patience: 15
2023-05-25 04:16:31 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:16:31 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:16:31 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:16:31 [INFO] log_path: logs/training.log
2023-05-25 04:16:31 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:16:31 [INFO] Training in progress
2023-05-25 04:16:31 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.
2023-05-25 04:16:33 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-25 04:16:34 [DEBUG] Epoch 1 - {'loss': '34595430989824.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 04:16:34 [INFO] Training finished, elapsed time: 3.01 seconds
2023-05-25 04:16:34 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:27:30 [INFO] Experiment for mlp test begins at 2023/05/25 04:27:30
2023-05-25 04:27:30 [INFO] Config file contents:
2023-05-25 04:27:30 [INFO] name: mlp test
2023-05-25 04:27:30 [INFO] activation: relu
2023-05-25 04:27:30 [INFO] activation_last_layer: relu
2023-05-25 04:27:30 [INFO] loss_function: mse
2023-05-25 04:27:30 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:27:30 [INFO] metric: mae
2023-05-25 04:27:30 [INFO] shuffle: True
2023-05-25 04:27:30 [INFO] epochs: 2
2023-05-25 04:27:30 [INFO] batch_size: 512
2023-05-25 04:27:30 [INFO] verbose: 2
2023-05-25 04:27:30 [INFO] TensorBoard_log_path: logs
2023-05-25 04:27:30 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:27:30 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:27:30 [INFO] EarlyStopping_patience: 15
2023-05-25 04:27:30 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:27:30 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:27:30 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:27:30 [INFO] log_path: logs/training.log
2023-05-25 04:27:30 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:27:30 [INFO] Training in progress
2023-05-25 04:27:31 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-25 04:27:32 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 04:27:33 [DEBUG] Epoch 1 - {'loss': '34595424698368.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 04:27:33 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 04:27:33 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:27:54 [INFO] Experiment for mlp test begins at 2023/05/25 04:27:54
2023-05-25 04:27:54 [INFO] Config file contents:
2023-05-25 04:27:54 [INFO] name: mlp test
2023-05-25 04:27:54 [INFO] activation: relu
2023-05-25 04:27:54 [INFO] activation_last_layer: relu
2023-05-25 04:27:54 [INFO] loss_function: mse
2023-05-25 04:27:54 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:27:54 [INFO] metric: mae
2023-05-25 04:27:54 [INFO] shuffle: True
2023-05-25 04:27:54 [INFO] epochs: 2
2023-05-25 04:27:54 [INFO] batch_size: 512
2023-05-25 04:27:54 [INFO] verbose: 2
2023-05-25 04:27:54 [INFO] TensorBoard_log_path: logs
2023-05-25 04:27:54 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:27:54 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:27:54 [INFO] EarlyStopping_patience: 15
2023-05-25 04:27:54 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:27:54 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:27:54 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:27:54 [INFO] log_path: logs/training.log
2023-05-25 04:27:54 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:27:54 [INFO] Training in progress
2023-05-25 04:27:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:27:56 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 04:27:57 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:27:57 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-25 04:27:57 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:30:38 [INFO] Experiment for mlp test begins at 2023/05/25 04:30:38
2023-05-25 04:30:38 [INFO] Config file contents:
2023-05-25 04:30:38 [INFO] name: mlp test
2023-05-25 04:30:38 [INFO] activation: relu
2023-05-25 04:30:38 [INFO] activation_last_layer: relu
2023-05-25 04:30:38 [INFO] loss_function: mse
2023-05-25 04:30:38 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:30:38 [INFO] metric: mae
2023-05-25 04:30:38 [INFO] shuffle: True
2023-05-25 04:30:38 [INFO] epochs: 2
2023-05-25 04:30:38 [INFO] batch_size: 512
2023-05-25 04:30:38 [INFO] verbose: 2
2023-05-25 04:30:38 [INFO] TensorBoard_log_path: logs
2023-05-25 04:30:38 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:30:38 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:30:38 [INFO] EarlyStopping_patience: 15
2023-05-25 04:30:38 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:30:38 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:30:38 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:30:38 [INFO] log_path: logs/training.log
2023-05-25 04:30:38 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:30:38 [INFO] Training in progress
2023-05-25 04:30:39 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 04:30:40 [DEBUG] Epoch 0 - {'loss': '34595433086976.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 04:30:41 [DEBUG] Epoch 1 - {'loss': '34595430989824.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:30:41 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 04:30:41 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:30:55 [INFO] Experiment for mlp test begins at 2023/05/25 04:30:55
2023-05-25 04:30:55 [INFO] Config file contents:
2023-05-25 04:30:55 [INFO] name: mlp test
2023-05-25 04:30:55 [INFO] activation: relu
2023-05-25 04:30:55 [INFO] activation_last_layer: relu
2023-05-25 04:30:55 [INFO] loss_function: mse
2023-05-25 04:30:55 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:30:55 [INFO] metric: mae
2023-05-25 04:30:55 [INFO] shuffle: True
2023-05-25 04:30:55 [INFO] epochs: 2
2023-05-25 04:30:55 [INFO] batch_size: 512
2023-05-25 04:30:55 [INFO] verbose: 2
2023-05-25 04:30:55 [INFO] TensorBoard_log_path: logs
2023-05-25 04:30:55 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:30:55 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:30:55 [INFO] EarlyStopping_patience: 15
2023-05-25 04:30:55 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:30:55 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:30:55 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:30:55 [INFO] log_path: logs/training.log
2023-05-25 04:30:55 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:30:56 [INFO] Training in progress
2023-05-25 04:30:56 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 04:30:57 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:30:59 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 04:30:59 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 04:30:59 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:32:16 [INFO] Experiment for mlp test begins at 2023/05/25 04:32:16
2023-05-25 04:32:16 [INFO] Config file contents:
2023-05-25 04:32:16 [INFO] name: mlp test
2023-05-25 04:32:16 [INFO] activation: relu
2023-05-25 04:32:16 [INFO] activation_last_layer: relu
2023-05-25 04:32:16 [INFO] loss_function: mse
2023-05-25 04:32:16 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:32:16 [INFO] metric: mae
2023-05-25 04:32:16 [INFO] shuffle: True
2023-05-25 04:32:16 [INFO] epochs: 2
2023-05-25 04:32:16 [INFO] batch_size: 512
2023-05-25 04:32:16 [INFO] verbose: 2
2023-05-25 04:32:16 [INFO] TensorBoard_log_path: logs
2023-05-25 04:32:16 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:32:16 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:32:16 [INFO] EarlyStopping_patience: 15
2023-05-25 04:32:16 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:32:16 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:32:16 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:32:16 [INFO] log_path: logs/training.log
2023-05-25 04:32:16 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:32:16 [INFO] Training in progress
2023-05-25 04:32:17 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-25 04:32:18 [DEBUG] Epoch 0 - {'loss': '34595451961344.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 04:32:19 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:32:19 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-25 04:32:19 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:32:27 [INFO] Experiment for mlp test begins at 2023/05/25 04:32:27
2023-05-25 04:32:27 [INFO] Config file contents:
2023-05-25 04:32:27 [INFO] name: mlp test
2023-05-25 04:32:27 [INFO] activation: relu
2023-05-25 04:32:27 [INFO] activation_last_layer: relu
2023-05-25 04:32:27 [INFO] loss_function: mse
2023-05-25 04:32:27 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:32:27 [INFO] metric: mae
2023-05-25 04:32:27 [INFO] shuffle: True
2023-05-25 04:32:27 [INFO] epochs: 2
2023-05-25 04:32:27 [INFO] batch_size: 512
2023-05-25 04:32:27 [INFO] verbose: 2
2023-05-25 04:32:27 [INFO] TensorBoard_log_path: logs
2023-05-25 04:32:27 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:32:27 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:32:27 [INFO] EarlyStopping_patience: 15
2023-05-25 04:32:27 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:32:27 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:32:27 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:32:27 [INFO] log_path: logs/training.log
2023-05-25 04:32:27 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:32:27 [INFO] Training in progress
2023-05-25 04:32:28 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-05-25 04:32:29 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-25 04:32:30 [DEBUG] Epoch 1 - {'loss': '34595433086976.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 04:32:30 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-25 04:32:30 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:32:52 [INFO] Experiment for mlp test begins at 2023/05/25 04:32:52
2023-05-25 04:32:52 [INFO] Config file contents:
2023-05-25 04:32:52 [INFO] name: mlp test
2023-05-25 04:32:52 [INFO] activation: relu
2023-05-25 04:32:52 [INFO] activation_last_layer: relu
2023-05-25 04:32:52 [INFO] loss_function: mse
2023-05-25 04:32:52 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:32:52 [INFO] metric: mae
2023-05-25 04:32:52 [INFO] shuffle: True
2023-05-25 04:32:52 [INFO] epochs: 2
2023-05-25 04:32:52 [INFO] batch_size: 512
2023-05-25 04:32:52 [INFO] verbose: 2
2023-05-25 04:32:52 [INFO] TensorBoard_log_path: logs
2023-05-25 04:32:52 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:32:52 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:32:52 [INFO] EarlyStopping_patience: 15
2023-05-25 04:32:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:32:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:32:52 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:32:52 [INFO] log_path: logs/training.log
2023-05-25 04:32:52 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:32:52 [INFO] Training in progress
2023-05-25 04:32:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:32:54 [DEBUG] Epoch 0 - {'loss': '34595433086976.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-25 04:32:55 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 04:32:55 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-25 04:32:55 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:39:53 [INFO] Experiment for mlp test begins at 2023/05/25 04:39:53
2023-05-25 04:39:53 [INFO] Config file contents:
2023-05-25 04:39:53 [INFO] name: mlp test
2023-05-25 04:39:53 [INFO] activation: relu
2023-05-25 04:39:53 [INFO] activation_last_layer: relu
2023-05-25 04:39:53 [INFO] loss_function: mse
2023-05-25 04:39:53 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:39:53 [INFO] metric: mae
2023-05-25 04:39:53 [INFO] shuffle: True
2023-05-25 04:39:53 [INFO] epochs: 2
2023-05-25 04:39:53 [INFO] batch_size: 512
2023-05-25 04:39:53 [INFO] verbose: 2
2023-05-25 04:39:53 [INFO] TensorBoard_log_path: logs
2023-05-25 04:39:53 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:39:53 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:39:53 [INFO] EarlyStopping_patience: 15
2023-05-25 04:39:53 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:39:53 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:39:53 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:39:53 [INFO] log_path: logs/training.log
2023-05-25 04:39:53 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:39:53 [INFO] Training in progress
2023-05-25 04:39:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-25 04:39:55 [DEBUG] Epoch 0 - {'loss': '34595439378432.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 04:39:56 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 04:39:56 [INFO] Training finished, elapsed time: 3.10 seconds
2023-05-25 04:39:56 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:42:17 [INFO] Experiment for mlp test begins at 2023/05/25 04:42:17
2023-05-25 04:42:17 [INFO] Config file contents:
2023-05-25 04:42:17 [INFO] name: mlp test
2023-05-25 04:42:17 [INFO] activation: relu
2023-05-25 04:42:17 [INFO] activation_last_layer: relu
2023-05-25 04:42:17 [INFO] loss_function: mse
2023-05-25 04:42:17 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:42:17 [INFO] metric: mae
2023-05-25 04:42:17 [INFO] shuffle: True
2023-05-25 04:42:17 [INFO] epochs: 2
2023-05-25 04:42:17 [INFO] batch_size: 512
2023-05-25 04:42:17 [INFO] verbose: 2
2023-05-25 04:42:17 [INFO] TensorBoard_log_path: logs
2023-05-25 04:42:17 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:42:17 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:42:17 [INFO] EarlyStopping_patience: 15
2023-05-25 04:42:17 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:42:17 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:42:17 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:42:17 [INFO] log_path: logs/training.log
2023-05-25 04:42:17 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:42:17 [INFO] Training in progress
2023-05-25 04:42:18 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.
2023-05-25 04:42:19 [DEBUG] Epoch 0 - {'loss': '34595445669888.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-25 04:42:20 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707971.000000', 'lr': '0.001000'} 
2023-05-25 04:42:20 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 04:42:20 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:46:29 [INFO] Experiment for mlp test begins at 2023/05/25 04:46:29
2023-05-25 04:46:29 [INFO] Config file contents:
2023-05-25 04:46:29 [INFO] name: mlp test
2023-05-25 04:46:29 [INFO] activation: relu
2023-05-25 04:46:29 [INFO] activation_last_layer: relu
2023-05-25 04:46:29 [INFO] loss_function: mse
2023-05-25 04:46:29 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:46:29 [INFO] metric: mae
2023-05-25 04:46:29 [INFO] shuffle: True
2023-05-25 04:46:29 [INFO] epochs: 2
2023-05-25 04:46:29 [INFO] batch_size: 512
2023-05-25 04:46:29 [INFO] verbose: 2
2023-05-25 04:46:29 [INFO] TensorBoard_log_path: logs
2023-05-25 04:46:29 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:46:29 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:46:29 [INFO] EarlyStopping_patience: 15
2023-05-25 04:46:29 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:46:29 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:46:29 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:46:29 [INFO] log_path: logs/training.log
2023-05-25 04:46:29 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:46:29 [INFO] Training in progress
2023-05-25 04:46:30 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-25 04:46:31 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707970.750000', 'lr': '0.001000'} 
2023-05-25 04:46:32 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-25 04:46:32 [INFO] Training finished, elapsed time: 3.10 seconds
2023-05-25 04:46:32 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:47:32 [INFO] Experiment for mlp test begins at 2023/05/25 04:47:32
2023-05-25 04:47:32 [INFO] Config file contents:
2023-05-25 04:47:32 [INFO] name: mlp test
2023-05-25 04:47:32 [INFO] activation: relu
2023-05-25 04:47:32 [INFO] activation_last_layer: relu
2023-05-25 04:47:32 [INFO] loss_function: mse
2023-05-25 04:47:32 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:47:32 [INFO] metric: mae
2023-05-25 04:47:32 [INFO] shuffle: True
2023-05-25 04:47:32 [INFO] epochs: 2
2023-05-25 04:47:32 [INFO] batch_size: 512
2023-05-25 04:47:32 [INFO] verbose: 2
2023-05-25 04:47:32 [INFO] TensorBoard_log_path: logs
2023-05-25 04:47:32 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:47:32 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:47:32 [INFO] EarlyStopping_patience: 15
2023-05-25 04:47:32 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:47:32 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:47:32 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:47:32 [INFO] log_path: logs/training.log
2023-05-25 04:47:32 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:47:33 [INFO] Training in progress
2023-05-25 04:47:33 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:47:34 [DEBUG] Epoch 0 - {'loss': '34595428892672.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-25 04:47:36 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 04:47:36 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-25 04:47:36 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:48:53 [INFO] Experiment for mlp test begins at 2023/05/25 04:48:53
2023-05-25 04:48:53 [INFO] Config file contents:
2023-05-25 04:48:53 [INFO] name: mlp test
2023-05-25 04:48:53 [INFO] activation: relu
2023-05-25 04:48:53 [INFO] activation_last_layer: relu
2023-05-25 04:48:53 [INFO] loss_function: mse
2023-05-25 04:48:53 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:48:53 [INFO] metric: mae
2023-05-25 04:48:53 [INFO] shuffle: True
2023-05-25 04:48:53 [INFO] epochs: 2
2023-05-25 04:48:53 [INFO] batch_size: 512
2023-05-25 04:48:53 [INFO] verbose: 2
2023-05-25 04:48:53 [INFO] TensorBoard_log_path: logs
2023-05-25 04:48:53 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:48:53 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:48:53 [INFO] EarlyStopping_patience: 15
2023-05-25 04:48:53 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:48:53 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:48:53 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:48:53 [INFO] log_path: logs/training.log
2023-05-25 04:48:53 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:48:53 [INFO] Training in progress
2023-05-25 04:48:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:48:55 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:48:56 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:48:56 [INFO] Training finished, elapsed time: 3.03 seconds
2023-05-25 04:48:56 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:49:25 [INFO] Experiment for mlp test begins at 2023/05/25 04:49:25
2023-05-25 04:49:25 [INFO] Config file contents:
2023-05-25 04:49:25 [INFO] name: mlp test
2023-05-25 04:49:25 [INFO] activation: relu
2023-05-25 04:49:25 [INFO] activation_last_layer: relu
2023-05-25 04:49:25 [INFO] loss_function: mse
2023-05-25 04:49:25 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:49:25 [INFO] metric: mae
2023-05-25 04:49:25 [INFO] shuffle: True
2023-05-25 04:49:25 [INFO] epochs: 2
2023-05-25 04:49:25 [INFO] batch_size: 512
2023-05-25 04:49:25 [INFO] verbose: 2
2023-05-25 04:49:25 [INFO] TensorBoard_log_path: logs
2023-05-25 04:49:25 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:49:25 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:49:25 [INFO] EarlyStopping_patience: 15
2023-05-25 04:49:25 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:49:25 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:49:25 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:49:25 [INFO] log_path: logs/training.log
2023-05-25 04:49:25 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:49:26 [INFO] Training in progress
2023-05-25 04:49:26 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 04:49:28 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 04:49:29 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:49:29 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-25 04:49:29 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 04:50:11 [INFO] Experiment for mlp test begins at 2023/05/25 04:50:11
2023-05-25 04:50:11 [INFO] Config file contents:
2023-05-25 04:50:11 [INFO] name: mlp test
2023-05-25 04:50:11 [INFO] activation: relu
2023-05-25 04:50:11 [INFO] activation_last_layer: relu
2023-05-25 04:50:11 [INFO] loss_function: mse
2023-05-25 04:50:11 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 04:50:11 [INFO] metric: mae
2023-05-25 04:50:11 [INFO] shuffle: True
2023-05-25 04:50:11 [INFO] epochs: 2
2023-05-25 04:50:11 [INFO] batch_size: 512
2023-05-25 04:50:11 [INFO] verbose: 2
2023-05-25 04:50:11 [INFO] TensorBoard_log_path: logs
2023-05-25 04:50:11 [INFO] TensorBoard_hist_freq: 1
2023-05-25 04:50:11 [INFO] EarlyStopping_monitor: loss
2023-05-25 04:50:11 [INFO] EarlyStopping_patience: 15
2023-05-25 04:50:11 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 04:50:11 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 04:50:11 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 04:50:11 [INFO] log_path: logs/training.log
2023-05-25 04:50:11 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 04:50:11 [INFO] Training in progress
2023-05-25 04:50:11 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 04:50:13 [DEBUG] Epoch 0 - {'loss': '34595430989824.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-25 04:50:14 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 04:50:14 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-25 04:50:14 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:19:54 [INFO] Experiment for mlp test begins at 2023/05/25 20:19:54
2023-05-25 20:19:54 [INFO] Config file contents:
2023-05-25 20:19:54 [INFO] name: mlp test
2023-05-25 20:19:54 [INFO] activation: relu
2023-05-25 20:19:54 [INFO] activation_last_layer: relu
2023-05-25 20:19:54 [INFO] loss_function: mse
2023-05-25 20:19:54 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:19:54 [INFO] metric: mae
2023-05-25 20:19:54 [INFO] shuffle: True
2023-05-25 20:19:54 [INFO] epochs: 2
2023-05-25 20:19:54 [INFO] batch_size: 512
2023-05-25 20:19:54 [INFO] verbose: 2
2023-05-25 20:19:54 [INFO] TensorBoard_log_path: logs
2023-05-25 20:19:54 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:19:54 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:19:54 [INFO] EarlyStopping_patience: 15
2023-05-25 20:19:54 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:19:54 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:19:54 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:19:54 [INFO] log_path: logs/training.log
2023-05-25 20:19:54 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:19:54 [INFO] Training in progress
2023-05-25 20:19:55 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 20:19:56 [DEBUG] Epoch 0 - {'loss': '34595433086976.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 20:19:58 [DEBUG] Epoch 1 - {'loss': '34595439378432.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 20:19:58 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-25 20:19:58 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:20:36 [INFO] Experiment for mlp test begins at 2023/05/25 20:20:36
2023-05-25 20:20:36 [INFO] Config file contents:
2023-05-25 20:20:36 [INFO] name: mlp test
2023-05-25 20:20:36 [INFO] activation: relu
2023-05-25 20:20:36 [INFO] activation_last_layer: relu
2023-05-25 20:20:36 [INFO] loss_function: mse
2023-05-25 20:20:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:20:36 [INFO] metric: mae
2023-05-25 20:20:36 [INFO] shuffle: True
2023-05-25 20:20:36 [INFO] epochs: 2
2023-05-25 20:20:36 [INFO] batch_size: 512
2023-05-25 20:20:36 [INFO] verbose: 2
2023-05-25 20:20:36 [INFO] TensorBoard_log_path: logs
2023-05-25 20:20:36 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:20:36 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:20:36 [INFO] EarlyStopping_patience: 15
2023-05-25 20:20:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:20:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:20:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:20:36 [INFO] log_path: logs/training.log
2023-05-25 20:20:36 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:20:36 [INFO] Training in progress
2023-05-25 20:20:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-25 20:20:38 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 20:20:39 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 20:20:39 [INFO] Training finished, elapsed time: 3.12 seconds
2023-05-25 20:20:39 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:21:08 [INFO] Experiment for mlp test begins at 2023/05/25 20:21:08
2023-05-25 20:21:08 [INFO] Config file contents:
2023-05-25 20:21:08 [INFO] name: mlp test
2023-05-25 20:21:08 [INFO] activation: relu
2023-05-25 20:21:08 [INFO] activation_last_layer: relu
2023-05-25 20:21:08 [INFO] loss_function: mse
2023-05-25 20:21:08 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:21:08 [INFO] metric: mae
2023-05-25 20:21:08 [INFO] shuffle: True
2023-05-25 20:21:08 [INFO] epochs: 2
2023-05-25 20:21:08 [INFO] batch_size: 512
2023-05-25 20:21:08 [INFO] verbose: 2
2023-05-25 20:21:08 [INFO] TensorBoard_log_path: logs
2023-05-25 20:21:08 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:21:08 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:21:08 [INFO] EarlyStopping_patience: 15
2023-05-25 20:21:08 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:21:08 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:21:08 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:21:08 [INFO] log_path: logs/training.log
2023-05-25 20:21:08 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:21:08 [INFO] Training in progress
2023-05-25 20:21:08 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 20:21:10 [DEBUG] Epoch 0 - {'loss': '34595449864192.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-25 20:21:11 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-25 20:21:11 [INFO] Training finished, elapsed time: 3.08 seconds
2023-05-25 20:21:11 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:21:47 [INFO] Experiment for mlp test begins at 2023/05/25 20:21:47
2023-05-25 20:21:47 [INFO] Config file contents:
2023-05-25 20:21:47 [INFO] name: mlp test
2023-05-25 20:21:47 [INFO] activation: relu
2023-05-25 20:21:47 [INFO] activation_last_layer: relu
2023-05-25 20:21:47 [INFO] loss_function: mse
2023-05-25 20:21:47 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:21:47 [INFO] metric: mae
2023-05-25 20:21:47 [INFO] shuffle: True
2023-05-25 20:21:47 [INFO] epochs: 2
2023-05-25 20:21:47 [INFO] batch_size: 512
2023-05-25 20:21:47 [INFO] verbose: 2
2023-05-25 20:21:47 [INFO] TensorBoard_log_path: logs
2023-05-25 20:21:47 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:21:47 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:21:47 [INFO] EarlyStopping_patience: 15
2023-05-25 20:21:47 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:21:47 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:21:47 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:21:47 [INFO] log_path: logs/training.log
2023-05-25 20:21:47 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:21:47 [INFO] Training in progress
2023-05-25 20:21:48 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-05-25 20:21:49 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-25 20:21:50 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 20:21:50 [INFO] Training finished, elapsed time: 2.99 seconds
2023-05-25 20:21:50 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:22:18 [INFO] Experiment for mlp test begins at 2023/05/25 20:22:18
2023-05-25 20:22:18 [INFO] Config file contents:
2023-05-25 20:22:18 [INFO] name: mlp test
2023-05-25 20:22:18 [INFO] activation: relu
2023-05-25 20:22:18 [INFO] activation_last_layer: relu
2023-05-25 20:22:18 [INFO] loss_function: mse
2023-05-25 20:22:18 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:22:18 [INFO] metric: mae
2023-05-25 20:22:18 [INFO] shuffle: True
2023-05-25 20:22:18 [INFO] epochs: 2
2023-05-25 20:22:18 [INFO] batch_size: 512
2023-05-25 20:22:18 [INFO] verbose: 2
2023-05-25 20:22:18 [INFO] TensorBoard_log_path: logs
2023-05-25 20:22:18 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:22:18 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:22:18 [INFO] EarlyStopping_patience: 15
2023-05-25 20:22:18 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:22:18 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:22:18 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:22:18 [INFO] log_path: logs/training.log
2023-05-25 20:22:18 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:22:19 [INFO] Training in progress
2023-05-25 20:22:19 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 20:22:20 [DEBUG] Epoch 0 - {'loss': '34595445669888.000000', 'mae': '3707970.250000', 'lr': '0.001000'} 
2023-05-25 20:22:22 [DEBUG] Epoch 1 - {'loss': '34595454058496.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-25 20:22:22 [INFO] Training finished, elapsed time: 3.02 seconds
2023-05-25 20:22:22 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:22:56 [INFO] Experiment for mlp test begins at 2023/05/25 20:22:56
2023-05-25 20:22:56 [INFO] Config file contents:
2023-05-25 20:22:56 [INFO] name: mlp test
2023-05-25 20:22:56 [INFO] activation: relu
2023-05-25 20:22:56 [INFO] activation_last_layer: relu
2023-05-25 20:22:56 [INFO] loss_function: mse
2023-05-25 20:22:56 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:22:56 [INFO] metric: mae
2023-05-25 20:22:56 [INFO] shuffle: True
2023-05-25 20:22:56 [INFO] epochs: 2
2023-05-25 20:22:56 [INFO] batch_size: 512
2023-05-25 20:22:56 [INFO] verbose: 2
2023-05-25 20:22:56 [INFO] TensorBoard_log_path: logs
2023-05-25 20:22:56 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:22:56 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:22:56 [INFO] EarlyStopping_patience: 15
2023-05-25 20:22:56 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:22:56 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:22:56 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:22:56 [INFO] log_path: logs/training.log
2023-05-25 20:22:56 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:22:56 [INFO] Training in progress
2023-05-25 20:22:57 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-25 20:22:58 [DEBUG] Epoch 0 - {'loss': '34595449864192.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-25 20:23:00 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 20:23:00 [INFO] Training finished, elapsed time: 3.09 seconds
2023-05-25 20:23:00 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 20:43:06 [INFO] Experiment for mlp test begins at 2023/05/25 20:43:06
2023-05-25 20:43:06 [INFO] Config file contents:
2023-05-25 20:43:06 [INFO] name: mlp test
2023-05-25 20:43:06 [INFO] activation: relu
2023-05-25 20:43:06 [INFO] activation_last_layer: relu
2023-05-25 20:43:06 [INFO] loss_function: mse
2023-05-25 20:43:06 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 20:43:06 [INFO] metric: mae
2023-05-25 20:43:06 [INFO] shuffle: True
2023-05-25 20:43:06 [INFO] epochs: 2
2023-05-25 20:43:06 [INFO] batch_size: 512
2023-05-25 20:43:06 [INFO] verbose: 2
2023-05-25 20:43:06 [INFO] TensorBoard_log_path: logs
2023-05-25 20:43:06 [INFO] TensorBoard_hist_freq: 1
2023-05-25 20:43:06 [INFO] EarlyStopping_monitor: loss
2023-05-25 20:43:06 [INFO] EarlyStopping_patience: 15
2023-05-25 20:43:06 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 20:43:06 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 20:43:06 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 20:43:06 [INFO] log_path: logs/training.log
2023-05-25 20:43:06 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 20:43:06 [INFO] Training in progress
2023-05-25 20:43:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-25 20:43:08 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-25 20:43:09 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707970.250000', 'lr': '0.001000'} 
2023-05-25 20:43:09 [INFO] Training finished, elapsed time: 3.07 seconds
2023-05-25 20:43:09 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 21:00:25 [INFO] Experiment for mlp test begins at 2023/05/25 21:00:25
2023-05-25 21:00:25 [INFO] Config file contents:
2023-05-25 21:00:25 [INFO] name: mlp test
2023-05-25 21:00:25 [INFO] activation: relu
2023-05-25 21:00:25 [INFO] activation_last_layer: relu
2023-05-25 21:00:25 [INFO] loss_function: mse
2023-05-25 21:00:25 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 21:00:25 [INFO] metric: mae
2023-05-25 21:00:25 [INFO] shuffle: True
2023-05-25 21:00:25 [INFO] epochs: 2
2023-05-25 21:00:25 [INFO] batch_size: 512
2023-05-25 21:00:25 [INFO] verbose: 2
2023-05-25 21:00:25 [INFO] TensorBoard_log_path: logs
2023-05-25 21:00:25 [INFO] TensorBoard_hist_freq: 1
2023-05-25 21:00:25 [INFO] EarlyStopping_monitor: loss
2023-05-25 21:00:25 [INFO] EarlyStopping_patience: 15
2023-05-25 21:00:25 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 21:00:25 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 21:00:25 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 21:00:25 [INFO] log_path: logs/training.log
2023-05-25 21:00:25 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 21:00:25 [INFO] Training in progress
2023-05-25 21:00:26 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 21:00:27 [DEBUG] Epoch 0 - {'loss': '34595430989824.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-25 21:00:28 [DEBUG] Epoch 1 - {'loss': '34595454058496.000000', 'mae': '3707967.250000', 'lr': '0.001000'} 
2023-05-25 21:00:28 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-25 21:00:28 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 21:17:05 [INFO] Experiment for mlp test begins at 2023/05/25 21:17:05
2023-05-25 21:17:05 [INFO] Config file contents:
2023-05-25 21:17:05 [INFO] name: mlp test
2023-05-25 21:17:05 [INFO] activation: relu
2023-05-25 21:17:05 [INFO] activation_last_layer: relu
2023-05-25 21:17:05 [INFO] loss_function: mse
2023-05-25 21:17:05 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 21:17:05 [INFO] metric: mae
2023-05-25 21:17:05 [INFO] shuffle: True
2023-05-25 21:17:05 [INFO] epochs: 2
2023-05-25 21:17:05 [INFO] batch_size: 512
2023-05-25 21:17:05 [INFO] verbose: 2
2023-05-25 21:17:05 [INFO] TensorBoard_log_path: logs
2023-05-25 21:17:05 [INFO] TensorBoard_hist_freq: 1
2023-05-25 21:17:05 [INFO] EarlyStopping_monitor: loss
2023-05-25 21:17:05 [INFO] EarlyStopping_patience: 15
2023-05-25 21:17:05 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 21:17:05 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 21:17:05 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 21:17:05 [INFO] log_path: logs/training.log
2023-05-25 21:17:05 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 21:17:05 [INFO] Training in progress
2023-05-25 21:17:06 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 21:17:07 [DEBUG] Epoch 0 - {'loss': '34595428892672.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-25 21:17:08 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-25 21:17:08 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-25 21:17:08 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 21:17:38 [INFO] Experiment for mlp test begins at 2023/05/25 21:17:38
2023-05-25 21:17:38 [INFO] Config file contents:
2023-05-25 21:17:38 [INFO] name: mlp test
2023-05-25 21:17:38 [INFO] activation: relu
2023-05-25 21:17:38 [INFO] activation_last_layer: relu
2023-05-25 21:17:38 [INFO] loss_function: mse
2023-05-25 21:17:38 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 21:17:38 [INFO] metric: mae
2023-05-25 21:17:38 [INFO] shuffle: True
2023-05-25 21:17:38 [INFO] epochs: 2
2023-05-25 21:17:38 [INFO] batch_size: 512
2023-05-25 21:17:38 [INFO] verbose: 2
2023-05-25 21:17:38 [INFO] TensorBoard_log_path: logs
2023-05-25 21:17:38 [INFO] TensorBoard_hist_freq: 1
2023-05-25 21:17:38 [INFO] EarlyStopping_monitor: loss
2023-05-25 21:17:38 [INFO] EarlyStopping_patience: 15
2023-05-25 21:17:38 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 21:17:38 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 21:17:38 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 21:17:38 [INFO] log_path: logs/training.log
2023-05-25 21:17:38 [INFO] save_path: models/pretrain_mlp.h5
2023-05-25 21:17:38 [INFO] Training in progress
2023-05-25 21:17:39 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 21:17:40 [DEBUG] Epoch 0 - {'loss': '34595422601216.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 21:17:41 [DEBUG] Epoch 1 - {'loss': '34595439378432.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 21:17:41 [INFO] Training finished, elapsed time: 3.05 seconds
2023-05-25 21:17:41 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 21:26:09 [INFO] Experiment for mlp test begins at 2023/05/25 21:26:09
2023-05-25 21:26:09 [INFO] Config file contents:
2023-05-25 21:26:09 [INFO] name: mlp test
2023-05-25 21:26:09 [INFO] activation: relu
2023-05-25 21:26:09 [INFO] activation_last_layer: relu
2023-05-25 21:26:09 [INFO] loss_function: mse
2023-05-25 21:26:09 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 21:26:09 [INFO] metric: mae
2023-05-25 21:26:09 [INFO] shuffle: True
2023-05-25 21:26:09 [INFO] epochs: 2
2023-05-25 21:26:09 [INFO] batch_size: 512
2023-05-25 21:26:09 [INFO] verbose: 2
2023-05-25 21:26:09 [INFO] TensorBoard_log_path: logs
2023-05-25 21:26:09 [INFO] TensorBoard_hist_freq: 1
2023-05-25 21:26:09 [INFO] EarlyStopping_monitor: loss
2023-05-25 21:26:09 [INFO] EarlyStopping_patience: 15
2023-05-25 21:26:09 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 21:26:09 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 21:26:09 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 21:26:09 [INFO] Checkpoint_monitor: loss
2023-05-25 21:26:09 [INFO] save_best_only: True
2023-05-25 21:26:09 [INFO] log_path: logs/training.log
2023-05-25 21:26:09 [INFO] save_model_path: models/pretrain_mlp.h5
2023-05-25 21:26:09 [INFO] Training in progress
2023-05-25 21:26:10 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.
2023-05-25 21:26:11 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-25 21:26:12 [DEBUG] Epoch 1 - {'loss': '34595451961344.000000', 'mae': '3707970.250000', 'lr': '0.001000'} 
2023-05-25 21:26:12 [INFO] Training finished, elapsed time: 3.04 seconds
2023-05-25 21:26:12 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-25 21:26:27 [INFO] Experiment for mlp test begins at 2023/05/25 21:26:27
2023-05-25 21:26:27 [INFO] Config file contents:
2023-05-25 21:26:27 [INFO] name: mlp test
2023-05-25 21:26:27 [INFO] activation: relu
2023-05-25 21:26:27 [INFO] activation_last_layer: relu
2023-05-25 21:26:27 [INFO] loss_function: mse
2023-05-25 21:26:27 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-25 21:26:27 [INFO] metric: mae
2023-05-25 21:26:27 [INFO] shuffle: True
2023-05-25 21:26:27 [INFO] epochs: 2
2023-05-25 21:26:27 [INFO] batch_size: 512
2023-05-25 21:26:27 [INFO] verbose: 2
2023-05-25 21:26:27 [INFO] TensorBoard_log_path: logs
2023-05-25 21:26:27 [INFO] TensorBoard_hist_freq: 1
2023-05-25 21:26:27 [INFO] EarlyStopping_monitor: loss
2023-05-25 21:26:27 [INFO] EarlyStopping_patience: 15
2023-05-25 21:26:27 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-25 21:26:27 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-25 21:26:27 [INFO] ReduceLROnPlateau_patience: 3
2023-05-25 21:26:27 [INFO] Checkpoint_monitor: loss
2023-05-25 21:26:27 [INFO] save_best_only: True
2023-05-25 21:26:27 [INFO] log_path: logs/training.log
2023-05-25 21:26:27 [INFO] save_model_path: models/pretrain_mlp.h5
2023-05-25 21:26:27 [INFO] Training in progress
2023-05-25 21:26:27 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-25 21:26:29 [DEBUG] Epoch 0 - {'loss': '34595443572736.000000', 'mae': '3707967.000000', 'lr': '0.001000'} 
2023-05-25 21:26:30 [DEBUG] Epoch 1 - {'loss': '34595428892672.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-25 21:26:30 [INFO] Training finished, elapsed time: 3.06 seconds
2023-05-25 21:26:30 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-26 02:22:56 [INFO] Experiment for mlp test begins at 2023/05/26 02:22:56
2023-05-26 02:22:56 [INFO] Model configurations:
2023-05-26 02:22:56 [INFO] name: mlp test
2023-05-26 02:22:56 [INFO] activation: relu
2023-05-26 02:22:56 [INFO] activation_last_layer: relu
2023-05-26 02:22:56 [INFO] loss_function: mse
2023-05-26 02:22:56 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:22:56 [INFO] metric: mae
2023-05-26 02:22:56 [INFO] shuffle: True
2023-05-26 02:22:56 [INFO] epochs: 2
2023-05-26 02:22:56 [INFO] batch_size: 512
2023-05-26 02:22:56 [INFO] verbose: 2
2023-05-26 02:22:56 [INFO] TensorBoard_log_path: logs
2023-05-26 02:22:56 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:22:56 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:22:56 [INFO] EarlyStopping_patience: 15
2023-05-26 02:22:56 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:22:56 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:22:56 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:22:56 [INFO] Checkpoint_monitor: loss
2023-05-26 02:22:56 [INFO] save_best_only: True
2023-05-26 02:22:56 [INFO] log_path: logs/training.log
2023-05-26 02:22:56 [INFO] save_model_path: models/pretrain_mlp.h5
2023-05-26 02:22:56 [INFO] Training in progress
2023-05-26 02:22:57 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-26 02:22:58 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707968.250000', 'lr': '0.001000'} 
2023-05-26 02:22:59 [DEBUG] Epoch 1 - {'loss': '34595435184128.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-26 02:22:59 [INFO] Training finished, elapsed time: 3.07 seconds
2023-05-26 02:22:59 [INFO] model is saved in 'models/pretrain_mlp.h5'

2023-05-26 02:29:28 [INFO] Experiment for resnet test begins at 2023/05/26 02:29:28
2023-05-26 02:29:28 [INFO] Model configurations:
2023-05-26 02:29:28 [INFO] name: resnet test
2023-05-26 02:29:28 [INFO] activation: relu
2023-05-26 02:29:28 [INFO] activation_last_layer: relu
2023-05-26 02:29:28 [INFO] loss_function: mse
2023-05-26 02:29:28 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:29:28 [INFO] metric: mae
2023-05-26 02:29:28 [INFO] shuffle: True
2023-05-26 02:29:28 [INFO] epochs: 2
2023-05-26 02:29:28 [INFO] batch_size: 512
2023-05-26 02:29:28 [INFO] verbose: 2
2023-05-26 02:29:28 [INFO] TensorBoard_log_path: logs
2023-05-26 02:29:28 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:29:28 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:29:28 [INFO] EarlyStopping_patience: 15
2023-05-26 02:29:28 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:29:28 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:29:28 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:29:28 [INFO] Checkpoint_monitor: loss
2023-05-26 02:29:28 [INFO] save_best_only: False
2023-05-26 02:29:28 [INFO] log_path: logs/training.log
2023-05-26 02:29:28 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-26 02:29:28 [INFO] Training in progress
2023-05-26 02:29:30 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0119s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-05-26 02:29:35 [DEBUG] Epoch 0 - {'loss': '34595441475584.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-26 02:29:39 [DEBUG] Epoch 1 - {'loss': '34595437281280.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-26 02:29:39 [INFO] Training finished, elapsed time: 10.88 seconds
2023-05-26 02:29:39 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-26 02:37:43 [INFO] Experiment for resnet test begins at 2023/05/26 02:37:43
2023-05-26 02:37:43 [INFO] Model configurations:
2023-05-26 02:37:43 [INFO] name: resnet test
2023-05-26 02:37:43 [INFO] loss_function: mse
2023-05-26 02:37:43 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:37:43 [INFO] metric: mae
2023-05-26 02:37:43 [INFO] shuffle: True
2023-05-26 02:37:43 [INFO] epochs: 2
2023-05-26 02:37:43 [INFO] batch_size: 512
2023-05-26 02:37:43 [INFO] verbose: 2
2023-05-26 02:37:43 [INFO] TensorBoard_log_path: logs
2023-05-26 02:37:43 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:37:43 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:37:43 [INFO] EarlyStopping_patience: 15
2023-05-26 02:37:43 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:37:43 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:37:43 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:37:43 [INFO] Checkpoint_monitor: loss
2023-05-26 02:37:43 [INFO] save_best_only: False
2023-05-26 02:37:43 [INFO] log_path: logs/training.log
2023-05-26 02:37:43 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-26 02:37:43 [INFO] Training in progress
2023-05-26 02:37:45 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0127s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.
2023-05-26 02:37:50 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707969.250000', 'lr': '0.001000'} 
2023-05-26 02:37:54 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-26 02:37:54 [INFO] Training finished, elapsed time: 11.24 seconds
2023-05-26 02:37:54 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-26 02:40:18 [INFO] Experiment for resnet test begins at 2023/05/26 02:40:18
2023-05-26 02:40:18 [INFO] Model configurations:
2023-05-26 02:40:18 [INFO] name: resnet test
2023-05-26 02:40:18 [INFO] loss_function: mse
2023-05-26 02:40:18 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:40:18 [INFO] metric: mae
2023-05-26 02:40:18 [INFO] shuffle: True
2023-05-26 02:40:18 [INFO] epochs: 2
2023-05-26 02:40:18 [INFO] batch_size: 512
2023-05-26 02:40:18 [INFO] verbose: 2
2023-05-26 02:40:18 [INFO] TensorBoard_log_path: logs
2023-05-26 02:40:18 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:40:18 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:40:18 [INFO] EarlyStopping_patience: 15
2023-05-26 02:40:18 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:40:18 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:40:18 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:40:18 [INFO] Checkpoint_monitor: loss
2023-05-26 02:40:18 [INFO] save_best_only: False
2023-05-26 02:40:18 [INFO] log_path: logs/training.log
2023-05-26 02:40:18 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-26 02:40:18 [INFO] Training in progress
2023-05-26 02:40:21 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-05-26 02:40:25 [DEBUG] Epoch 0 - {'loss': '34595433086976.000000', 'mae': '3707969.500000', 'lr': '0.001000'} 
2023-05-26 02:40:29 [DEBUG] Epoch 1 - {'loss': '34595456155648.000000', 'mae': '3707970.000000', 'lr': '0.001000'} 
2023-05-26 02:40:29 [INFO] Training finished, elapsed time: 11.24 seconds
2023-05-26 02:40:29 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-26 02:44:11 [INFO] Experiment for resnet test begins at 2023/05/26 02:44:11
2023-05-26 02:44:11 [INFO] Model configurations:
2023-05-26 02:44:11 [INFO] name: resnet test
2023-05-26 02:44:11 [INFO] loss_function: mse
2023-05-26 02:44:11 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:44:11 [INFO] metric: mae
2023-05-26 02:44:11 [INFO] shuffle: True
2023-05-26 02:44:11 [INFO] epochs: 2
2023-05-26 02:44:11 [INFO] batch_size: 512
2023-05-26 02:44:11 [INFO] verbose: 2
2023-05-26 02:44:11 [INFO] TensorBoard_log_path: logs
2023-05-26 02:44:11 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:44:11 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:44:11 [INFO] EarlyStopping_patience: 15
2023-05-26 02:44:11 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:44:11 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:44:11 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:44:11 [INFO] Checkpoint_monitor: loss
2023-05-26 02:44:11 [INFO] save_best_only: False
2023-05-26 02:44:11 [INFO] log_path: logs/training.log
2023-05-26 02:44:11 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-26 02:44:11 [INFO] Training in progress
2023-05-26 02:44:14 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0230s). Check your callbacks.
2023-05-26 02:44:18 [DEBUG] Epoch 0 - {'loss': '34595451961344.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-26 02:44:22 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707968.000000', 'lr': '0.001000'} 
2023-05-26 02:44:22 [INFO] Training finished, elapsed time: 11.30 seconds
2023-05-26 02:44:22 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-26 02:45:33 [INFO] Experiment for resnet test begins at 2023/05/26 02:45:33
2023-05-26 02:45:33 [INFO] Model configurations:
2023-05-26 02:45:33 [INFO] name: resnet test
2023-05-26 02:45:33 [INFO] loss_function: mse
2023-05-26 02:45:33 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:45:33 [INFO] metric: mae
2023-05-26 02:45:33 [INFO] shuffle: True
2023-05-26 02:45:33 [INFO] epochs: 2
2023-05-26 02:45:33 [INFO] batch_size: 512
2023-05-26 02:45:33 [INFO] verbose: 2
2023-05-26 02:45:33 [INFO] TensorBoard_log_path: logs
2023-05-26 02:45:33 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:45:33 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:45:33 [INFO] EarlyStopping_patience: 15
2023-05-26 02:45:33 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:45:33 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:45:33 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:45:33 [INFO] Checkpoint_monitor: loss
2023-05-26 02:45:33 [INFO] save_best_only: False
2023-05-26 02:45:33 [INFO] log_path: logs/training.log
2023-05-26 02:45:33 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-26 02:45:33 [INFO] Training in progress
2023-05-26 02:45:36 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0122s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-05-26 02:45:40 [DEBUG] Epoch 0 - {'loss': '34595435184128.000000', 'mae': '3707968.500000', 'lr': '0.001000'} 
2023-05-26 02:45:44 [DEBUG] Epoch 1 - {'loss': '34595443572736.000000', 'mae': '3707969.000000', 'lr': '0.001000'} 
2023-05-26 02:45:44 [INFO] Training finished, elapsed time: 11.03 seconds
2023-05-26 02:45:44 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-26 02:49:17 [INFO] Experiment for resnet test begins at 2023/05/26 02:49:17
2023-05-26 02:49:17 [INFO] Model configurations:
2023-05-26 02:49:17 [INFO] name: resnet test
2023-05-26 02:49:17 [INFO] loss_function: mse
2023-05-26 02:49:17 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-26 02:49:17 [INFO] metric: mae
2023-05-26 02:49:17 [INFO] shuffle: True
2023-05-26 02:49:17 [INFO] epochs: 2
2023-05-26 02:49:17 [INFO] batch_size: 512
2023-05-26 02:49:17 [INFO] verbose: 2
2023-05-26 02:49:17 [INFO] TensorBoard_log_path: logs
2023-05-26 02:49:17 [INFO] TensorBoard_hist_freq: 1
2023-05-26 02:49:17 [INFO] EarlyStopping_monitor: loss
2023-05-26 02:49:17 [INFO] EarlyStopping_patience: 15
2023-05-26 02:49:17 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-26 02:49:17 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-26 02:49:17 [INFO] ReduceLROnPlateau_patience: 3
2023-05-26 02:49:17 [INFO] Checkpoint_monitor: loss
2023-05-26 02:49:17 [INFO] save_best_only: True
2023-05-26 02:49:17 [INFO] log_path: logs/training.log
2023-05-26 02:49:17 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-26 02:49:17 [INFO] Training in progress
2023-05-26 02:49:20 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-05-26 02:49:24 [DEBUG] Epoch 0 - {'loss': '34595437281280.000000', 'mae': '3707968.750000', 'lr': '0.001000'} 
2023-05-26 02:49:28 [DEBUG] Epoch 1 - {'loss': '34595445669888.000000', 'mae': '3707969.750000', 'lr': '0.001000'} 
2023-05-26 02:49:28 [INFO] Training finished, elapsed time: 11.15 seconds
2023-05-26 02:49:28 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-27 17:09:04 [INFO] Experiment for resnet test begins at 2023/05/27 17:09:04
2023-05-27 17:09:04 [INFO] Model configurations:
2023-05-27 17:09:04 [INFO] name: resnet test
2023-05-27 17:09:04 [INFO] loss_function: mse
2023-05-27 17:09:04 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:09:04 [INFO] metric: mae
2023-05-27 17:09:04 [INFO] shuffle: True
2023-05-27 17:09:04 [INFO] epochs: 2
2023-05-27 17:09:04 [INFO] batch_size: 512
2023-05-27 17:09:04 [INFO] verbose: 2
2023-05-27 17:09:04 [INFO] TensorBoard_log_path: logs
2023-05-27 17:09:04 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:09:04 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:09:04 [INFO] EarlyStopping_patience: 15
2023-05-27 17:09:04 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:09:04 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:09:04 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:09:04 [INFO] Checkpoint_monitor: loss
2023-05-27 17:09:04 [INFO] save_best_only: True
2023-05-27 17:09:04 [INFO] log_path: logs/training.log
2023-05-27 17:09:04 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-27 17:09:04 [INFO] Training in progress
2023-05-27 17:09:48 [INFO] Experiment for resnet test begins at 2023/05/27 17:09:48
2023-05-27 17:09:48 [INFO] Model configurations:
2023-05-27 17:09:48 [INFO] name: resnet test
2023-05-27 17:09:48 [INFO] loss_function: mse
2023-05-27 17:09:48 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:09:48 [INFO] metric: mae
2023-05-27 17:09:48 [INFO] shuffle: True
2023-05-27 17:09:48 [INFO] epochs: 2
2023-05-27 17:09:48 [INFO] batch_size: 512
2023-05-27 17:09:48 [INFO] verbose: 2
2023-05-27 17:09:48 [INFO] TensorBoard_log_path: logs
2023-05-27 17:09:48 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:09:48 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:09:48 [INFO] EarlyStopping_patience: 15
2023-05-27 17:09:48 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:09:48 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:09:48 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:09:48 [INFO] Checkpoint_monitor: loss
2023-05-27 17:09:48 [INFO] save_best_only: True
2023-05-27 17:09:48 [INFO] log_path: logs/training.log
2023-05-27 17:09:48 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-27 17:09:48 [INFO] Training in progress
2023-05-27 17:10:41 [INFO] Experiment for resnet test begins at 2023/05/27 17:10:41
2023-05-27 17:10:41 [INFO] Model configurations:
2023-05-27 17:10:41 [INFO] name: resnet test
2023-05-27 17:10:41 [INFO] loss_function: mse
2023-05-27 17:10:41 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:10:41 [INFO] metric: mae
2023-05-27 17:10:41 [INFO] shuffle: True
2023-05-27 17:10:41 [INFO] epochs: 2
2023-05-27 17:10:41 [INFO] batch_size: 512
2023-05-27 17:10:41 [INFO] verbose: 2
2023-05-27 17:10:41 [INFO] TensorBoard_log_path: logs
2023-05-27 17:10:41 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:10:41 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:10:41 [INFO] EarlyStopping_patience: 15
2023-05-27 17:10:41 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:10:41 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:10:41 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:10:41 [INFO] Checkpoint_monitor: loss
2023-05-27 17:10:41 [INFO] save_best_only: True
2023-05-27 17:10:41 [INFO] log_path: logs/training.log
2023-05-27 17:10:41 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-27 17:10:41 [INFO] Training in progress
2023-05-27 17:10:44 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-05-27 17:10:56 [DEBUG] Epoch 0 - {'loss': '7.913461', 'mae': '2.573441', 'lr': '0.001000'} 
2023-05-27 17:11:08 [DEBUG] Epoch 1 - {'loss': '7.903337', 'mae': '2.571055', 'lr': '0.001000'} 
2023-05-27 17:11:08 [INFO] Training finished, elapsed time: 26.54 seconds
2023-05-27 17:11:08 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-27 17:12:38 [INFO] Experiment for resnet test begins at 2023/05/27 17:12:38
2023-05-27 17:12:38 [INFO] Model configurations:
2023-05-27 17:12:38 [INFO] name: resnet test
2023-05-27 17:12:38 [INFO] loss_function: mse
2023-05-27 17:12:38 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:12:38 [INFO] metric: mae
2023-05-27 17:12:38 [INFO] shuffle: True
2023-05-27 17:12:38 [INFO] epochs: 2
2023-05-27 17:12:38 [INFO] batch_size: 512
2023-05-27 17:12:38 [INFO] verbose: 2
2023-05-27 17:12:38 [INFO] TensorBoard_log_path: logs
2023-05-27 17:12:38 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:12:38 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:12:38 [INFO] EarlyStopping_patience: 15
2023-05-27 17:12:38 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:12:38 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:12:38 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:12:38 [INFO] Checkpoint_monitor: loss
2023-05-27 17:12:38 [INFO] save_best_only: True
2023-05-27 17:12:38 [INFO] log_path: logs/training.log
2023-05-27 17:12:38 [INFO] save_model_path: models/pretrain_resnet.h5
2023-05-27 17:12:38 [INFO] Training in progress
2023-05-27 17:12:39 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 17:12:42 [DEBUG] Epoch 0 - {'loss': '7.909419', 'mae': '2.571494', 'lr': '0.001000'} 
2023-05-27 17:12:46 [DEBUG] Epoch 1 - {'loss': '7.901787', 'mae': '2.569676', 'lr': '0.001000'} 
2023-05-27 17:12:46 [INFO] Training finished, elapsed time: 7.29 seconds
2023-05-27 17:12:46 [INFO] model is saved in 'models/pretrain_resnet.h5'

2023-05-27 17:24:09 [INFO] Experiment for resnet test begins at 2023/05/27 17:24:09
2023-05-27 17:24:09 [INFO] Model configurations:
2023-05-27 17:24:09 [INFO] name: resnet test
2023-05-27 17:24:09 [INFO] loss_function: mse
2023-05-27 17:24:09 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:24:09 [INFO] metric: mae
2023-05-27 17:24:09 [INFO] shuffle: True
2023-05-27 17:24:09 [INFO] epochs: 2
2023-05-27 17:24:09 [INFO] batch_size: 512
2023-05-27 17:24:09 [INFO] verbose: 2
2023-05-27 17:24:09 [INFO] TensorBoard_log_path: logs
2023-05-27 17:24:09 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:24:09 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:24:09 [INFO] EarlyStopping_patience: 15
2023-05-27 17:24:09 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:24:09 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:24:09 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:24:09 [INFO] Checkpoint_monitor: loss
2023-05-27 17:24:09 [INFO] save_best_only: True
2023-05-27 17:24:09 [INFO] log_path: logs/training.log
2023-05-27 17:24:09 [INFO] save_model_path: models/best_epoch.h5
2023-05-27 17:24:10 [INFO] Training in progress
2023-05-27 17:24:10 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 17:24:14 [DEBUG] Epoch 0 - {'loss': '7.917665', 'mae': '2.573560', 'lr': '0.001000'} 
2023-05-27 17:24:17 [DEBUG] Epoch 1 - {'loss': '7.901847', 'mae': '2.569682', 'lr': '0.001000'} 
2023-05-27 17:24:17 [INFO] Training finished, elapsed time: 7.25 seconds
2023-05-27 17:24:17 [INFO] model is saved in 'models/best_epoch.h5'

2023-05-27 17:42:23 [INFO] Experiment for resnet test begins at 2023/05/27 17:42:23
2023-05-27 17:42:23 [INFO] Model configurations:
2023-05-27 17:42:23 [INFO] name: resnet test
2023-05-27 17:42:23 [INFO] loss_function: mse
2023-05-27 17:42:23 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:42:23 [INFO] metric: mae
2023-05-27 17:42:23 [INFO] shuffle: True
2023-05-27 17:42:23 [INFO] epochs: 2
2023-05-27 17:42:23 [INFO] batch_size: 512
2023-05-27 17:42:23 [INFO] verbose: 2
2023-05-27 17:42:23 [INFO] TensorBoard_log_path: logs
2023-05-27 17:42:23 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:42:23 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:42:23 [INFO] EarlyStopping_patience: 15
2023-05-27 17:42:23 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:42:23 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:42:23 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:42:23 [INFO] Checkpoint_monitor: loss
2023-05-27 17:42:23 [INFO] save_best_only: True
2023-05-27 17:42:23 [INFO] log_path: logs/training.log
2023-05-27 17:42:23 [INFO] save_model_path: models/${basename}_best_epoch.h5
2023-05-27 17:42:23 [INFO] Training in progress
2023-05-27 17:42:24 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-27 17:44:54 [INFO] Experiment for resnet test begins at 2023/05/27 17:44:54
2023-05-27 17:44:54 [INFO] Model configurations:
2023-05-27 17:44:54 [INFO] name: resnet test
2023-05-27 17:44:54 [INFO] loss_function: mse
2023-05-27 17:44:54 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:44:54 [INFO] metric: mae
2023-05-27 17:44:54 [INFO] shuffle: True
2023-05-27 17:44:54 [INFO] epochs: 2
2023-05-27 17:44:54 [INFO] batch_size: 512
2023-05-27 17:44:54 [INFO] verbose: 2
2023-05-27 17:44:54 [INFO] TensorBoard_log_path: logs
2023-05-27 17:44:54 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:44:54 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:44:54 [INFO] EarlyStopping_patience: 15
2023-05-27 17:44:54 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:44:54 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:44:54 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:44:54 [INFO] Checkpoint_monitor: loss
2023-05-27 17:44:54 [INFO] save_best_only: True
2023-05-27 17:44:54 [INFO] log_path: logs/training.log
2023-05-27 17:44:54 [INFO] save_model_path: models/${basename}_best_epoch.h5
2023-05-27 17:44:54 [INFO] Training in progress
2023-05-27 17:44:55 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 17:46:53 [INFO] Experiment for resnet test begins at 2023/05/27 17:46:53
2023-05-27 17:46:53 [INFO] Model configurations:
2023-05-27 17:46:53 [INFO] name: resnet test
2023-05-27 17:46:53 [INFO] loss_function: mse
2023-05-27 17:46:53 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:46:53 [INFO] metric: mae
2023-05-27 17:46:53 [INFO] shuffle: True
2023-05-27 17:46:53 [INFO] epochs: 2
2023-05-27 17:46:53 [INFO] batch_size: 512
2023-05-27 17:46:53 [INFO] verbose: 2
2023-05-27 17:46:53 [INFO] TensorBoard_log_path: logs
2023-05-27 17:46:53 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:46:53 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:46:53 [INFO] EarlyStopping_patience: 15
2023-05-27 17:46:53 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:46:53 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:46:53 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:46:53 [INFO] Checkpoint_monitor: loss
2023-05-27 17:46:53 [INFO] save_best_only: True
2023-05-27 17:46:53 [INFO] log_path: logs/training.log
2023-05-27 17:46:53 [INFO] save_model_path: models/${basename}_best_epoch.h5
2023-05-27 17:46:53 [INFO] Training in progress
2023-05-27 17:46:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 17:47:44 [INFO] Experiment for resnet test begins at 2023/05/27 17:47:44
2023-05-27 17:47:44 [INFO] Model configurations:
2023-05-27 17:47:44 [INFO] name: resnet test
2023-05-27 17:47:44 [INFO] loss_function: mse
2023-05-27 17:47:44 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:47:44 [INFO] metric: mae
2023-05-27 17:47:44 [INFO] shuffle: True
2023-05-27 17:47:44 [INFO] epochs: 2
2023-05-27 17:47:44 [INFO] batch_size: 512
2023-05-27 17:47:44 [INFO] verbose: 2
2023-05-27 17:47:44 [INFO] TensorBoard_log_path: logs
2023-05-27 17:47:44 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:47:44 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:47:44 [INFO] EarlyStopping_patience: 15
2023-05-27 17:47:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:47:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:47:44 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:47:44 [INFO] Checkpoint_monitor: loss
2023-05-27 17:47:44 [INFO] save_best_only: True
2023-05-27 17:47:44 [INFO] log_path: logs/training.log
2023-05-27 17:47:44 [INFO] save_model_path: models/${basename}_best_epoch.h5
2023-05-27 17:47:44 [INFO] Training in progress
2023-05-27 17:47:45 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 17:50:36 [INFO] Experiment for resnet test begins at 2023/05/27 17:50:36
2023-05-27 17:50:36 [INFO] Model configurations:
2023-05-27 17:50:36 [INFO] name: resnet test
2023-05-27 17:50:36 [INFO] loss_function: mse
2023-05-27 17:50:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:50:36 [INFO] metric: mae
2023-05-27 17:50:36 [INFO] shuffle: True
2023-05-27 17:50:36 [INFO] epochs: 2
2023-05-27 17:50:36 [INFO] batch_size: 512
2023-05-27 17:50:36 [INFO] verbose: 2
2023-05-27 17:50:36 [INFO] TensorBoard_log_path: logs
2023-05-27 17:50:36 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:50:36 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:50:36 [INFO] EarlyStopping_patience: 15
2023-05-27 17:50:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:50:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:50:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:50:36 [INFO] Checkpoint_monitor: loss
2023-05-27 17:50:36 [INFO] save_best_only: True
2023-05-27 17:50:36 [INFO] log_path: logs/training.log
2023-05-27 17:50:36 [INFO] save_model_path: models/${subject_id}_best_epoch.h5
2023-05-27 17:50:36 [INFO] Training in progress
2023-05-27 17:50:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 17:52:58 [INFO] Experiment for resnet test begins at 2023/05/27 17:52:58
2023-05-27 17:52:58 [INFO] Model configurations:
2023-05-27 17:52:58 [INFO] name: resnet test
2023-05-27 17:52:58 [INFO] loss_function: mse
2023-05-27 17:52:58 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 17:52:58 [INFO] metric: mae
2023-05-27 17:52:58 [INFO] shuffle: True
2023-05-27 17:52:58 [INFO] epochs: 2
2023-05-27 17:52:58 [INFO] batch_size: 512
2023-05-27 17:52:58 [INFO] verbose: 2
2023-05-27 17:52:58 [INFO] TensorBoard_log_path: logs
2023-05-27 17:52:58 [INFO] TensorBoard_hist_freq: 1
2023-05-27 17:52:58 [INFO] EarlyStopping_monitor: loss
2023-05-27 17:52:58 [INFO] EarlyStopping_patience: 15
2023-05-27 17:52:58 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 17:52:58 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 17:52:58 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 17:52:58 [INFO] Checkpoint_monitor: loss
2023-05-27 17:52:58 [INFO] save_best_only: True
2023-05-27 17:52:58 [INFO] log_path: logs/training.log
2023-05-27 17:52:58 [INFO] save_model_path: models/%{subject_id}s_best_epoch.h5
2023-05-27 17:52:58 [INFO] Training in progress
2023-05-27 17:52:59 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 18:03:26 [INFO] Experiment for resnet test begins at 2023/05/27 18:03:26
2023-05-27 18:03:26 [INFO] Model configurations:
2023-05-27 18:03:26 [INFO] name: resnet test
2023-05-27 18:03:26 [INFO] loss_function: mse
2023-05-27 18:03:26 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:03:26 [INFO] metric: mae
2023-05-27 18:03:26 [INFO] shuffle: True
2023-05-27 18:03:26 [INFO] epochs: 2
2023-05-27 18:03:26 [INFO] batch_size: 512
2023-05-27 18:03:26 [INFO] verbose: 2
2023-05-27 18:03:26 [INFO] TensorBoard_log_path: logs
2023-05-27 18:03:26 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:03:26 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:03:26 [INFO] EarlyStopping_patience: 15
2023-05-27 18:03:26 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:03:26 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:03:26 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:03:26 [INFO] Checkpoint_monitor: loss
2023-05-27 18:03:26 [INFO] save_best_only: True
2023-05-27 18:03:26 [INFO] log_path: logs/training.log
2023-05-27 18:03:26 [INFO] save_model_path: models/best_epoch.h5
2023-05-27 18:03:27 [INFO] Training in progress
2023-05-27 18:03:27 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 18:03:31 [DEBUG] Epoch 0 - {'loss': '7.909152', 'mae': '2.571725', 'lr': '0.001000'} 
2023-05-27 18:03:34 [DEBUG] Epoch 1 - {'loss': '7.901862', 'mae': '2.569790', 'lr': '0.001000'} 
2023-05-27 18:03:34 [INFO] Training finished, elapsed time: 7.24 seconds
2023-05-27 18:03:34 [INFO] model is saved in 'models/best_epoch.h5'

2023-05-27 18:07:23 [INFO] Experiment for resnet test begins at 2023/05/27 18:07:23
2023-05-27 18:07:23 [INFO] Model configurations:
2023-05-27 18:07:23 [INFO] name: resnet test
2023-05-27 18:07:23 [INFO] loss_function: mse
2023-05-27 18:07:23 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:07:23 [INFO] metric: mae
2023-05-27 18:07:23 [INFO] shuffle: True
2023-05-27 18:07:23 [INFO] epochs: 2
2023-05-27 18:07:23 [INFO] batch_size: 512
2023-05-27 18:07:23 [INFO] verbose: 2
2023-05-27 18:07:23 [INFO] TensorBoard_log_path: logs
2023-05-27 18:07:23 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:07:23 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:07:23 [INFO] EarlyStopping_patience: 15
2023-05-27 18:07:23 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:07:23 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:07:23 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:07:23 [INFO] Checkpoint_monitor: loss
2023-05-27 18:07:23 [INFO] save_best_only: True
2023-05-27 18:07:23 [INFO] log_path: logs/training.log
2023-05-27 18:07:23 [INFO] save_model_path: models/best_epoch.h5
2023-05-27 18:07:23 [INFO] Training in progress
2023-05-27 18:07:24 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 18:07:28 [DEBUG] Epoch 0 - {'loss': '7.908984', 'mae': '2.571466', 'lr': '0.001000'} 
2023-05-27 18:07:31 [DEBUG] Epoch 1 - {'loss': '7.901823', 'mae': '2.569677', 'lr': '0.001000'} 
2023-05-27 18:07:31 [INFO] Training finished, elapsed time: 7.27 seconds
2023-05-27 18:07:31 [INFO] model is saved in 'models/best_epoch.h5'

2023-05-27 18:08:24 [INFO] Experiment for resnet test begins at 2023/05/27 18:08:24
2023-05-27 18:08:24 [INFO] Model configurations:
2023-05-27 18:08:24 [INFO] name: resnet test
2023-05-27 18:08:24 [INFO] loss_function: mse
2023-05-27 18:08:24 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:08:24 [INFO] metric: mae
2023-05-27 18:08:24 [INFO] shuffle: True
2023-05-27 18:08:24 [INFO] epochs: 2
2023-05-27 18:08:24 [INFO] batch_size: 512
2023-05-27 18:08:24 [INFO] verbose: 2
2023-05-27 18:08:24 [INFO] TensorBoard_log_path: logs
2023-05-27 18:08:24 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:08:24 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:08:24 [INFO] EarlyStopping_patience: 15
2023-05-27 18:08:24 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:08:24 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:08:24 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:08:24 [INFO] Checkpoint_monitor: loss
2023-05-27 18:08:24 [INFO] save_best_only: True
2023-05-27 18:08:24 [INFO] log_path: logs/training.log
2023-05-27 18:08:24 [INFO] save_model_path: models/best_epoch.h5
2023-05-27 18:08:25 [INFO] Training in progress
2023-05-27 18:08:25 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 18:08:29 [DEBUG] Epoch 0 - {'loss': '7.908299', 'mae': '2.571360', 'lr': '0.001000'} 
2023-05-27 18:08:32 [DEBUG] Epoch 1 - {'loss': '7.901817', 'mae': '2.569682', 'lr': '0.001000'} 
2023-05-27 18:08:32 [INFO] Training finished, elapsed time: 7.30 seconds
2023-05-27 18:08:32 [INFO] model is saved in 'models/best_epoch.h5'

2023-05-27 18:13:52 [INFO] Experiment for resnet test begins at 2023/05/27 18:13:52
2023-05-27 18:13:52 [INFO] Model configurations:
2023-05-27 18:13:52 [INFO] name: resnet test
2023-05-27 18:13:52 [INFO] loss_function: mse
2023-05-27 18:13:52 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:13:52 [INFO] metric: mae
2023-05-27 18:13:52 [INFO] shuffle: True
2023-05-27 18:13:52 [INFO] epochs: 2
2023-05-27 18:13:52 [INFO] batch_size: 512
2023-05-27 18:13:52 [INFO] verbose: 2
2023-05-27 18:13:52 [INFO] TensorBoard_log_path: logs
2023-05-27 18:13:52 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:13:52 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:13:52 [INFO] EarlyStopping_patience: 15
2023-05-27 18:13:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:13:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:13:52 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:13:52 [INFO] Checkpoint_monitor: loss
2023-05-27 18:13:52 [INFO] save_best_only: True
2023-05-27 18:13:52 [INFO] log_path: logs/training.log
2023-05-27 18:13:52 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 18:13:52 [INFO] Training in progress
2023-05-27 18:13:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 18:13:56 [DEBUG] Epoch 0 - {'loss': '7.907800', 'mae': '2.571269', 'lr': '0.001000'} 
2023-05-27 18:14:00 [DEBUG] Epoch 1 - {'loss': '7.901812', 'mae': '2.569681', 'lr': '0.001000'} 
2023-05-27 18:14:00 [INFO] Training finished, elapsed time: 7.23 seconds
2023-05-27 18:14:00 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 18:15:50 [INFO] Experiment for resnet test begins at 2023/05/27 18:15:50
2023-05-27 18:15:50 [INFO] Model configurations:
2023-05-27 18:15:50 [INFO] name: resnet test
2023-05-27 18:15:50 [INFO] loss_function: mse
2023-05-27 18:15:50 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:15:50 [INFO] metric: mae
2023-05-27 18:15:50 [INFO] shuffle: True
2023-05-27 18:15:50 [INFO] epochs: 2
2023-05-27 18:15:50 [INFO] batch_size: 512
2023-05-27 18:15:50 [INFO] verbose: 2
2023-05-27 18:15:50 [INFO] TensorBoard_log_path: logs
2023-05-27 18:15:50 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:15:50 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:15:50 [INFO] EarlyStopping_patience: 15
2023-05-27 18:15:50 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:15:50 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:15:50 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:15:50 [INFO] Checkpoint_monitor: loss
2023-05-27 18:15:50 [INFO] save_best_only: True
2023-05-27 18:15:50 [INFO] log_path: logs/training.log
2023-05-27 18:15:50 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 18:15:50 [INFO] Training in progress
2023-05-27 18:15:51 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-27 18:15:54 [DEBUG] Epoch 0 - {'loss': '7.908579', 'mae': '2.571353', 'lr': '0.001000'} 
2023-05-27 18:15:58 [DEBUG] Epoch 1 - {'loss': '7.901807', 'mae': '2.569678', 'lr': '0.001000'} 
2023-05-27 18:15:58 [INFO] Training finished, elapsed time: 7.35 seconds
2023-05-27 18:15:58 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 18:16:58 [INFO] Experiment for resnet test begins at 2023/05/27 18:16:58
2023-05-27 18:16:58 [INFO] Model configurations:
2023-05-27 18:16:58 [INFO] name: resnet test
2023-05-27 18:16:58 [INFO] loss_function: mse
2023-05-27 18:16:58 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:16:58 [INFO] metric: mae
2023-05-27 18:16:58 [INFO] shuffle: True
2023-05-27 18:16:58 [INFO] epochs: 2
2023-05-27 18:16:58 [INFO] batch_size: 512
2023-05-27 18:16:58 [INFO] verbose: 2
2023-05-27 18:16:58 [INFO] TensorBoard_log_path: logs
2023-05-27 18:16:58 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:16:58 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:16:58 [INFO] EarlyStopping_patience: 15
2023-05-27 18:16:58 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:16:58 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:16:58 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:16:58 [INFO] Checkpoint_monitor: loss
2023-05-27 18:16:58 [INFO] save_best_only: True
2023-05-27 18:16:58 [INFO] log_path: logs/training.log
2023-05-27 18:16:58 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 18:16:58 [INFO] Training in progress
2023-05-27 18:16:58 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-27 18:17:02 [DEBUG] Epoch 0 - {'loss': '7.922481', 'mae': '2.574631', 'lr': '0.001000'} 
2023-05-27 18:17:05 [DEBUG] Epoch 1 - {'loss': '7.901790', 'mae': '2.569672', 'lr': '0.001000'} 
2023-05-27 18:17:05 [INFO] Training finished, elapsed time: 7.43 seconds
2023-05-27 18:17:05 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 18:20:55 [INFO] Experiment for resnet test begins at 2023/05/27 18:20:55
2023-05-27 18:20:55 [INFO] Model configurations:
2023-05-27 18:20:55 [INFO] name: resnet test
2023-05-27 18:20:55 [INFO] loss_function: mse
2023-05-27 18:20:55 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:20:55 [INFO] metric: mae
2023-05-27 18:20:55 [INFO] shuffle: True
2023-05-27 18:20:55 [INFO] epochs: 50
2023-05-27 18:20:55 [INFO] batch_size: 512
2023-05-27 18:20:55 [INFO] verbose: 2
2023-05-27 18:20:55 [INFO] TensorBoard_log_path: logs
2023-05-27 18:20:55 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:20:55 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:20:55 [INFO] EarlyStopping_patience: 15
2023-05-27 18:20:55 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:20:55 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:20:55 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:20:55 [INFO] Checkpoint_monitor: loss
2023-05-27 18:20:55 [INFO] save_best_only: True
2023-05-27 18:20:55 [INFO] log_path: logs/training.log
2023-05-27 18:20:55 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 18:20:55 [INFO] Training in progress
2023-05-27 18:20:56 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-27 18:20:59 [DEBUG] Epoch 0 - {'loss': '7.906638', 'mae': '2.570992', 'lr': '0.001000'} 
2023-05-27 18:21:02 [DEBUG] Epoch 1 - {'loss': '7.901799', 'mae': '2.569672', 'lr': '0.001000'} 
2023-05-27 18:21:05 [DEBUG] Epoch 2 - {'loss': '7.901759', 'mae': '2.569653', 'lr': '0.001000'} 
2023-05-27 18:21:09 [DEBUG] Epoch 3 - {'loss': '7.901779', 'mae': '2.569650', 'lr': '0.001000'} 
2023-05-27 18:21:12 [DEBUG] Epoch 4 - {'loss': '7.901743', 'mae': '2.569639', 'lr': '0.001000'} 
2023-05-27 18:21:15 [DEBUG] Epoch 5 - {'loss': '7.901726', 'mae': '2.569625', 'lr': '0.000500'} 
2023-05-27 18:21:18 [DEBUG] Epoch 6 - {'loss': '7.901705', 'mae': '2.569625', 'lr': '0.000500'} 
2023-05-27 18:21:22 [DEBUG] Epoch 7 - {'loss': '7.901729', 'mae': '2.569623', 'lr': '0.000500'} 
2023-05-27 18:21:25 [DEBUG] Epoch 8 - {'loss': '7.901763', 'mae': '2.569630', 'lr': '0.000250'} 
2023-05-27 18:21:28 [DEBUG] Epoch 9 - {'loss': '7.901742', 'mae': '2.569621', 'lr': '0.000250'} 
2023-05-27 18:21:31 [DEBUG] Epoch 10 - {'loss': '7.901746', 'mae': '2.569623', 'lr': '0.000250'} 
2023-05-27 18:21:34 [DEBUG] Epoch 11 - {'loss': '7.901738', 'mae': '2.569618', 'lr': '0.000125'} 
2023-05-27 18:21:37 [DEBUG] Epoch 12 - {'loss': '7.901697', 'mae': '2.569615', 'lr': '0.000125'} 
2023-05-27 18:21:41 [DEBUG] Epoch 13 - {'loss': '7.901747', 'mae': '2.569623', 'lr': '0.000125'} 
2023-05-27 18:21:44 [DEBUG] Epoch 14 - {'loss': '7.901737', 'mae': '2.569621', 'lr': '0.000125'} 
2023-05-27 18:21:47 [DEBUG] Epoch 15 - {'loss': '7.901742', 'mae': '2.569618', 'lr': '0.000125'} 
2023-05-27 18:21:50 [DEBUG] Epoch 16 - {'loss': '7.901732', 'mae': '2.569622', 'lr': '0.000063'} 
2023-05-27 18:21:53 [DEBUG] Epoch 17 - {'loss': '7.901752', 'mae': '2.569624', 'lr': '0.000063'} 
2023-05-27 18:21:57 [DEBUG] Epoch 18 - {'loss': '7.901733', 'mae': '2.569618', 'lr': '0.000063'} 
2023-05-27 18:22:00 [DEBUG] Epoch 19 - {'loss': '7.901745', 'mae': '2.569621', 'lr': '0.000031'} 
2023-05-27 18:22:03 [DEBUG] Epoch 20 - {'loss': '7.901725', 'mae': '2.569618', 'lr': '0.000031'} 
2023-05-27 18:22:06 [DEBUG] Epoch 21 - {'loss': '7.901732', 'mae': '2.569618', 'lr': '0.000031'} 
2023-05-27 18:22:09 [DEBUG] Epoch 22 - {'loss': '7.901698', 'mae': '2.569614', 'lr': '0.000016'} 
2023-05-27 18:22:12 [DEBUG] Epoch 23 - {'loss': '7.901731', 'mae': '2.569618', 'lr': '0.000016'} 
2023-05-27 18:22:16 [DEBUG] Epoch 24 - {'loss': '7.901717', 'mae': '2.569617', 'lr': '0.000016'} 
2023-05-27 18:22:19 [DEBUG] Epoch 25 - {'loss': '7.901732', 'mae': '2.569617', 'lr': '0.000008'} 
2023-05-27 18:22:22 [DEBUG] Epoch 26 - {'loss': '7.901753', 'mae': '2.569622', 'lr': '0.000008'} 
2023-05-27 18:22:25 [DEBUG] Epoch 27 - {'loss': '7.901728', 'mae': '2.569617', 'lr': '0.000008'} 
2023-05-27 18:22:25 [INFO] Training finished, elapsed time: 90.13 seconds
2023-05-27 18:22:25 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 18:23:45 [INFO] Experiment for resnet test begins at 2023/05/27 18:23:45
2023-05-27 18:23:45 [INFO] Model configurations:
2023-05-27 18:23:45 [INFO] name: resnet test
2023-05-27 18:23:45 [INFO] loss_function: mse
2023-05-27 18:23:45 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 18:23:45 [INFO] metric: mae
2023-05-27 18:23:45 [INFO] shuffle: True
2023-05-27 18:23:45 [INFO] epochs: 50
2023-05-27 18:23:45 [INFO] batch_size: 512
2023-05-27 18:23:45 [INFO] verbose: 2
2023-05-27 18:23:45 [INFO] TensorBoard_log_path: logs
2023-05-27 18:23:45 [INFO] TensorBoard_hist_freq: 1
2023-05-27 18:23:45 [INFO] EarlyStopping_monitor: loss
2023-05-27 18:23:45 [INFO] EarlyStopping_patience: 15
2023-05-27 18:23:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 18:23:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 18:23:45 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 18:23:45 [INFO] Checkpoint_monitor: loss
2023-05-27 18:23:45 [INFO] save_best_only: True
2023-05-27 18:23:45 [INFO] log_path: logs/training.log
2023-05-27 18:23:45 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 18:23:45 [INFO] Training in progress
2023-05-27 18:23:47 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.
2023-05-27 18:24:00 [DEBUG] Epoch 0 - {'loss': '7.916127', 'mae': '2.573700', 'lr': '0.001000'} 
2023-05-27 18:24:12 [DEBUG] Epoch 1 - {'loss': '7.902887', 'mae': '2.570565', 'lr': '0.001000'} 
2023-05-27 18:24:25 [DEBUG] Epoch 2 - {'loss': '7.902575', 'mae': '2.570485', 'lr': '0.001000'} 
2023-05-27 18:24:37 [DEBUG] Epoch 3 - {'loss': '7.902504', 'mae': '2.570488', 'lr': '0.001000'} 
2023-05-27 18:24:50 [DEBUG] Epoch 4 - {'loss': '7.902395', 'mae': '2.570446', 'lr': '0.001000'} 
2023-05-27 18:25:02 [DEBUG] Epoch 5 - {'loss': '7.902352', 'mae': '2.570428', 'lr': '0.001000'} 
2023-05-27 18:25:15 [DEBUG] Epoch 6 - {'loss': '7.902359', 'mae': '2.570452', 'lr': '0.001000'} 
2023-05-27 18:25:27 [DEBUG] Epoch 7 - {'loss': '7.902611', 'mae': '2.570748', 'lr': '0.001000'} 
2023-05-27 18:25:39 [DEBUG] Epoch 8 - {'loss': '7.902464', 'mae': '2.570632', 'lr': '0.000500'} 
2023-05-27 18:25:52 [DEBUG] Epoch 9 - {'loss': '7.902312', 'mae': '2.570440', 'lr': '0.000500'} 
2023-05-27 18:26:04 [DEBUG] Epoch 10 - {'loss': '7.902285', 'mae': '2.570398', 'lr': '0.000500'} 
2023-05-27 18:26:17 [DEBUG] Epoch 11 - {'loss': '7.902308', 'mae': '2.570410', 'lr': '0.000500'} 
2023-05-27 18:26:29 [DEBUG] Epoch 12 - {'loss': '7.902310', 'mae': '2.570405', 'lr': '0.000500'} 
2023-05-27 18:26:42 [DEBUG] Epoch 13 - {'loss': '7.902278', 'mae': '2.570409', 'lr': '0.000500'} 
2023-05-27 18:26:54 [DEBUG] Epoch 14 - {'loss': '7.902262', 'mae': '2.570411', 'lr': '0.000250'} 
2023-05-27 18:27:06 [DEBUG] Epoch 15 - {'loss': '7.902273', 'mae': '2.570392', 'lr': '0.000250'} 
2023-05-27 18:27:19 [DEBUG] Epoch 16 - {'loss': '7.902292', 'mae': '2.570406', 'lr': '0.000250'} 
2023-05-27 18:27:31 [DEBUG] Epoch 17 - {'loss': '7.902243', 'mae': '2.570386', 'lr': '0.000125'} 
2023-05-27 18:27:44 [DEBUG] Epoch 18 - {'loss': '7.902219', 'mae': '2.570375', 'lr': '0.000125'} 
2023-05-27 18:27:56 [DEBUG] Epoch 19 - {'loss': '7.902248', 'mae': '2.570393', 'lr': '0.000125'} 
2023-05-27 18:28:09 [DEBUG] Epoch 20 - {'loss': '7.902218', 'mae': '2.570384', 'lr': '0.000063'} 
2023-05-27 18:28:21 [DEBUG] Epoch 21 - {'loss': '7.902220', 'mae': '2.570392', 'lr': '0.000063'} 
2023-05-27 18:28:34 [DEBUG] Epoch 22 - {'loss': '7.902225', 'mae': '2.570385', 'lr': '0.000063'} 
2023-05-27 18:28:46 [DEBUG] Epoch 23 - {'loss': '7.902267', 'mae': '2.570385', 'lr': '0.000031'} 
2023-05-27 18:28:58 [DEBUG] Epoch 24 - {'loss': '7.902233', 'mae': '2.570382', 'lr': '0.000031'} 
2023-05-27 18:29:11 [DEBUG] Epoch 25 - {'loss': '7.902240', 'mae': '2.570387', 'lr': '0.000031'} 
2023-05-27 18:29:23 [DEBUG] Epoch 26 - {'loss': '7.902237', 'mae': '2.570381', 'lr': '0.000016'} 
2023-05-27 18:29:36 [DEBUG] Epoch 27 - {'loss': '7.902217', 'mae': '2.570384', 'lr': '0.000016'} 
2023-05-27 18:29:48 [DEBUG] Epoch 28 - {'loss': '7.902255', 'mae': '2.570390', 'lr': '0.000016'} 
2023-05-27 18:30:01 [DEBUG] Epoch 29 - {'loss': '7.902211', 'mae': '2.570380', 'lr': '0.000008'} 
2023-05-27 18:30:13 [DEBUG] Epoch 30 - {'loss': '7.902216', 'mae': '2.570381', 'lr': '0.000008'} 
2023-05-27 18:30:25 [DEBUG] Epoch 31 - {'loss': '7.902217', 'mae': '2.570384', 'lr': '0.000008'} 
2023-05-27 18:30:38 [DEBUG] Epoch 32 - {'loss': '7.902229', 'mae': '2.570383', 'lr': '0.000004'} 
2023-05-27 18:30:50 [DEBUG] Epoch 33 - {'loss': '7.902260', 'mae': '2.570388', 'lr': '0.000004'} 
2023-05-27 18:31:03 [DEBUG] Epoch 34 - {'loss': '7.902256', 'mae': '2.570391', 'lr': '0.000004'} 
2023-05-27 18:31:15 [DEBUG] Epoch 35 - {'loss': '7.902232', 'mae': '2.570383', 'lr': '0.000002'} 
2023-05-27 18:31:27 [DEBUG] Epoch 36 - {'loss': '7.902253', 'mae': '2.570382', 'lr': '0.000002'} 
2023-05-27 18:31:40 [DEBUG] Epoch 37 - {'loss': '7.902195', 'mae': '2.570377', 'lr': '0.000002'} 
2023-05-27 18:31:52 [DEBUG] Epoch 38 - {'loss': '7.902234', 'mae': '2.570383', 'lr': '0.000001'} 
2023-05-27 18:32:05 [DEBUG] Epoch 39 - {'loss': '7.902220', 'mae': '2.570382', 'lr': '0.000001'} 
2023-05-27 18:32:17 [DEBUG] Epoch 40 - {'loss': '7.902261', 'mae': '2.570389', 'lr': '0.000001'} 
2023-05-27 18:32:30 [DEBUG] Epoch 41 - {'loss': '7.902248', 'mae': '2.570386', 'lr': '0.000000'} 
2023-05-27 18:32:42 [DEBUG] Epoch 42 - {'loss': '7.902220', 'mae': '2.570376', 'lr': '0.000000'} 
2023-05-27 18:32:54 [DEBUG] Epoch 43 - {'loss': '7.902272', 'mae': '2.570392', 'lr': '0.000000'} 
2023-05-27 18:33:07 [DEBUG] Epoch 44 - {'loss': '7.902271', 'mae': '2.570388', 'lr': '0.000000'} 
2023-05-27 18:33:19 [DEBUG] Epoch 45 - {'loss': '7.902248', 'mae': '2.570387', 'lr': '0.000000'} 
2023-05-27 18:33:32 [DEBUG] Epoch 46 - {'loss': '7.902232', 'mae': '2.570385', 'lr': '0.000000'} 
2023-05-27 18:33:44 [DEBUG] Epoch 47 - {'loss': '7.902229', 'mae': '2.570384', 'lr': '0.000000'} 
2023-05-27 18:33:56 [DEBUG] Epoch 48 - {'loss': '7.902260', 'mae': '2.570389', 'lr': '0.000000'} 
2023-05-27 18:34:09 [DEBUG] Epoch 49 - {'loss': '7.902216', 'mae': '2.570382', 'lr': '0.000000'} 
2023-05-27 18:34:09 [INFO] Training finished, elapsed time: 624.04 seconds
2023-05-27 18:34:09 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 19:52:07 [INFO] Experiment for resnet test begins at 2023/05/27 19:52:07
2023-05-27 19:52:07 [INFO] Model configurations:
2023-05-27 19:52:07 [INFO] name: resnet test
2023-05-27 19:52:07 [INFO] loss_function: mse
2023-05-27 19:52:07 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 19:52:07 [INFO] metric: mae
2023-05-27 19:52:07 [INFO] shuffle: True
2023-05-27 19:52:07 [INFO] epochs: 10
2023-05-27 19:52:07 [INFO] batch_size: 512
2023-05-27 19:52:07 [INFO] verbose: 2
2023-05-27 19:52:07 [INFO] TensorBoard_log_path: logs
2023-05-27 19:52:07 [INFO] TensorBoard_hist_freq: 1
2023-05-27 19:52:07 [INFO] EarlyStopping_monitor: loss
2023-05-27 19:52:07 [INFO] EarlyStopping_patience: 15
2023-05-27 19:52:07 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 19:52:07 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 19:52:07 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 19:52:07 [INFO] Checkpoint_monitor: loss
2023-05-27 19:52:07 [INFO] save_best_only: True
2023-05-27 19:52:07 [INFO] log_path: logs/training.log
2023-05-27 19:52:07 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 19:52:07 [INFO] Training in progress
2023-05-27 19:52:10 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0121s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.
2023-05-27 19:52:22 [DEBUG] Epoch 0 - {'loss': '0.006524', 'mae': '0.035720', 'lr': '0.001000'} 
2023-05-27 19:52:34 [DEBUG] Epoch 1 - {'loss': '0.005834', 'mae': '0.031023', 'lr': '0.001000'} 
2023-05-27 19:52:47 [DEBUG] Epoch 2 - {'loss': '0.005787', 'mae': '0.030650', 'lr': '0.001000'} 
2023-05-27 19:52:59 [DEBUG] Epoch 3 - {'loss': '0.005781', 'mae': '0.030613', 'lr': '0.001000'} 
2023-05-27 19:53:12 [DEBUG] Epoch 4 - {'loss': '0.005780', 'mae': '0.030583', 'lr': '0.001000'} 
2023-05-27 19:53:24 [DEBUG] Epoch 5 - {'loss': '0.005742', 'mae': '0.030282', 'lr': '0.000500'} 
2023-05-27 19:53:36 [DEBUG] Epoch 6 - {'loss': '0.005737', 'mae': '0.030236', 'lr': '0.000500'} 
2023-05-27 19:53:49 [DEBUG] Epoch 7 - {'loss': '0.005735', 'mae': '0.030230', 'lr': '0.000500'} 
2023-05-27 19:54:01 [DEBUG] Epoch 8 - {'loss': '0.005726', 'mae': '0.030132', 'lr': '0.000250'} 
2023-05-27 19:54:14 [DEBUG] Epoch 9 - {'loss': '0.005724', 'mae': '0.030117', 'lr': '0.000250'} 
2023-05-27 19:54:14 [INFO] Training finished, elapsed time: 126.58 seconds
2023-05-27 19:54:14 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 20:13:36 [INFO] Experiment for resnet test begins at 2023/05/27 20:13:36
2023-05-27 20:13:36 [INFO] Model configurations:
2023-05-27 20:13:36 [INFO] name: resnet test
2023-05-27 20:13:36 [INFO] loss_function: mse
2023-05-27 20:13:36 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 20:13:36 [INFO] metric: mae
2023-05-27 20:13:36 [INFO] shuffle: True
2023-05-27 20:13:36 [INFO] epochs: 2
2023-05-27 20:13:36 [INFO] batch_size: 512
2023-05-27 20:13:36 [INFO] verbose: 2
2023-05-27 20:13:36 [INFO] TensorBoard_log_path: logs
2023-05-27 20:13:36 [INFO] TensorBoard_hist_freq: 1
2023-05-27 20:13:36 [INFO] EarlyStopping_monitor: loss
2023-05-27 20:13:36 [INFO] EarlyStopping_patience: 15
2023-05-27 20:13:36 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 20:13:36 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 20:13:36 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 20:13:36 [INFO] Checkpoint_monitor: loss
2023-05-27 20:13:36 [INFO] save_best_only: True
2023-05-27 20:13:36 [INFO] log_path: logs/training.log
2023-05-27 20:13:36 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 20:13:36 [INFO] Training in progress
2023-05-27 20:13:39 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0215s). Check your callbacks.
2023-05-27 20:13:51 [DEBUG] Epoch 0 - {'loss': '0.990339', 'mae': '0.629060', 'lr': '0.001000'} 
2023-05-27 20:14:04 [DEBUG] Epoch 1 - {'loss': '0.926388', 'mae': '0.594305', 'lr': '0.001000'} 
2023-05-27 20:14:04 [INFO] Training finished, elapsed time: 27.39 seconds
2023-05-27 20:14:04 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 20:26:10 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 20:26:10
2023-05-27 20:26:10 [INFO] Model configurations:
2023-05-27 20:26:10 [INFO] name: WF_T_21
2023-05-27 20:26:10 [INFO] loss_function: mse
2023-05-27 20:26:10 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 20:26:10 [INFO] metric: mae
2023-05-27 20:26:10 [INFO] shuffle: True
2023-05-27 20:26:10 [INFO] epochs: 20
2023-05-27 20:26:10 [INFO] batch_size: 512
2023-05-27 20:26:10 [INFO] verbose: 2
2023-05-27 20:26:10 [INFO] TensorBoard_log_path: logs
2023-05-27 20:26:10 [INFO] TensorBoard_hist_freq: 1
2023-05-27 20:26:10 [INFO] EarlyStopping_monitor: loss
2023-05-27 20:26:10 [INFO] EarlyStopping_patience: 15
2023-05-27 20:26:10 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 20:26:10 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 20:26:10 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 20:26:10 [INFO] Checkpoint_monitor: loss
2023-05-27 20:26:10 [INFO] save_best_only: True
2023-05-27 20:26:10 [INFO] log_path: logs/training.log
2023-05-27 20:26:10 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 20:26:11 [INFO] Training in progress
2023-05-27 20:26:11 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 20:26:15 [DEBUG] Epoch 0 - {'loss': '0.005899', 'mae': '0.031029', 'lr': '0.001000'} 
2023-05-27 20:26:18 [DEBUG] Epoch 1 - {'loss': '0.005754', 'mae': '0.030268', 'lr': '0.001000'} 
2023-05-27 20:26:21 [DEBUG] Epoch 2 - {'loss': '0.005745', 'mae': '0.030218', 'lr': '0.001000'} 
2023-05-27 20:26:25 [DEBUG] Epoch 3 - {'loss': '0.005742', 'mae': '0.030207', 'lr': '0.001000'} 
2023-05-27 20:26:28 [DEBUG] Epoch 4 - {'loss': '0.005741', 'mae': '0.030197', 'lr': '0.001000'} 
2023-05-27 20:26:31 [DEBUG] Epoch 5 - {'loss': '0.005736', 'mae': '0.030143', 'lr': '0.000500'} 
2023-05-27 20:26:34 [DEBUG] Epoch 6 - {'loss': '0.005736', 'mae': '0.030144', 'lr': '0.000500'} 
2023-05-27 20:26:43 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 20:26:43
2023-05-27 20:26:43 [INFO] Model configurations:
2023-05-27 20:26:43 [INFO] name: WF_T_21
2023-05-27 20:26:43 [INFO] loss_function: mse
2023-05-27 20:26:43 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 20:26:43 [INFO] metric: mae
2023-05-27 20:26:43 [INFO] shuffle: True
2023-05-27 20:26:43 [INFO] epochs: 20
2023-05-27 20:26:43 [INFO] batch_size: 512
2023-05-27 20:26:43 [INFO] verbose: 2
2023-05-27 20:26:43 [INFO] TensorBoard_log_path: logs
2023-05-27 20:26:43 [INFO] TensorBoard_hist_freq: 1
2023-05-27 20:26:43 [INFO] EarlyStopping_monitor: loss
2023-05-27 20:26:43 [INFO] EarlyStopping_patience: 15
2023-05-27 20:26:43 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 20:26:43 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 20:26:43 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 20:26:43 [INFO] Checkpoint_monitor: loss
2023-05-27 20:26:43 [INFO] save_best_only: True
2023-05-27 20:26:43 [INFO] log_path: logs/training.log
2023-05-27 20:26:43 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 20:26:44 [INFO] Training in progress
2023-05-27 20:26:44 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-27 20:26:48 [DEBUG] Epoch 0 - {'loss': '0.006799', 'mae': '0.038520', 'lr': '0.001000'} 
2023-05-27 20:26:51 [DEBUG] Epoch 1 - {'loss': '0.006480', 'mae': '0.036583', 'lr': '0.001000'} 
2023-05-27 20:26:54 [DEBUG] Epoch 2 - {'loss': '0.006458', 'mae': '0.036430', 'lr': '0.001000'} 
2023-05-27 20:26:57 [DEBUG] Epoch 3 - {'loss': '0.006449', 'mae': '0.036370', 'lr': '0.001000'} 
2023-05-27 20:27:00 [DEBUG] Epoch 4 - {'loss': '0.006457', 'mae': '0.036462', 'lr': '0.001000'} 
2023-05-27 20:27:04 [DEBUG] Epoch 5 - {'loss': '0.006432', 'mae': '0.036240', 'lr': '0.000500'} 
2023-05-27 20:27:07 [DEBUG] Epoch 6 - {'loss': '0.006431', 'mae': '0.036234', 'lr': '0.000500'} 
2023-05-27 20:27:10 [DEBUG] Epoch 7 - {'loss': '0.006430', 'mae': '0.036232', 'lr': '0.000500'} 
2023-05-27 20:27:13 [DEBUG] Epoch 8 - {'loss': '0.006427', 'mae': '0.036208', 'lr': '0.000250'} 
2023-05-27 20:27:17 [DEBUG] Epoch 9 - {'loss': '0.006426', 'mae': '0.036202', 'lr': '0.000250'} 
2023-05-27 20:27:20 [DEBUG] Epoch 10 - {'loss': '0.006426', 'mae': '0.036204', 'lr': '0.000250'} 
2023-05-27 20:27:23 [DEBUG] Epoch 11 - {'loss': '0.006425', 'mae': '0.036199', 'lr': '0.000125'} 
2023-05-27 20:27:26 [DEBUG] Epoch 12 - {'loss': '0.006426', 'mae': '0.036200', 'lr': '0.000125'} 
2023-05-27 20:27:30 [DEBUG] Epoch 13 - {'loss': '0.006425', 'mae': '0.036196', 'lr': '0.000125'} 
2023-05-27 20:27:33 [DEBUG] Epoch 14 - {'loss': '0.006425', 'mae': '0.036196', 'lr': '0.000063'} 
2023-05-27 20:27:36 [DEBUG] Epoch 15 - {'loss': '0.006425', 'mae': '0.036196', 'lr': '0.000063'} 
2023-05-27 20:27:39 [DEBUG] Epoch 16 - {'loss': '0.006426', 'mae': '0.036196', 'lr': '0.000063'} 
2023-05-27 20:27:43 [DEBUG] Epoch 17 - {'loss': '0.006425', 'mae': '0.036193', 'lr': '0.000031'} 
2023-05-27 20:27:46 [DEBUG] Epoch 18 - {'loss': '0.006424', 'mae': '0.036191', 'lr': '0.000031'} 
2023-05-27 20:27:49 [DEBUG] Epoch 19 - {'loss': '0.006425', 'mae': '0.036192', 'lr': '0.000031'} 
2023-05-27 20:27:49 [INFO] Training finished, elapsed time: 65.53 seconds
2023-05-27 20:27:49 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 20:29:41 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 20:29:41
2023-05-27 20:29:41 [INFO] Model configurations:
2023-05-27 20:29:41 [INFO] name: WF_T_21
2023-05-27 20:29:41 [INFO] loss_function: mse
2023-05-27 20:29:41 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 20:29:41 [INFO] metric: mae
2023-05-27 20:29:41 [INFO] shuffle: True
2023-05-27 20:29:41 [INFO] epochs: 20
2023-05-27 20:29:41 [INFO] batch_size: 512
2023-05-27 20:29:41 [INFO] verbose: 2
2023-05-27 20:29:41 [INFO] TensorBoard_log_path: logs
2023-05-27 20:29:41 [INFO] TensorBoard_hist_freq: 1
2023-05-27 20:29:41 [INFO] EarlyStopping_monitor: loss
2023-05-27 20:29:41 [INFO] EarlyStopping_patience: 15
2023-05-27 20:29:41 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 20:29:41 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 20:29:41 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 20:29:41 [INFO] Checkpoint_monitor: loss
2023-05-27 20:29:41 [INFO] save_best_only: True
2023-05-27 20:29:41 [INFO] log_path: logs/training.log
2023-05-27 20:29:41 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 20:29:41 [INFO] Training in progress
2023-05-27 20:29:42 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-27 20:29:45 [DEBUG] Epoch 0 - {'loss': '0.111709', 'mae': '0.226712', 'lr': '0.001000'} 
2023-05-27 20:29:48 [DEBUG] Epoch 1 - {'loss': '0.082674', 'mae': '0.195142', 'lr': '0.001000'} 
2023-05-27 20:29:51 [DEBUG] Epoch 2 - {'loss': '0.079160', 'mae': '0.190068', 'lr': '0.001000'} 
2023-05-27 20:29:55 [DEBUG] Epoch 3 - {'loss': '0.081770', 'mae': '0.194709', 'lr': '0.001000'} 
2023-05-27 20:29:58 [DEBUG] Epoch 4 - {'loss': '0.070252', 'mae': '0.178807', 'lr': '0.001000'} 
2023-05-27 20:30:01 [DEBUG] Epoch 5 - {'loss': '0.069357', 'mae': '0.177417', 'lr': '0.001000'} 
2023-05-27 20:30:04 [DEBUG] Epoch 6 - {'loss': '0.068877', 'mae': '0.176704', 'lr': '0.001000'} 
2023-05-27 20:30:08 [DEBUG] Epoch 7 - {'loss': '0.068544', 'mae': '0.176214', 'lr': '0.001000'} 
2023-05-27 20:30:11 [DEBUG] Epoch 8 - {'loss': '0.068314', 'mae': '0.175857', 'lr': '0.001000'} 
2023-05-27 20:30:14 [DEBUG] Epoch 9 - {'loss': '0.068124', 'mae': '0.175565', 'lr': '0.001000'} 
2023-05-27 20:30:17 [DEBUG] Epoch 10 - {'loss': '0.067942', 'mae': '0.175268', 'lr': '0.001000'} 
2023-05-27 20:30:21 [DEBUG] Epoch 11 - {'loss': '0.067915', 'mae': '0.175231', 'lr': '0.001000'} 
2023-05-27 20:30:24 [DEBUG] Epoch 12 - {'loss': '0.067770', 'mae': '0.174964', 'lr': '0.001000'} 
2023-05-27 20:30:27 [DEBUG] Epoch 13 - {'loss': '0.067768', 'mae': '0.174986', 'lr': '0.001000'} 
2023-05-27 20:30:30 [DEBUG] Epoch 14 - {'loss': '0.067607', 'mae': '0.174686', 'lr': '0.001000'} 
2023-05-27 20:30:34 [DEBUG] Epoch 15 - {'loss': '0.067596', 'mae': '0.174648', 'lr': '0.001000'} 
2023-05-27 20:30:37 [DEBUG] Epoch 16 - {'loss': '0.069617', 'mae': '0.178049', 'lr': '0.001000'} 
2023-05-27 20:30:40 [DEBUG] Epoch 17 - {'loss': '0.068218', 'mae': '0.175448', 'lr': '0.001000'} 
2023-05-27 20:30:43 [DEBUG] Epoch 18 - {'loss': '0.067720', 'mae': '0.174575', 'lr': '0.000500'} 
2023-05-27 20:30:46 [DEBUG] Epoch 19 - {'loss': '0.067572', 'mae': '0.174360', 'lr': '0.000500'} 
2023-05-27 20:30:46 [INFO] Training finished, elapsed time: 65.36 seconds
2023-05-27 20:30:46 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 20:31:51 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 20:31:51
2023-05-27 20:31:51 [INFO] Model configurations:
2023-05-27 20:31:51 [INFO] name: WF_T_21
2023-05-27 20:31:51 [INFO] loss_function: mse
2023-05-27 20:31:51 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 20:31:51 [INFO] metric: mae
2023-05-27 20:31:51 [INFO] shuffle: True
2023-05-27 20:31:51 [INFO] epochs: 20
2023-05-27 20:31:51 [INFO] batch_size: 512
2023-05-27 20:31:51 [INFO] verbose: 2
2023-05-27 20:31:51 [INFO] TensorBoard_log_path: logs
2023-05-27 20:31:51 [INFO] TensorBoard_hist_freq: 1
2023-05-27 20:31:51 [INFO] EarlyStopping_monitor: loss
2023-05-27 20:31:51 [INFO] EarlyStopping_patience: 15
2023-05-27 20:31:51 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 20:31:51 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 20:31:51 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 20:31:51 [INFO] Checkpoint_monitor: loss
2023-05-27 20:31:51 [INFO] save_best_only: True
2023-05-27 20:31:51 [INFO] log_path: logs/training.log
2023-05-27 20:31:51 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 20:31:51 [INFO] Training in progress
2023-05-27 20:31:54 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-05-27 20:32:06 [DEBUG] Epoch 0 - {'loss': '0.096773', 'mae': '0.207937', 'lr': '0.001000'} 
2023-05-27 20:32:19 [DEBUG] Epoch 1 - {'loss': '0.070990', 'mae': '0.180412', 'lr': '0.001000'} 
2023-05-27 20:32:31 [DEBUG] Epoch 2 - {'loss': '0.068954', 'mae': '0.177304', 'lr': '0.001000'} 
2023-05-27 20:32:43 [DEBUG] Epoch 3 - {'loss': '0.068406', 'mae': '0.176454', 'lr': '0.001000'} 
2023-05-27 20:32:56 [DEBUG] Epoch 4 - {'loss': '0.068310', 'mae': '0.176298', 'lr': '0.001000'} 
2023-05-27 20:33:08 [DEBUG] Epoch 5 - {'loss': '0.068057', 'mae': '0.175873', 'lr': '0.001000'} 
2023-05-27 20:33:20 [DEBUG] Epoch 6 - {'loss': '0.067905', 'mae': '0.175643', 'lr': '0.001000'} 
2023-05-27 20:33:33 [DEBUG] Epoch 7 - {'loss': '0.067856', 'mae': '0.175579', 'lr': '0.001000'} 
2023-05-27 20:33:45 [DEBUG] Epoch 8 - {'loss': '0.067603', 'mae': '0.175205', 'lr': '0.001000'} 
2023-05-27 20:33:58 [DEBUG] Epoch 9 - {'loss': '0.067548', 'mae': '0.175115', 'lr': '0.001000'} 
2023-05-27 20:34:10 [DEBUG] Epoch 10 - {'loss': '0.067726', 'mae': '0.175367', 'lr': '0.001000'} 
2023-05-27 20:34:22 [DEBUG] Epoch 11 - {'loss': '0.067454', 'mae': '0.174966', 'lr': '0.001000'} 
2023-05-27 20:34:35 [DEBUG] Epoch 12 - {'loss': '0.067426', 'mae': '0.174911', 'lr': '0.001000'} 
2023-05-27 20:34:47 [DEBUG] Epoch 13 - {'loss': '0.067378', 'mae': '0.174832', 'lr': '0.001000'} 
2023-05-27 20:35:00 [DEBUG] Epoch 14 - {'loss': '0.067344', 'mae': '0.174802', 'lr': '0.001000'} 
2023-05-27 20:35:12 [DEBUG] Epoch 15 - {'loss': '0.067207', 'mae': '0.174560', 'lr': '0.001000'} 
2023-05-27 20:35:24 [DEBUG] Epoch 16 - {'loss': '0.067177', 'mae': '0.174504', 'lr': '0.001000'} 
2023-05-27 20:35:37 [DEBUG] Epoch 17 - {'loss': '0.067168', 'mae': '0.174502', 'lr': '0.001000'} 
2023-05-27 20:35:49 [DEBUG] Epoch 18 - {'loss': '0.067185', 'mae': '0.174534', 'lr': '0.001000'} 
2023-05-27 20:36:01 [DEBUG] Epoch 19 - {'loss': '0.066820', 'mae': '0.173918', 'lr': '0.000500'} 
2023-05-27 20:36:01 [INFO] Training finished, elapsed time: 250.46 seconds
2023-05-27 20:36:01 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 21:00:28 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 21:00:28
2023-05-27 21:00:28 [INFO] Model configurations:
2023-05-27 21:00:28 [INFO] name: WF_T_21
2023-05-27 21:00:28 [INFO] loss_function: mse
2023-05-27 21:00:28 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 21:00:28 [INFO] metric: mae
2023-05-27 21:00:28 [INFO] shuffle: True
2023-05-27 21:00:28 [INFO] epochs: 20
2023-05-27 21:00:28 [INFO] batch_size: 512
2023-05-27 21:00:28 [INFO] verbose: 2
2023-05-27 21:00:28 [INFO] TensorBoard_log_path: logs
2023-05-27 21:00:28 [INFO] TensorBoard_hist_freq: 1
2023-05-27 21:00:28 [INFO] EarlyStopping_monitor: loss
2023-05-27 21:00:28 [INFO] EarlyStopping_patience: 15
2023-05-27 21:00:28 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 21:00:28 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 21:00:28 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 21:00:28 [INFO] Checkpoint_monitor: loss
2023-05-27 21:00:28 [INFO] save_best_only: True
2023-05-27 21:00:28 [INFO] log_path: logs/training.log
2023-05-27 21:00:28 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 21:00:29 [INFO] Training in progress
2023-05-27 21:00:29 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0070s). Check your callbacks.
2023-05-27 21:00:33 [DEBUG] Epoch 0 - {'loss': '0.134908', 'mae': '0.243770', 'lr': '0.001000'} 
2023-05-27 21:00:37 [DEBUG] Epoch 1 - {'loss': '0.092424', 'mae': '0.208258', 'lr': '0.001000'} 
2023-05-27 21:00:41 [DEBUG] Epoch 2 - {'loss': '0.080962', 'mae': '0.192908', 'lr': '0.001000'} 
2023-05-27 21:00:45 [DEBUG] Epoch 3 - {'loss': '0.076681', 'mae': '0.188092', 'lr': '0.001000'} 
2023-05-27 21:00:48 [DEBUG] Epoch 4 - {'loss': '0.070576', 'mae': '0.179334', 'lr': '0.001000'} 
2023-05-27 21:00:52 [DEBUG] Epoch 5 - {'loss': '0.069894', 'mae': '0.178235', 'lr': '0.001000'} 
2023-05-27 21:00:56 [DEBUG] Epoch 6 - {'loss': '0.069336', 'mae': '0.177319', 'lr': '0.001000'} 
2023-05-27 21:01:00 [DEBUG] Epoch 7 - {'loss': '0.069065', 'mae': '0.176892', 'lr': '0.001000'} 
2023-05-27 21:01:03 [DEBUG] Epoch 8 - {'loss': '0.068680', 'mae': '0.176326', 'lr': '0.001000'} 
2023-05-27 21:01:07 [DEBUG] Epoch 9 - {'loss': '0.068447', 'mae': '0.175945', 'lr': '0.001000'} 
2023-05-27 21:01:11 [DEBUG] Epoch 10 - {'loss': '0.068306', 'mae': '0.175799', 'lr': '0.001000'} 
2023-05-27 21:01:15 [DEBUG] Epoch 11 - {'loss': '0.068047', 'mae': '0.175408', 'lr': '0.001000'} 
2023-05-27 21:01:18 [DEBUG] Epoch 12 - {'loss': '0.067942', 'mae': '0.175247', 'lr': '0.001000'} 
2023-05-27 21:01:22 [DEBUG] Epoch 13 - {'loss': '0.067848', 'mae': '0.175109', 'lr': '0.001000'} 
2023-05-27 21:01:26 [DEBUG] Epoch 14 - {'loss': '0.067767', 'mae': '0.174976', 'lr': '0.001000'} 
2023-05-27 21:01:30 [DEBUG] Epoch 15 - {'loss': '0.067681', 'mae': '0.174809', 'lr': '0.001000'} 
2023-05-27 21:01:34 [DEBUG] Epoch 16 - {'loss': '0.067657', 'mae': '0.174743', 'lr': '0.001000'} 
2023-05-27 21:01:37 [DEBUG] Epoch 17 - {'loss': '0.067566', 'mae': '0.174588', 'lr': '0.001000'} 
2023-05-27 21:01:41 [DEBUG] Epoch 18 - {'loss': '0.067586', 'mae': '0.174651', 'lr': '0.001000'} 
2023-05-27 21:01:45 [DEBUG] Epoch 19 - {'loss': '0.067454', 'mae': '0.174392', 'lr': '0.001000'} 
2023-05-27 21:01:45 [INFO] Training finished, elapsed time: 76.26 seconds
2023-05-27 21:01:45 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 21:06:45 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 21:06:45
2023-05-27 21:06:45 [INFO] Model configurations:
2023-05-27 21:06:45 [INFO] name: WF_T_21
2023-05-27 21:06:45 [INFO] loss_function: mse
2023-05-27 21:06:45 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 21:06:45 [INFO] metric: mae
2023-05-27 21:06:45 [INFO] shuffle: True
2023-05-27 21:06:45 [INFO] epochs: 2
2023-05-27 21:06:45 [INFO] batch_size: 512
2023-05-27 21:06:45 [INFO] verbose: 2
2023-05-27 21:06:45 [INFO] TensorBoard_log_path: logs
2023-05-27 21:06:45 [INFO] TensorBoard_hist_freq: 1
2023-05-27 21:06:45 [INFO] EarlyStopping_monitor: loss
2023-05-27 21:06:45 [INFO] EarlyStopping_patience: 15
2023-05-27 21:06:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 21:06:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 21:06:45 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 21:06:45 [INFO] Checkpoint_monitor: loss
2023-05-27 21:06:45 [INFO] save_best_only: True
2023-05-27 21:06:45 [INFO] log_path: logs/training.log
2023-05-27 21:06:45 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 21:06:45 [INFO] Training in progress
2023-05-27 21:06:46 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0067s). Check your callbacks.
2023-05-27 21:06:50 [DEBUG] Epoch 0 - {'loss': '0.113262', 'mae': '0.230471', 'lr': '0.001000'} 
2023-05-27 21:06:54 [DEBUG] Epoch 1 - {'loss': '0.084172', 'mae': '0.198005', 'lr': '0.001000'} 
2023-05-27 21:06:54 [INFO] Training finished, elapsed time: 8.29 seconds
2023-05-27 21:06:54 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 21:10:46 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 21:10:46
2023-05-27 21:10:46 [INFO] Model configurations:
2023-05-27 21:10:46 [INFO] name: WF_T_21
2023-05-27 21:10:46 [INFO] loss_function: mse
2023-05-27 21:10:46 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 21:10:46 [INFO] metric: mae
2023-05-27 21:10:46 [INFO] shuffle: True
2023-05-27 21:10:46 [INFO] epochs: 2
2023-05-27 21:10:46 [INFO] batch_size: 512
2023-05-27 21:10:46 [INFO] verbose: 2
2023-05-27 21:10:46 [INFO] TensorBoard_log_path: logs
2023-05-27 21:10:46 [INFO] TensorBoard_hist_freq: 1
2023-05-27 21:10:46 [INFO] EarlyStopping_monitor: loss
2023-05-27 21:10:46 [INFO] EarlyStopping_patience: 15
2023-05-27 21:10:46 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 21:10:46 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 21:10:46 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 21:10:46 [INFO] Checkpoint_monitor: loss
2023-05-27 21:10:46 [INFO] save_best_only: True
2023-05-27 21:10:46 [INFO] log_path: logs/training.log
2023-05-27 21:10:46 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 21:10:46 [INFO] Training in progress
2023-05-27 21:10:46 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 21:10:50 [DEBUG] Epoch 0 - {'loss': '0.132285', 'mae': '0.237900', 'lr': '0.001000'} 
2023-05-27 21:10:53 [DEBUG] Epoch 1 - {'loss': '0.088229', 'mae': '0.203406', 'lr': '0.001000'} 
2023-05-27 21:10:53 [INFO] Training finished, elapsed time: 7.28 seconds
2023-05-27 21:10:53 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 21:11:35 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 21:11:35
2023-05-27 21:11:35 [INFO] Model configurations:
2023-05-27 21:11:35 [INFO] name: WF_T_21
2023-05-27 21:11:35 [INFO] loss_function: mse
2023-05-27 21:11:35 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 21:11:35 [INFO] metric: mae
2023-05-27 21:11:35 [INFO] shuffle: True
2023-05-27 21:11:35 [INFO] epochs: 2
2023-05-27 21:11:35 [INFO] batch_size: 512
2023-05-27 21:11:35 [INFO] verbose: 2
2023-05-27 21:11:35 [INFO] TensorBoard_log_path: logs
2023-05-27 21:11:35 [INFO] TensorBoard_hist_freq: 1
2023-05-27 21:11:35 [INFO] EarlyStopping_monitor: loss
2023-05-27 21:11:35 [INFO] EarlyStopping_patience: 15
2023-05-27 21:11:35 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 21:11:35 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 21:11:35 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 21:11:35 [INFO] Checkpoint_monitor: loss
2023-05-27 21:11:35 [INFO] save_best_only: True
2023-05-27 21:11:35 [INFO] log_path: logs/training.log
2023-05-27 21:11:35 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 21:11:35 [INFO] Training in progress
2023-05-27 21:11:36 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.
2023-05-27 21:11:40 [DEBUG] Epoch 0 - {'loss': '0.136012', 'mae': '0.242003', 'lr': '0.001000'} 
2023-05-27 21:11:43 [DEBUG] Epoch 1 - {'loss': '0.083351', 'mae': '0.196825', 'lr': '0.001000'} 
2023-05-27 21:11:43 [INFO] Training finished, elapsed time: 7.28 seconds
2023-05-27 21:11:43 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 22:13:44 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 22:13:44
2023-05-27 22:13:44 [INFO] Model configurations:
2023-05-27 22:13:44 [INFO] name: WF_T_21
2023-05-27 22:13:44 [INFO] loss_function: mse
2023-05-27 22:13:44 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 22:13:44 [INFO] metric: mae
2023-05-27 22:13:44 [INFO] shuffle: True
2023-05-27 22:13:44 [INFO] epochs: 2
2023-05-27 22:13:44 [INFO] batch_size: 512
2023-05-27 22:13:44 [INFO] verbose: 2
2023-05-27 22:13:44 [INFO] TensorBoard_log_path: logs
2023-05-27 22:13:44 [INFO] TensorBoard_hist_freq: 1
2023-05-27 22:13:44 [INFO] EarlyStopping_monitor: loss
2023-05-27 22:13:44 [INFO] EarlyStopping_patience: 15
2023-05-27 22:13:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 22:13:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 22:13:44 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 22:13:44 [INFO] Checkpoint_monitor: loss
2023-05-27 22:13:44 [INFO] save_best_only: True
2023-05-27 22:13:44 [INFO] log_path: logs/training.log
2023-05-27 22:13:44 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 22:13:44 [INFO] Training in progress
2023-05-27 22:13:45 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-05-27 22:13:48 [DEBUG] Epoch 0 - {'loss': '0.127056', 'mae': '0.236281', 'lr': '0.001000'} 
2023-05-27 22:13:51 [DEBUG] Epoch 1 - {'loss': '0.089487', 'mae': '0.203783', 'lr': '0.001000'} 
2023-05-27 22:13:51 [INFO] Training finished, elapsed time: 7.22 seconds
2023-05-27 22:13:51 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 22:14:52 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 22:14:52
2023-05-27 22:14:52 [INFO] Model configurations:
2023-05-27 22:14:52 [INFO] name: WF_T_21
2023-05-27 22:14:52 [INFO] loss_function: mse
2023-05-27 22:14:52 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 22:14:52 [INFO] metric: mae
2023-05-27 22:14:52 [INFO] shuffle: True
2023-05-27 22:14:52 [INFO] epochs: 2
2023-05-27 22:14:52 [INFO] batch_size: 512
2023-05-27 22:14:52 [INFO] verbose: 2
2023-05-27 22:14:52 [INFO] TensorBoard_log_path: logs
2023-05-27 22:14:52 [INFO] TensorBoard_hist_freq: 1
2023-05-27 22:14:52 [INFO] EarlyStopping_monitor: loss
2023-05-27 22:14:52 [INFO] EarlyStopping_patience: 15
2023-05-27 22:14:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 22:14:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 22:14:52 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 22:14:52 [INFO] Checkpoint_monitor: loss
2023-05-27 22:14:52 [INFO] save_best_only: True
2023-05-27 22:14:52 [INFO] log_path: logs/training.log
2023-05-27 22:14:52 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 22:14:52 [INFO] Training in progress
2023-05-27 22:14:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.
2023-05-27 22:14:56 [DEBUG] Epoch 0 - {'loss': '0.647056', 'mae': '0.588328', 'lr': '0.001000'} 
2023-05-27 22:14:59 [DEBUG] Epoch 1 - {'loss': '2.091262', 'mae': '1.089510', 'lr': '0.001000'} 
2023-05-27 22:14:59 [INFO] Training finished, elapsed time: 7.26 seconds
2023-05-27 22:14:59 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 22:25:07 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 22:25:07
2023-05-27 22:25:07 [INFO] Model configurations:
2023-05-27 22:25:07 [INFO] name: WF_T_21
2023-05-27 22:25:07 [INFO] loss_function: mse
2023-05-27 22:25:07 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 22:25:07 [INFO] metric: mae
2023-05-27 22:25:07 [INFO] shuffle: True
2023-05-27 22:25:07 [INFO] epochs: 2
2023-05-27 22:25:07 [INFO] batch_size: 512
2023-05-27 22:25:07 [INFO] verbose: 2
2023-05-27 22:25:07 [INFO] TensorBoard_log_path: logs
2023-05-27 22:25:07 [INFO] TensorBoard_hist_freq: 1
2023-05-27 22:25:07 [INFO] EarlyStopping_monitor: loss
2023-05-27 22:25:07 [INFO] EarlyStopping_patience: 15
2023-05-27 22:25:07 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 22:25:07 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 22:25:07 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 22:25:07 [INFO] Checkpoint_monitor: loss
2023-05-27 22:25:07 [INFO] save_best_only: True
2023-05-27 22:25:07 [INFO] log_path: logs/training.log
2023-05-27 22:25:07 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 22:25:07 [INFO] Training in progress
2023-05-27 22:25:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 22:25:11 [DEBUG] Epoch 0 - {'loss': '0.125559', 'mae': '0.233999', 'lr': '0.001000'} 
2023-05-27 22:25:14 [DEBUG] Epoch 1 - {'loss': '0.083164', 'mae': '0.196230', 'lr': '0.001000'} 
2023-05-27 22:25:14 [INFO] Training finished, elapsed time: 7.22 seconds
2023-05-27 22:25:14 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 22:26:10 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 22:26:10
2023-05-27 22:26:10 [INFO] Model configurations:
2023-05-27 22:26:10 [INFO] name: WF_T_21
2023-05-27 22:26:10 [INFO] loss_function: mse
2023-05-27 22:26:10 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 22:26:10 [INFO] metric: mae
2023-05-27 22:26:10 [INFO] shuffle: True
2023-05-27 22:26:10 [INFO] epochs: 2
2023-05-27 22:26:10 [INFO] batch_size: 512
2023-05-27 22:26:10 [INFO] verbose: 2
2023-05-27 22:26:10 [INFO] TensorBoard_log_path: logs
2023-05-27 22:26:10 [INFO] TensorBoard_hist_freq: 1
2023-05-27 22:26:10 [INFO] EarlyStopping_monitor: loss
2023-05-27 22:26:10 [INFO] EarlyStopping_patience: 15
2023-05-27 22:26:10 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 22:26:10 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 22:26:10 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 22:26:10 [INFO] Checkpoint_monitor: loss
2023-05-27 22:26:10 [INFO] save_best_only: True
2023-05-27 22:26:10 [INFO] log_path: logs/training.log
2023-05-27 22:26:10 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 22:26:10 [INFO] Training in progress
2023-05-27 22:26:11 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-05-27 22:26:14 [DEBUG] Epoch 0 - {'loss': '0.113685', 'mae': '0.225630', 'lr': '0.001000'} 
2023-05-27 22:26:17 [DEBUG] Epoch 1 - {'loss': '0.089454', 'mae': '0.204087', 'lr': '0.001000'} 
2023-05-27 22:26:17 [INFO] Training finished, elapsed time: 7.26 seconds
2023-05-27 22:26:17 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-05-27 22:27:22 [INFO] Experiment for subject: WF_T_21 begins at 2023/05/27 22:27:22
2023-05-27 22:27:22 [INFO] Model configurations:
2023-05-27 22:27:22 [INFO] name: WF_T_21
2023-05-27 22:27:22 [INFO] loss_function: mse
2023-05-27 22:27:22 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-05-27 22:27:22 [INFO] metric: mae
2023-05-27 22:27:22 [INFO] shuffle: True
2023-05-27 22:27:22 [INFO] epochs: 50
2023-05-27 22:27:22 [INFO] batch_size: 512
2023-05-27 22:27:22 [INFO] verbose: 2
2023-05-27 22:27:22 [INFO] TensorBoard_log_path: logs
2023-05-27 22:27:22 [INFO] TensorBoard_hist_freq: 1
2023-05-27 22:27:22 [INFO] EarlyStopping_monitor: loss
2023-05-27 22:27:22 [INFO] EarlyStopping_patience: 15
2023-05-27 22:27:22 [INFO] ReduceLROnPlateau_monitor: loss
2023-05-27 22:27:22 [INFO] ReduceLROnPlateau_factor: 0.5
2023-05-27 22:27:22 [INFO] ReduceLROnPlateau_patience: 3
2023-05-27 22:27:22 [INFO] Checkpoint_monitor: loss
2023-05-27 22:27:22 [INFO] save_best_only: True
2023-05-27 22:27:22 [INFO] log_path: logs/training.log
2023-05-27 22:27:22 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-05-27 22:27:22 [INFO] Training in progress
2023-05-27 22:27:25 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.
2023-05-27 22:27:37 [DEBUG] Epoch 0 - {'loss': '0.085886', 'mae': '0.195922', 'lr': '0.001000'} 
2023-05-27 22:27:49 [DEBUG] Epoch 1 - {'loss': '0.070348', 'mae': '0.179264', 'lr': '0.001000'} 
2023-05-27 22:28:02 [DEBUG] Epoch 2 - {'loss': '0.068531', 'mae': '0.176645', 'lr': '0.001000'} 
2023-05-27 22:28:14 [DEBUG] Epoch 3 - {'loss': '0.068167', 'mae': '0.176119', 'lr': '0.001000'} 
2023-05-27 22:28:27 [DEBUG] Epoch 4 - {'loss': '0.068058', 'mae': '0.175964', 'lr': '0.001000'} 
2023-05-27 22:28:39 [DEBUG] Epoch 5 - {'loss': '0.067679', 'mae': '0.175308', 'lr': '0.001000'} 
2023-05-27 22:28:51 [DEBUG] Epoch 6 - {'loss': '0.067740', 'mae': '0.175449', 'lr': '0.001000'} 
2023-05-27 22:29:04 [DEBUG] Epoch 7 - {'loss': '0.067572', 'mae': '0.175177', 'lr': '0.001000'} 
2023-05-27 22:29:16 [DEBUG] Epoch 8 - {'loss': '0.067597', 'mae': '0.175256', 'lr': '0.001000'} 
2023-05-27 22:29:28 [DEBUG] Epoch 9 - {'loss': '0.067504', 'mae': '0.175051', 'lr': '0.001000'} 
2023-05-27 22:29:40 [DEBUG] Epoch 10 - {'loss': '0.067360', 'mae': '0.174835', 'lr': '0.001000'} 
2023-05-27 22:29:53 [DEBUG] Epoch 11 - {'loss': '0.067435', 'mae': '0.174989', 'lr': '0.001000'} 
2023-05-27 22:30:05 [DEBUG] Epoch 12 - {'loss': '0.067182', 'mae': '0.174507', 'lr': '0.001000'} 
2023-05-27 22:30:17 [DEBUG] Epoch 13 - {'loss': '0.067125', 'mae': '0.174434', 'lr': '0.001000'} 
2023-05-27 22:30:30 [DEBUG] Epoch 14 - {'loss': '0.067256', 'mae': '0.174650', 'lr': '0.001000'} 
2023-05-27 22:30:42 [DEBUG] Epoch 15 - {'loss': '0.067002', 'mae': '0.174227', 'lr': '0.001000'} 
2023-05-27 22:30:54 [DEBUG] Epoch 16 - {'loss': '0.067017', 'mae': '0.174247', 'lr': '0.001000'} 
2023-05-27 22:31:07 [DEBUG] Epoch 17 - {'loss': '0.066997', 'mae': '0.174190', 'lr': '0.001000'} 
2023-05-27 22:31:19 [DEBUG] Epoch 18 - {'loss': '0.066983', 'mae': '0.174231', 'lr': '0.001000'} 
2023-05-27 22:31:32 [DEBUG] Epoch 19 - {'loss': '0.066776', 'mae': '0.173850', 'lr': '0.000500'} 
2023-05-27 22:31:44 [DEBUG] Epoch 20 - {'loss': '0.066723', 'mae': '0.173726', 'lr': '0.000500'} 
2023-05-27 22:31:56 [DEBUG] Epoch 21 - {'loss': '0.066738', 'mae': '0.173758', 'lr': '0.000500'} 
2023-05-27 22:32:08 [DEBUG] Epoch 22 - {'loss': '0.066730', 'mae': '0.173762', 'lr': '0.000500'} 
2023-05-27 22:32:21 [DEBUG] Epoch 23 - {'loss': '0.066696', 'mae': '0.173698', 'lr': '0.000250'} 
2023-05-27 22:32:33 [DEBUG] Epoch 24 - {'loss': '0.066658', 'mae': '0.173631', 'lr': '0.000250'} 
2023-05-27 22:32:45 [DEBUG] Epoch 25 - {'loss': '0.066664', 'mae': '0.173632', 'lr': '0.000250'} 
2023-05-27 22:32:58 [DEBUG] Epoch 26 - {'loss': '0.066631', 'mae': '0.173574', 'lr': '0.000250'} 
2023-05-27 22:33:10 [DEBUG] Epoch 27 - {'loss': '0.066625', 'mae': '0.173576', 'lr': '0.000250'} 
2023-05-27 22:33:23 [DEBUG] Epoch 28 - {'loss': '0.066608', 'mae': '0.173533', 'lr': '0.000125'} 
2023-05-27 22:33:35 [DEBUG] Epoch 29 - {'loss': '0.066585', 'mae': '0.173493', 'lr': '0.000125'} 
2023-05-27 22:33:47 [DEBUG] Epoch 30 - {'loss': '0.066595', 'mae': '0.173504', 'lr': '0.000125'} 
2023-05-27 22:34:00 [DEBUG] Epoch 31 - {'loss': '0.066584', 'mae': '0.173506', 'lr': '0.000063'} 
2023-05-27 22:34:12 [DEBUG] Epoch 32 - {'loss': '0.066574', 'mae': '0.173481', 'lr': '0.000063'} 
2023-05-27 22:34:25 [DEBUG] Epoch 33 - {'loss': '0.066558', 'mae': '0.173444', 'lr': '0.000063'} 
2023-05-27 22:34:37 [DEBUG] Epoch 34 - {'loss': '0.066553', 'mae': '0.173453', 'lr': '0.000063'} 
2023-05-27 22:34:49 [DEBUG] Epoch 35 - {'loss': '0.066562', 'mae': '0.173458', 'lr': '0.000063'} 
2023-05-27 22:35:02 [DEBUG] Epoch 36 - {'loss': '0.066562', 'mae': '0.173448', 'lr': '0.000063'} 
2023-05-27 22:35:14 [DEBUG] Epoch 37 - {'loss': '0.066554', 'mae': '0.173448', 'lr': '0.000031'} 
2023-05-27 22:35:26 [DEBUG] Epoch 38 - {'loss': '0.066544', 'mae': '0.173414', 'lr': '0.000031'} 
2023-05-27 22:35:39 [DEBUG] Epoch 39 - {'loss': '0.066572', 'mae': '0.173448', 'lr': '0.000031'} 
2023-05-27 22:35:51 [DEBUG] Epoch 40 - {'loss': '0.066521', 'mae': '0.173368', 'lr': '0.000016'} 
2023-05-27 22:36:03 [DEBUG] Epoch 41 - {'loss': '0.066548', 'mae': '0.173414', 'lr': '0.000016'} 
2023-05-27 22:36:16 [DEBUG] Epoch 42 - {'loss': '0.066517', 'mae': '0.173368', 'lr': '0.000016'} 
2023-05-27 22:36:28 [DEBUG] Epoch 43 - {'loss': '0.066534', 'mae': '0.173375', 'lr': '0.000008'} 
2023-05-27 22:36:40 [DEBUG] Epoch 44 - {'loss': '0.066534', 'mae': '0.173393', 'lr': '0.000008'} 
2023-05-27 22:36:52 [DEBUG] Epoch 45 - {'loss': '0.066538', 'mae': '0.173418', 'lr': '0.000008'} 
2023-05-27 22:37:05 [DEBUG] Epoch 46 - {'loss': '0.066531', 'mae': '0.173374', 'lr': '0.000004'} 
2023-05-27 22:37:17 [DEBUG] Epoch 47 - {'loss': '0.066526', 'mae': '0.173381', 'lr': '0.000004'} 
2023-05-27 22:37:29 [DEBUG] Epoch 48 - {'loss': '0.066542', 'mae': '0.173408', 'lr': '0.000004'} 
2023-05-27 22:37:42 [DEBUG] Epoch 49 - {'loss': '0.066535', 'mae': '0.173405', 'lr': '0.000002'} 
2023-05-27 22:37:42 [INFO] Training finished, elapsed time: 619.58 seconds
2023-05-27 22:37:42 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:19:14 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:19:14
2023-06-03 18:19:14 [INFO] Model configurations:
2023-06-03 18:19:14 [INFO] name: WF_T_21
2023-06-03 18:19:14 [INFO] loss_function: mse
2023-06-03 18:19:14 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:19:14 [INFO] metric: mae
2023-06-03 18:19:14 [INFO] shuffle: True
2023-06-03 18:19:14 [INFO] epochs: 2
2023-06-03 18:19:14 [INFO] batch_size: 512
2023-06-03 18:19:14 [INFO] verbose: 2
2023-06-03 18:19:14 [INFO] TensorBoard_log_path: logs
2023-06-03 18:19:14 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:19:14 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:19:14 [INFO] EarlyStopping_patience: 15
2023-06-03 18:19:14 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:19:14 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:19:14 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:19:14 [INFO] Checkpoint_monitor: loss
2023-06-03 18:19:14 [INFO] save_best_only: True
2023-06-03 18:19:14 [INFO] log_path: logs/training.log
2023-06-03 18:19:14 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:19:14 [INFO] Training in progress
2023-06-03 18:19:17 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-06-03 18:19:29 [DEBUG] Epoch 0 - {'loss': '0.084540', 'mae': '0.194750', 'lr': '0.001000'} 
2023-06-03 18:19:41 [DEBUG] Epoch 1 - {'loss': '0.070613', 'mae': '0.179196', 'lr': '0.001000'} 
2023-06-03 18:19:41 [INFO] Training finished, elapsed time: 27.40 seconds
2023-06-03 18:19:41 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:35:00 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:35:00
2023-06-03 18:35:00 [INFO] Model configurations:
2023-06-03 18:35:00 [INFO] name: WF_T_21
2023-06-03 18:35:00 [INFO] loss_function: mse
2023-06-03 18:35:00 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:35:00 [INFO] metric: mae
2023-06-03 18:35:00 [INFO] shuffle: True
2023-06-03 18:35:00 [INFO] epochs: 2
2023-06-03 18:35:00 [INFO] batch_size: 512
2023-06-03 18:35:00 [INFO] verbose: 2
2023-06-03 18:35:00 [INFO] TensorBoard_log_path: logs
2023-06-03 18:35:00 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:35:00 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:35:00 [INFO] EarlyStopping_patience: 15
2023-06-03 18:35:00 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:35:00 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:35:00 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:35:00 [INFO] Checkpoint_monitor: loss
2023-06-03 18:35:00 [INFO] save_best_only: True
2023-06-03 18:35:00 [INFO] log_path: logs/training.log
2023-06-03 18:35:00 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:35:00 [INFO] Training in progress
2023-06-03 18:35:03 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0114s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.
2023-06-03 18:35:15 [DEBUG] Epoch 0 - {'loss': '0.094573', 'mae': '0.203141', 'lr': '0.001000'} 
2023-06-03 18:35:26 [DEBUG] Epoch 1 - {'loss': '0.071431', 'mae': '0.180420', 'lr': '0.001000'} 
2023-06-03 18:35:26 [INFO] Training finished, elapsed time: 26.16 seconds
2023-06-03 18:35:26 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:39:41 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:39:41
2023-06-03 18:39:41 [INFO] Model configurations:
2023-06-03 18:39:41 [INFO] name: WF_T_21
2023-06-03 18:39:41 [INFO] loss_function: mse
2023-06-03 18:39:41 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:39:41 [INFO] metric: mae
2023-06-03 18:39:41 [INFO] shuffle: True
2023-06-03 18:39:41 [INFO] epochs: 2
2023-06-03 18:39:41 [INFO] batch_size: 512
2023-06-03 18:39:41 [INFO] verbose: 2
2023-06-03 18:39:41 [INFO] TensorBoard_log_path: logs
2023-06-03 18:39:41 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:39:41 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:39:41 [INFO] EarlyStopping_patience: 15
2023-06-03 18:39:41 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:39:41 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:39:41 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:39:41 [INFO] Checkpoint_monitor: loss
2023-06-03 18:39:41 [INFO] save_best_only: True
2023-06-03 18:39:41 [INFO] log_path: logs/training.log
2023-06-03 18:39:41 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:39:41 [INFO] Training in progress
2023-06-03 18:39:44 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.
2023-06-03 18:39:56 [DEBUG] Epoch 0 - {'loss': '0.088477', 'mae': '0.196285', 'lr': '0.001000'} 
2023-06-03 18:40:08 [DEBUG] Epoch 1 - {'loss': '0.070219', 'mae': '0.178669', 'lr': '0.001000'} 
2023-06-03 18:40:08 [INFO] Training finished, elapsed time: 26.68 seconds
2023-06-03 18:40:08 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:42:30 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:42:30
2023-06-03 18:42:30 [INFO] Model configurations:
2023-06-03 18:42:30 [INFO] name: WF_T_21
2023-06-03 18:42:30 [INFO] loss_function: mse
2023-06-03 18:42:30 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:42:30 [INFO] metric: mae
2023-06-03 18:42:30 [INFO] shuffle: True
2023-06-03 18:42:30 [INFO] epochs: 2
2023-06-03 18:42:30 [INFO] batch_size: 512
2023-06-03 18:42:30 [INFO] verbose: 2
2023-06-03 18:42:30 [INFO] TensorBoard_log_path: logs
2023-06-03 18:42:30 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:42:30 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:42:30 [INFO] EarlyStopping_patience: 15
2023-06-03 18:42:30 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:42:30 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:42:30 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:42:30 [INFO] Checkpoint_monitor: loss
2023-06-03 18:42:30 [INFO] save_best_only: True
2023-06-03 18:42:30 [INFO] log_path: logs/training.log
2023-06-03 18:42:30 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:42:31 [INFO] Training in progress
2023-06-03 18:42:33 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.
2023-06-03 18:42:46 [DEBUG] Epoch 0 - {'loss': '0.112417', 'mae': '0.210901', 'lr': '0.001000'} 
2023-06-03 18:42:58 [DEBUG] Epoch 1 - {'loss': '0.077994', 'mae': '0.188261', 'lr': '0.001000'} 
2023-06-03 18:42:58 [INFO] Training finished, elapsed time: 27.39 seconds
2023-06-03 18:42:58 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:43:50 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:43:50
2023-06-03 18:43:50 [INFO] Model configurations:
2023-06-03 18:43:50 [INFO] name: WF_T_21
2023-06-03 18:43:50 [INFO] loss_function: mse
2023-06-03 18:43:50 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:43:50 [INFO] metric: mae
2023-06-03 18:43:50 [INFO] shuffle: True
2023-06-03 18:43:50 [INFO] epochs: 2
2023-06-03 18:43:50 [INFO] batch_size: 512
2023-06-03 18:43:50 [INFO] verbose: 2
2023-06-03 18:43:50 [INFO] TensorBoard_log_path: logs
2023-06-03 18:43:50 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:43:50 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:43:50 [INFO] EarlyStopping_patience: 15
2023-06-03 18:43:50 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:43:50 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:43:50 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:43:50 [INFO] Checkpoint_monitor: loss
2023-06-03 18:43:50 [INFO] save_best_only: True
2023-06-03 18:43:50 [INFO] log_path: logs/training.log
2023-06-03 18:43:50 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:43:50 [INFO] Training in progress
2023-06-03 18:43:52 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0123s vs `on_train_batch_end` time: 0.0209s). Check your callbacks.
2023-06-03 18:44:05 [DEBUG] Epoch 0 - {'loss': '0.098997', 'mae': '0.205999', 'lr': '0.001000'} 
2023-06-03 18:44:17 [DEBUG] Epoch 1 - {'loss': '0.073907', 'mae': '0.183588', 'lr': '0.001000'} 
2023-06-03 18:44:17 [INFO] Training finished, elapsed time: 27.42 seconds
2023-06-03 18:44:17 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:45:05 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:45:05
2023-06-03 18:45:05 [INFO] Model configurations:
2023-06-03 18:45:05 [INFO] name: WF_T_21
2023-06-03 18:45:05 [INFO] loss_function: mse
2023-06-03 18:45:05 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:45:05 [INFO] metric: mae
2023-06-03 18:45:05 [INFO] shuffle: True
2023-06-03 18:45:05 [INFO] epochs: 2
2023-06-03 18:45:05 [INFO] batch_size: 512
2023-06-03 18:45:05 [INFO] verbose: 2
2023-06-03 18:45:05 [INFO] TensorBoard_log_path: logs
2023-06-03 18:45:05 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:45:05 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:45:05 [INFO] EarlyStopping_patience: 15
2023-06-03 18:45:05 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:45:05 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:45:05 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:45:05 [INFO] Checkpoint_monitor: loss
2023-06-03 18:45:05 [INFO] save_best_only: True
2023-06-03 18:45:05 [INFO] log_path: logs/training.log
2023-06-03 18:45:05 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:45:05 [INFO] Training in progress
2023-06-03 18:45:08 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0126s vs `on_train_batch_end` time: 0.0217s). Check your callbacks.
2023-06-03 18:45:21 [DEBUG] Epoch 0 - {'loss': '0.093432', 'mae': '0.202213', 'lr': '0.001000'} 
2023-06-03 18:45:33 [DEBUG] Epoch 1 - {'loss': '0.071144', 'mae': '0.180098', 'lr': '0.001000'} 
2023-06-03 18:45:33 [INFO] Training finished, elapsed time: 27.49 seconds
2023-06-03 18:45:33 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:46:05 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:46:05
2023-06-03 18:46:05 [INFO] Model configurations:
2023-06-03 18:46:05 [INFO] name: WF_T_21
2023-06-03 18:46:05 [INFO] loss_function: mse
2023-06-03 18:46:05 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:46:05 [INFO] metric: mae
2023-06-03 18:46:05 [INFO] shuffle: True
2023-06-03 18:46:05 [INFO] epochs: 2
2023-06-03 18:46:05 [INFO] batch_size: 512
2023-06-03 18:46:05 [INFO] verbose: 2
2023-06-03 18:46:05 [INFO] TensorBoard_log_path: logs
2023-06-03 18:46:05 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:46:05 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:46:05 [INFO] EarlyStopping_patience: 15
2023-06-03 18:46:05 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:46:05 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:46:05 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:46:05 [INFO] Checkpoint_monitor: loss
2023-06-03 18:46:05 [INFO] save_best_only: True
2023-06-03 18:46:05 [INFO] log_path: logs/training.log
2023-06-03 18:46:05 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:46:06 [INFO] Training in progress
2023-06-03 18:46:08 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.
2023-06-03 18:46:21 [DEBUG] Epoch 0 - {'loss': '0.094253', 'mae': '0.198124', 'lr': '0.001000'} 
2023-06-03 18:46:33 [DEBUG] Epoch 1 - {'loss': '0.071034', 'mae': '0.179465', 'lr': '0.001000'} 
2023-06-03 18:46:33 [INFO] Training finished, elapsed time: 27.35 seconds
2023-06-03 18:46:33 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:47:04 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:47:04
2023-06-03 18:47:04 [INFO] Model configurations:
2023-06-03 18:47:04 [INFO] name: WF_T_21
2023-06-03 18:47:04 [INFO] loss_function: mse
2023-06-03 18:47:04 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:47:04 [INFO] metric: mae
2023-06-03 18:47:04 [INFO] shuffle: True
2023-06-03 18:47:04 [INFO] epochs: 2
2023-06-03 18:47:04 [INFO] batch_size: 512
2023-06-03 18:47:04 [INFO] verbose: 2
2023-06-03 18:47:04 [INFO] TensorBoard_log_path: logs
2023-06-03 18:47:04 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:47:04 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:47:04 [INFO] EarlyStopping_patience: 15
2023-06-03 18:47:04 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:47:04 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:47:04 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:47:04 [INFO] Checkpoint_monitor: loss
2023-06-03 18:47:04 [INFO] save_best_only: True
2023-06-03 18:47:04 [INFO] log_path: logs/training.log
2023-06-03 18:47:04 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:47:04 [INFO] Training in progress
2023-06-03 18:47:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0126s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.
2023-06-03 18:47:19 [DEBUG] Epoch 0 - {'loss': '0.090876', 'mae': '0.199966', 'lr': '0.001000'} 
2023-06-03 18:47:32 [DEBUG] Epoch 1 - {'loss': '0.074403', 'mae': '0.184475', 'lr': '0.001000'} 
2023-06-03 18:47:32 [INFO] Training finished, elapsed time: 27.37 seconds
2023-06-03 18:47:32 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:47:54 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:47:54
2023-06-03 18:47:54 [INFO] Model configurations:
2023-06-03 18:47:54 [INFO] name: WF_T_21
2023-06-03 18:47:54 [INFO] loss_function: mse
2023-06-03 18:47:54 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:47:54 [INFO] metric: mae
2023-06-03 18:47:54 [INFO] shuffle: True
2023-06-03 18:47:54 [INFO] epochs: 2
2023-06-03 18:47:54 [INFO] batch_size: 512
2023-06-03 18:47:54 [INFO] verbose: 2
2023-06-03 18:47:54 [INFO] TensorBoard_log_path: logs
2023-06-03 18:47:54 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:47:54 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:47:54 [INFO] EarlyStopping_patience: 15
2023-06-03 18:47:54 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:47:54 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:47:54 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:47:54 [INFO] Checkpoint_monitor: loss
2023-06-03 18:47:54 [INFO] save_best_only: True
2023-06-03 18:47:54 [INFO] log_path: logs/training.log
2023-06-03 18:47:54 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:47:54 [INFO] Training in progress
2023-06-03 18:47:57 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.
2023-06-03 18:48:08 [DEBUG] Epoch 0 - {'loss': '0.090207', 'mae': '0.200955', 'lr': '0.001000'} 
2023-06-03 18:48:20 [DEBUG] Epoch 1 - {'loss': '0.074697', 'mae': '0.184868', 'lr': '0.001000'} 
2023-06-03 18:48:20 [INFO] Training finished, elapsed time: 26.45 seconds
2023-06-03 18:48:20 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:50:19 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:50:19
2023-06-03 18:50:19 [INFO] Model configurations:
2023-06-03 18:50:19 [INFO] name: WF_T_21
2023-06-03 18:50:19 [INFO] loss_function: mse
2023-06-03 18:50:19 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:50:19 [INFO] metric: mae
2023-06-03 18:50:19 [INFO] shuffle: True
2023-06-03 18:50:19 [INFO] epochs: 2
2023-06-03 18:50:19 [INFO] batch_size: 512
2023-06-03 18:50:19 [INFO] verbose: 2
2023-06-03 18:50:19 [INFO] TensorBoard_log_path: logs
2023-06-03 18:50:19 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:50:19 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:50:19 [INFO] EarlyStopping_patience: 15
2023-06-03 18:50:19 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:50:19 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:50:19 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:50:19 [INFO] Checkpoint_monitor: loss
2023-06-03 18:50:19 [INFO] save_best_only: True
2023-06-03 18:50:19 [INFO] log_path: logs/training.log
2023-06-03 18:50:19 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:50:19 [INFO] Training in progress
2023-06-03 18:50:22 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0123s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-06-03 18:50:34 [DEBUG] Epoch 0 - {'loss': '0.087715', 'mae': '0.199328', 'lr': '0.001000'} 
2023-06-03 18:50:47 [DEBUG] Epoch 1 - {'loss': '0.070201', 'mae': '0.179335', 'lr': '0.001000'} 
2023-06-03 18:50:47 [INFO] Training finished, elapsed time: 27.37 seconds
2023-06-03 18:50:47 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:54:55 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:54:55
2023-06-03 18:54:55 [INFO] Model configurations:
2023-06-03 18:54:55 [INFO] name: WF_T_21
2023-06-03 18:54:55 [INFO] loss_function: mse
2023-06-03 18:54:55 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:54:55 [INFO] metric: mae
2023-06-03 18:54:55 [INFO] shuffle: True
2023-06-03 18:54:55 [INFO] epochs: 2
2023-06-03 18:54:55 [INFO] batch_size: 512
2023-06-03 18:54:55 [INFO] verbose: 2
2023-06-03 18:54:55 [INFO] TensorBoard_log_path: logs
2023-06-03 18:54:55 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:54:55 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:54:55 [INFO] EarlyStopping_patience: 15
2023-06-03 18:54:55 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:54:55 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:54:55 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:54:55 [INFO] Checkpoint_monitor: loss
2023-06-03 18:54:55 [INFO] save_best_only: True
2023-06-03 18:54:55 [INFO] log_path: logs/training.log
2023-06-03 18:54:55 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:54:55 [INFO] Training in progress
2023-06-03 18:54:58 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-06-03 18:55:10 [DEBUG] Epoch 0 - {'loss': '0.100519', 'mae': '0.207676', 'lr': '0.001000'} 
2023-06-03 18:55:23 [DEBUG] Epoch 1 - {'loss': '0.073202', 'mae': '0.180904', 'lr': '0.001000'} 
2023-06-03 18:55:23 [INFO] Training finished, elapsed time: 27.44 seconds
2023-06-03 18:55:23 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 18:55:48 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 18:55:48
2023-06-03 18:55:48 [INFO] Model configurations:
2023-06-03 18:55:48 [INFO] name: WF_T_21
2023-06-03 18:55:48 [INFO] loss_function: mse
2023-06-03 18:55:48 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 18:55:48 [INFO] metric: mae
2023-06-03 18:55:48 [INFO] shuffle: True
2023-06-03 18:55:48 [INFO] epochs: 2
2023-06-03 18:55:48 [INFO] batch_size: 512
2023-06-03 18:55:48 [INFO] verbose: 2
2023-06-03 18:55:48 [INFO] TensorBoard_log_path: logs
2023-06-03 18:55:48 [INFO] TensorBoard_hist_freq: 1
2023-06-03 18:55:48 [INFO] EarlyStopping_monitor: loss
2023-06-03 18:55:48 [INFO] EarlyStopping_patience: 15
2023-06-03 18:55:48 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 18:55:48 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 18:55:48 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 18:55:48 [INFO] Checkpoint_monitor: loss
2023-06-03 18:55:48 [INFO] save_best_only: True
2023-06-03 18:55:48 [INFO] log_path: logs/training.log
2023-06-03 18:55:48 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 18:55:48 [INFO] Training in progress
2023-06-03 18:55:51 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.
2023-06-03 18:56:03 [DEBUG] Epoch 0 - {'loss': '0.100227', 'mae': '0.212485', 'lr': '0.001000'} 
2023-06-03 18:56:16 [DEBUG] Epoch 1 - {'loss': '0.071331', 'mae': '0.180964', 'lr': '0.001000'} 
2023-06-03 18:56:16 [INFO] Training finished, elapsed time: 27.58 seconds
2023-06-03 18:56:16 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:02:50 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:02:50
2023-06-03 19:02:50 [INFO] Model configurations:
2023-06-03 19:02:50 [INFO] name: WF_T_21
2023-06-03 19:02:50 [INFO] loss_function: mse
2023-06-03 19:02:50 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:02:50 [INFO] metric: mae
2023-06-03 19:02:50 [INFO] shuffle: True
2023-06-03 19:02:50 [INFO] epochs: 2
2023-06-03 19:02:50 [INFO] batch_size: 512
2023-06-03 19:02:50 [INFO] verbose: 2
2023-06-03 19:02:50 [INFO] TensorBoard_log_path: logs
2023-06-03 19:02:50 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:02:50 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:02:50 [INFO] EarlyStopping_patience: 15
2023-06-03 19:02:50 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:02:50 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:02:50 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:02:50 [INFO] Checkpoint_monitor: loss
2023-06-03 19:02:50 [INFO] save_best_only: True
2023-06-03 19:02:50 [INFO] log_path: logs/training.log
2023-06-03 19:02:50 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:02:50 [INFO] Training in progress
2023-06-03 19:02:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0127s vs `on_train_batch_end` time: 0.0216s). Check your callbacks.
2023-06-03 19:03:05 [DEBUG] Epoch 0 - {'loss': '0.097251', 'mae': '0.207515', 'lr': '0.001000'} 
2023-06-03 19:03:18 [DEBUG] Epoch 1 - {'loss': '0.079401', 'mae': '0.190291', 'lr': '0.001000'} 
2023-06-03 19:03:18 [INFO] Training finished, elapsed time: 27.65 seconds
2023-06-03 19:03:18 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:04:52 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:04:52
2023-06-03 19:04:52 [INFO] Model configurations:
2023-06-03 19:04:52 [INFO] name: WF_T_21
2023-06-03 19:04:52 [INFO] loss_function: mse
2023-06-03 19:04:52 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:04:52 [INFO] metric: mae
2023-06-03 19:04:52 [INFO] shuffle: True
2023-06-03 19:04:52 [INFO] epochs: 2
2023-06-03 19:04:52 [INFO] batch_size: 512
2023-06-03 19:04:52 [INFO] verbose: 2
2023-06-03 19:04:52 [INFO] TensorBoard_log_path: logs
2023-06-03 19:04:52 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:04:52 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:04:52 [INFO] EarlyStopping_patience: 15
2023-06-03 19:04:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:04:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:04:52 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:04:52 [INFO] Checkpoint_monitor: loss
2023-06-03 19:04:52 [INFO] save_best_only: True
2023-06-03 19:04:52 [INFO] log_path: logs/training.log
2023-06-03 19:04:52 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:04:52 [INFO] Training in progress
2023-06-03 19:04:55 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0127s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.
2023-06-03 19:05:07 [DEBUG] Epoch 0 - {'loss': '0.086927', 'mae': '0.197150', 'lr': '0.001000'} 
2023-06-03 19:05:19 [DEBUG] Epoch 1 - {'loss': '0.070754', 'mae': '0.179445', 'lr': '0.001000'} 
2023-06-03 19:05:19 [INFO] Training finished, elapsed time: 26.98 seconds
2023-06-03 19:05:19 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:06:33 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:06:33
2023-06-03 19:06:33 [INFO] Model configurations:
2023-06-03 19:06:33 [INFO] name: WF_T_21
2023-06-03 19:06:33 [INFO] loss_function: mse
2023-06-03 19:06:33 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:06:33 [INFO] metric: mae
2023-06-03 19:06:33 [INFO] shuffle: True
2023-06-03 19:06:33 [INFO] epochs: 2
2023-06-03 19:06:33 [INFO] batch_size: 512
2023-06-03 19:06:33 [INFO] verbose: 2
2023-06-03 19:06:33 [INFO] TensorBoard_log_path: logs
2023-06-03 19:06:33 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:06:33 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:06:33 [INFO] EarlyStopping_patience: 15
2023-06-03 19:06:33 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:06:33 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:06:33 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:06:33 [INFO] Checkpoint_monitor: loss
2023-06-03 19:06:33 [INFO] save_best_only: True
2023-06-03 19:06:33 [INFO] log_path: logs/training.log
2023-06-03 19:06:33 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:06:33 [INFO] Training in progress
2023-06-03 19:06:36 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0219s). Check your callbacks.
2023-06-03 19:06:48 [DEBUG] Epoch 0 - {'loss': '0.085531', 'mae': '0.196354', 'lr': '0.001000'} 
2023-06-03 19:07:00 [DEBUG] Epoch 1 - {'loss': '0.069675', 'mae': '0.178567', 'lr': '0.001000'} 
2023-06-03 19:07:00 [INFO] Training finished, elapsed time: 27.35 seconds
2023-06-03 19:07:00 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:09:17 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:09:17
2023-06-03 19:09:17 [INFO] Model configurations:
2023-06-03 19:09:17 [INFO] name: WF_T_21
2023-06-03 19:09:17 [INFO] loss_function: mse
2023-06-03 19:09:17 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:09:17 [INFO] metric: mae
2023-06-03 19:09:17 [INFO] shuffle: True
2023-06-03 19:09:17 [INFO] epochs: 2
2023-06-03 19:09:17 [INFO] batch_size: 512
2023-06-03 19:09:17 [INFO] verbose: 2
2023-06-03 19:09:17 [INFO] TensorBoard_log_path: logs
2023-06-03 19:09:17 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:09:17 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:09:17 [INFO] EarlyStopping_patience: 15
2023-06-03 19:09:17 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:09:17 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:09:17 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:09:17 [INFO] Checkpoint_monitor: loss
2023-06-03 19:09:17 [INFO] save_best_only: True
2023-06-03 19:09:17 [INFO] log_path: logs/training.log
2023-06-03 19:09:17 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:09:17 [INFO] Training in progress
2023-06-03 19:09:20 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.
2023-06-03 19:09:32 [DEBUG] Epoch 0 - {'loss': '0.084402', 'mae': '0.196404', 'lr': '0.001000'} 
2023-06-03 19:09:45 [DEBUG] Epoch 1 - {'loss': '0.069817', 'mae': '0.178207', 'lr': '0.001000'} 
2023-06-03 19:09:45 [INFO] Training finished, elapsed time: 27.49 seconds
2023-06-03 19:09:45 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:14:59 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:14:59
2023-06-03 19:14:59 [INFO] Model configurations:
2023-06-03 19:14:59 [INFO] name: WF_T_21
2023-06-03 19:14:59 [INFO] loss_function: mse
2023-06-03 19:14:59 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:14:59 [INFO] metric: mae
2023-06-03 19:14:59 [INFO] shuffle: True
2023-06-03 19:14:59 [INFO] epochs: 2
2023-06-03 19:14:59 [INFO] batch_size: 512
2023-06-03 19:14:59 [INFO] verbose: 2
2023-06-03 19:14:59 [INFO] TensorBoard_log_path: logs
2023-06-03 19:14:59 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:14:59 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:14:59 [INFO] EarlyStopping_patience: 15
2023-06-03 19:14:59 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:14:59 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:14:59 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:14:59 [INFO] Checkpoint_monitor: loss
2023-06-03 19:14:59 [INFO] save_best_only: True
2023-06-03 19:14:59 [INFO] log_path: logs/training.log
2023-06-03 19:14:59 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:14:59 [INFO] Training in progress
2023-06-03 19:15:02 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0123s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.
2023-06-03 19:15:14 [DEBUG] Epoch 0 - {'loss': '0.084836', 'mae': '0.195997', 'lr': '0.001000'} 
2023-06-03 19:15:26 [DEBUG] Epoch 1 - {'loss': '0.070461', 'mae': '0.179670', 'lr': '0.001000'} 
2023-06-03 19:15:26 [INFO] Training finished, elapsed time: 27.36 seconds
2023-06-03 19:15:26 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:16:00 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:16:00
2023-06-03 19:16:00 [INFO] Model configurations:
2023-06-03 19:16:00 [INFO] name: WF_T_21
2023-06-03 19:16:00 [INFO] loss_function: mse
2023-06-03 19:16:00 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:16:00 [INFO] metric: mae
2023-06-03 19:16:00 [INFO] shuffle: True
2023-06-03 19:16:00 [INFO] epochs: 2
2023-06-03 19:16:00 [INFO] batch_size: 512
2023-06-03 19:16:00 [INFO] verbose: 2
2023-06-03 19:16:00 [INFO] TensorBoard_log_path: logs
2023-06-03 19:16:00 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:16:00 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:16:00 [INFO] EarlyStopping_patience: 15
2023-06-03 19:16:00 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:16:00 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:16:00 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:16:00 [INFO] Checkpoint_monitor: loss
2023-06-03 19:16:00 [INFO] save_best_only: True
2023-06-03 19:16:00 [INFO] log_path: logs/training.log
2023-06-03 19:16:00 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:16:00 [INFO] Training in progress
2023-06-03 19:16:03 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0126s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.
2023-06-03 19:16:15 [DEBUG] Epoch 0 - {'loss': '0.098240', 'mae': '0.205693', 'lr': '0.001000'} 
2023-06-03 19:16:27 [DEBUG] Epoch 1 - {'loss': '0.072745', 'mae': '0.181725', 'lr': '0.001000'} 
2023-06-03 19:16:27 [INFO] Training finished, elapsed time: 27.30 seconds
2023-06-03 19:16:27 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:17:44 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:17:44
2023-06-03 19:17:44 [INFO] Model configurations:
2023-06-03 19:17:44 [INFO] name: WF_T_21
2023-06-03 19:17:44 [INFO] loss_function: mse
2023-06-03 19:17:44 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:17:44 [INFO] metric: mae
2023-06-03 19:17:44 [INFO] shuffle: True
2023-06-03 19:17:44 [INFO] epochs: 2
2023-06-03 19:17:44 [INFO] batch_size: 512
2023-06-03 19:17:44 [INFO] verbose: 2
2023-06-03 19:17:44 [INFO] TensorBoard_log_path: logs
2023-06-03 19:17:44 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:17:44 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:17:44 [INFO] EarlyStopping_patience: 15
2023-06-03 19:17:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:17:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:17:44 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:17:44 [INFO] Checkpoint_monitor: loss
2023-06-03 19:17:44 [INFO] save_best_only: True
2023-06-03 19:17:44 [INFO] log_path: logs/training.log
2023-06-03 19:17:44 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:17:44 [INFO] Training in progress
2023-06-03 19:17:47 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.
2023-06-03 19:17:59 [DEBUG] Epoch 0 - {'loss': '0.099392', 'mae': '0.204298', 'lr': '0.001000'} 
2023-06-03 19:18:11 [DEBUG] Epoch 1 - {'loss': '0.071571', 'mae': '0.180543', 'lr': '0.001000'} 
2023-06-03 19:18:11 [INFO] Training finished, elapsed time: 27.13 seconds
2023-06-03 19:18:11 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:19:45 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:19:45
2023-06-03 19:19:45 [INFO] Model configurations:
2023-06-03 19:19:45 [INFO] name: WF_T_21
2023-06-03 19:19:45 [INFO] loss_function: mse
2023-06-03 19:19:45 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:19:45 [INFO] metric: mae
2023-06-03 19:19:45 [INFO] shuffle: True
2023-06-03 19:19:45 [INFO] epochs: 2
2023-06-03 19:19:45 [INFO] batch_size: 512
2023-06-03 19:19:45 [INFO] verbose: 2
2023-06-03 19:19:45 [INFO] TensorBoard_log_path: logs
2023-06-03 19:19:45 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:19:45 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:19:45 [INFO] EarlyStopping_patience: 15
2023-06-03 19:19:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:19:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:19:45 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:19:45 [INFO] Checkpoint_monitor: loss
2023-06-03 19:19:45 [INFO] save_best_only: True
2023-06-03 19:19:45 [INFO] log_path: logs/training.log
2023-06-03 19:19:45 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:19:45 [INFO] Training in progress
2023-06-03 19:19:48 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0225s). Check your callbacks.
2023-06-03 19:20:01 [DEBUG] Epoch 0 - {'loss': '0.082750', 'mae': '0.193648', 'lr': '0.001000'} 
2023-06-03 19:20:13 [DEBUG] Epoch 1 - {'loss': '0.070613', 'mae': '0.179262', 'lr': '0.001000'} 
2023-06-03 19:20:13 [INFO] Training finished, elapsed time: 27.40 seconds
2023-06-03 19:20:13 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:29:13 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:29:13
2023-06-03 19:29:13 [INFO] Model configurations:
2023-06-03 19:29:13 [INFO] name: WF_T_21
2023-06-03 19:29:13 [INFO] loss_function: mse
2023-06-03 19:29:13 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:29:13 [INFO] metric: mae
2023-06-03 19:29:13 [INFO] shuffle: True
2023-06-03 19:29:13 [INFO] epochs: 20
2023-06-03 19:29:13 [INFO] batch_size: 512
2023-06-03 19:29:13 [INFO] verbose: 2
2023-06-03 19:29:13 [INFO] TensorBoard_log_path: logs
2023-06-03 19:29:13 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:29:13 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:29:13 [INFO] EarlyStopping_patience: 15
2023-06-03 19:29:13 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:29:13 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:29:13 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:29:13 [INFO] Checkpoint_monitor: loss
2023-06-03 19:29:13 [INFO] save_best_only: True
2023-06-03 19:29:13 [INFO] log_path: logs/training.log
2023-06-03 19:29:13 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:29:13 [INFO] Training in progress
2023-06-03 19:29:16 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0217s). Check your callbacks.
2023-06-03 19:29:28 [DEBUG] Epoch 0 - {'loss': '0.090995', 'mae': '0.199656', 'lr': '0.001000'} 
2023-06-03 19:29:41 [DEBUG] Epoch 1 - {'loss': '0.070783', 'mae': '0.179385', 'lr': '0.001000'} 
2023-06-03 19:29:53 [DEBUG] Epoch 2 - {'loss': '0.069484', 'mae': '0.177448', 'lr': '0.001000'} 
2023-06-03 19:30:06 [DEBUG] Epoch 3 - {'loss': '0.069243', 'mae': '0.177131', 'lr': '0.001000'} 
2023-06-03 19:30:18 [DEBUG] Epoch 4 - {'loss': '0.068710', 'mae': '0.176279', 'lr': '0.001000'} 
2023-06-03 19:30:30 [DEBUG] Epoch 5 - {'loss': '0.068525', 'mae': '0.176039', 'lr': '0.001000'} 
2023-06-03 19:30:43 [DEBUG] Epoch 6 - {'loss': '0.068373', 'mae': '0.175777', 'lr': '0.001000'} 
2023-06-03 19:30:55 [DEBUG] Epoch 7 - {'loss': '0.068162', 'mae': '0.175486', 'lr': '0.001000'} 
2023-06-03 19:31:07 [DEBUG] Epoch 8 - {'loss': '0.068357', 'mae': '0.175813', 'lr': '0.001000'} 
2023-06-03 19:31:20 [DEBUG] Epoch 9 - {'loss': '0.068086', 'mae': '0.175379', 'lr': '0.001000'} 
2023-06-03 19:31:32 [DEBUG] Epoch 10 - {'loss': '0.068028', 'mae': '0.175254', 'lr': '0.001000'} 
2023-06-03 19:31:44 [DEBUG] Epoch 11 - {'loss': '0.067846', 'mae': '0.174990', 'lr': '0.001000'} 
2023-06-03 19:31:57 [DEBUG] Epoch 12 - {'loss': '0.067901', 'mae': '0.175117', 'lr': '0.001000'} 
2023-06-03 19:32:09 [DEBUG] Epoch 13 - {'loss': '0.067711', 'mae': '0.174769', 'lr': '0.001000'} 
2023-06-03 19:32:21 [DEBUG] Epoch 14 - {'loss': '0.067730', 'mae': '0.174827', 'lr': '0.001000'} 
2023-06-03 19:32:34 [DEBUG] Epoch 15 - {'loss': '0.067697', 'mae': '0.174795', 'lr': '0.001000'} 
2023-06-03 19:32:46 [DEBUG] Epoch 16 - {'loss': '0.067753', 'mae': '0.174894', 'lr': '0.001000'} 
2023-06-03 19:32:58 [DEBUG] Epoch 17 - {'loss': '0.067462', 'mae': '0.174347', 'lr': '0.000500'} 
2023-06-03 19:33:11 [DEBUG] Epoch 18 - {'loss': '0.067419', 'mae': '0.174280', 'lr': '0.000500'} 
2023-06-03 19:33:23 [DEBUG] Epoch 19 - {'loss': '0.067458', 'mae': '0.174338', 'lr': '0.000500'} 
2023-06-03 19:33:23 [INFO] Training finished, elapsed time: 249.74 seconds
2023-06-03 19:33:23 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:38:01 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:38:01
2023-06-03 19:38:01 [INFO] Model configurations:
2023-06-03 19:38:01 [INFO] name: WF_T_21
2023-06-03 19:38:01 [INFO] loss_function: mse
2023-06-03 19:38:01 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:38:01 [INFO] metric: mae
2023-06-03 19:38:01 [INFO] shuffle: True
2023-06-03 19:38:01 [INFO] epochs: 20
2023-06-03 19:38:01 [INFO] batch_size: 512
2023-06-03 19:38:01 [INFO] verbose: 2
2023-06-03 19:38:01 [INFO] TensorBoard_log_path: logs
2023-06-03 19:38:01 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:38:01 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:38:01 [INFO] EarlyStopping_patience: 15
2023-06-03 19:38:01 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:38:01 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:38:01 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:38:01 [INFO] Checkpoint_monitor: loss
2023-06-03 19:38:01 [INFO] save_best_only: True
2023-06-03 19:38:01 [INFO] log_path: logs/training.log
2023-06-03 19:38:01 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:38:01 [INFO] Training in progress
2023-06-03 19:38:04 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0123s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.
2023-06-03 19:38:16 [DEBUG] Epoch 0 - {'loss': '0.090013', 'mae': '0.197268', 'lr': '0.001000'} 
2023-06-03 19:38:28 [DEBUG] Epoch 1 - {'loss': '0.069695', 'mae': '0.178014', 'lr': '0.001000'} 
2023-06-03 19:38:41 [DEBUG] Epoch 2 - {'loss': '0.068733', 'mae': '0.176417', 'lr': '0.001000'} 
2023-06-03 19:38:53 [DEBUG] Epoch 3 - {'loss': '0.063389', 'mae': '0.166931', 'lr': '0.001000'} 
2023-06-03 19:39:05 [DEBUG] Epoch 4 - {'loss': '0.061369', 'mae': '0.162974', 'lr': '0.001000'} 
2023-06-03 19:39:18 [DEBUG] Epoch 5 - {'loss': '0.061344', 'mae': '0.162934', 'lr': '0.001000'} 
2023-06-03 19:39:30 [DEBUG] Epoch 6 - {'loss': '0.061030', 'mae': '0.162291', 'lr': '0.001000'} 
2023-06-03 19:39:42 [DEBUG] Epoch 7 - {'loss': '0.060920', 'mae': '0.162084', 'lr': '0.001000'} 
2023-06-03 19:39:54 [DEBUG] Epoch 8 - {'loss': '0.061000', 'mae': '0.162273', 'lr': '0.001000'} 
2023-06-03 19:40:07 [DEBUG] Epoch 9 - {'loss': '0.060654', 'mae': '0.161519', 'lr': '0.001000'} 
2023-06-03 19:40:19 [DEBUG] Epoch 10 - {'loss': '0.060692', 'mae': '0.161635', 'lr': '0.001000'} 
2023-06-03 19:40:31 [DEBUG] Epoch 11 - {'loss': '0.060713', 'mae': '0.161722', 'lr': '0.001000'} 
2023-06-03 19:40:43 [DEBUG] Epoch 12 - {'loss': '0.060595', 'mae': '0.161486', 'lr': '0.001000'} 
2023-06-03 19:40:56 [DEBUG] Epoch 13 - {'loss': '0.060289', 'mae': '0.160833', 'lr': '0.000500'} 
2023-06-03 19:41:08 [DEBUG] Epoch 14 - {'loss': '0.060237', 'mae': '0.160723', 'lr': '0.000500'} 
2023-06-03 19:41:20 [DEBUG] Epoch 15 - {'loss': '0.060189', 'mae': '0.160627', 'lr': '0.000500'} 
2023-06-03 19:41:33 [DEBUG] Epoch 16 - {'loss': '0.060232', 'mae': '0.160737', 'lr': '0.000500'} 
2023-06-03 19:41:45 [DEBUG] Epoch 17 - {'loss': '0.060035', 'mae': '0.160294', 'lr': '0.000250'} 
2023-06-03 19:41:57 [DEBUG] Epoch 18 - {'loss': '0.060073', 'mae': '0.160375', 'lr': '0.000250'} 
2023-06-03 19:42:10 [DEBUG] Epoch 19 - {'loss': '0.060011', 'mae': '0.160222', 'lr': '0.000250'} 
2023-06-03 19:42:10 [INFO] Training finished, elapsed time: 248.56 seconds
2023-06-03 19:42:10 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:48:05 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:48:05
2023-06-03 19:48:05 [INFO] Model configurations:
2023-06-03 19:48:05 [INFO] name: WF_T_21
2023-06-03 19:48:05 [INFO] loss_function: mse
2023-06-03 19:48:05 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:48:05 [INFO] metric: mae
2023-06-03 19:48:05 [INFO] shuffle: True
2023-06-03 19:48:05 [INFO] epochs: 20
2023-06-03 19:48:05 [INFO] batch_size: 512
2023-06-03 19:48:05 [INFO] verbose: 2
2023-06-03 19:48:05 [INFO] TensorBoard_log_path: logs
2023-06-03 19:48:05 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:48:05 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:48:05 [INFO] EarlyStopping_patience: 15
2023-06-03 19:48:05 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:48:05 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:48:05 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:48:05 [INFO] Checkpoint_monitor: loss
2023-06-03 19:48:05 [INFO] save_best_only: True
2023-06-03 19:48:05 [INFO] log_path: logs/training.log
2023-06-03 19:48:05 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:48:05 [INFO] Training in progress
2023-06-03 19:48:06 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.
2023-06-03 19:48:10 [DEBUG] Epoch 0 - {'loss': '0.119110', 'mae': '0.231987', 'lr': '0.001000'} 
2023-06-03 19:48:13 [DEBUG] Epoch 1 - {'loss': '0.085902', 'mae': '0.200323', 'lr': '0.001000'} 
2023-06-03 19:48:16 [DEBUG] Epoch 2 - {'loss': '0.080416', 'mae': '0.191891', 'lr': '0.001000'} 
2023-06-03 19:48:19 [DEBUG] Epoch 3 - {'loss': '0.076948', 'mae': '0.188725', 'lr': '0.001000'} 
2023-06-03 19:48:22 [DEBUG] Epoch 4 - {'loss': '0.070509', 'mae': '0.179471', 'lr': '0.001000'} 
2023-06-03 19:48:26 [DEBUG] Epoch 5 - {'loss': '0.069107', 'mae': '0.177124', 'lr': '0.001000'} 
2023-06-03 19:48:29 [DEBUG] Epoch 6 - {'loss': '0.069244', 'mae': '0.177580', 'lr': '0.001000'} 
2023-06-03 19:48:32 [DEBUG] Epoch 7 - {'loss': '0.068282', 'mae': '0.175790', 'lr': '0.001000'} 
2023-06-03 19:48:35 [DEBUG] Epoch 8 - {'loss': '0.069027', 'mae': '0.177154', 'lr': '0.001000'} 
2023-06-03 19:48:38 [DEBUG] Epoch 9 - {'loss': '0.068112', 'mae': '0.175530', 'lr': '0.001000'} 
2023-06-03 19:48:42 [DEBUG] Epoch 10 - {'loss': '0.067953', 'mae': '0.175317', 'lr': '0.001000'} 
2023-06-03 19:48:45 [DEBUG] Epoch 11 - {'loss': '0.067866', 'mae': '0.175147', 'lr': '0.001000'} 
2023-06-03 19:48:48 [DEBUG] Epoch 12 - {'loss': '0.067896', 'mae': '0.175200', 'lr': '0.001000'} 
2023-06-03 19:48:51 [DEBUG] Epoch 13 - {'loss': '0.067750', 'mae': '0.174952', 'lr': '0.001000'} 
2023-06-03 19:48:54 [DEBUG] Epoch 14 - {'loss': '0.067686', 'mae': '0.174822', 'lr': '0.001000'} 
2023-06-03 19:48:58 [DEBUG] Epoch 15 - {'loss': '0.067652', 'mae': '0.174762', 'lr': '0.001000'} 
2023-06-03 19:49:01 [DEBUG] Epoch 16 - {'loss': '0.067548', 'mae': '0.174602', 'lr': '0.001000'} 
2023-06-03 19:49:04 [DEBUG] Epoch 17 - {'loss': '0.067565', 'mae': '0.174608', 'lr': '0.001000'} 
2023-06-03 19:49:07 [DEBUG] Epoch 18 - {'loss': '0.067494', 'mae': '0.174460', 'lr': '0.001000'} 
2023-06-03 19:49:10 [DEBUG] Epoch 19 - {'loss': '0.067523', 'mae': '0.174516', 'lr': '0.001000'} 
2023-06-03 19:49:10 [INFO] Training finished, elapsed time: 64.99 seconds
2023-06-03 19:49:10 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:52:34 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:52:34
2023-06-03 19:52:34 [INFO] Model configurations:
2023-06-03 19:52:34 [INFO] name: WF_T_21
2023-06-03 19:52:34 [INFO] loss_function: mse
2023-06-03 19:52:34 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:52:34 [INFO] metric: mae
2023-06-03 19:52:34 [INFO] shuffle: True
2023-06-03 19:52:34 [INFO] epochs: 20
2023-06-03 19:52:34 [INFO] batch_size: 512
2023-06-03 19:52:34 [INFO] verbose: 2
2023-06-03 19:52:34 [INFO] TensorBoard_log_path: logs
2023-06-03 19:52:34 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:52:34 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:52:34 [INFO] EarlyStopping_patience: 15
2023-06-03 19:52:34 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:52:34 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:52:34 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:52:34 [INFO] Checkpoint_monitor: loss
2023-06-03 19:52:34 [INFO] save_best_only: True
2023-06-03 19:52:34 [INFO] log_path: logs/training.log
2023-06-03 19:52:34 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:52:34 [INFO] Training in progress
2023-06-03 19:52:34 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0051s). Check your callbacks.
2023-06-03 19:52:37 [DEBUG] Epoch 0 - {'loss': '0.124138', 'mae': '0.237266', 'lr': '0.001000'} 
2023-06-03 19:52:40 [DEBUG] Epoch 1 - {'loss': '0.086026', 'mae': '0.201297', 'lr': '0.001000'} 
2023-06-03 19:52:42 [DEBUG] Epoch 2 - {'loss': '0.081721', 'mae': '0.194260', 'lr': '0.001000'} 
2023-06-03 19:52:45 [DEBUG] Epoch 3 - {'loss': '1.044039', 'mae': '0.552332', 'lr': '0.001000'} 
2023-06-03 19:52:47 [DEBUG] Epoch 4 - {'loss': '6.452134', 'mae': '2.403631', 'lr': '0.001000'} 
2023-06-03 19:52:50 [DEBUG] Epoch 5 - {'loss': '6.451934', 'mae': '2.403588', 'lr': '0.001000'} 
2023-06-03 19:52:53 [DEBUG] Epoch 6 - {'loss': '6.451928', 'mae': '2.403593', 'lr': '0.000500'} 
2023-06-03 19:52:55 [DEBUG] Epoch 7 - {'loss': '6.452118', 'mae': '2.403625', 'lr': '0.000500'} 
2023-06-03 19:52:58 [DEBUG] Epoch 8 - {'loss': '6.451939', 'mae': '2.403589', 'lr': '0.000500'} 
2023-06-03 19:53:00 [DEBUG] Epoch 9 - {'loss': '6.452032', 'mae': '2.403608', 'lr': '0.000250'} 
2023-06-03 19:53:03 [DEBUG] Epoch 10 - {'loss': '6.452003', 'mae': '2.403591', 'lr': '0.000250'} 
2023-06-03 19:53:05 [DEBUG] Epoch 11 - {'loss': '6.452103', 'mae': '2.403623', 'lr': '0.000250'} 
2023-06-03 19:53:08 [DEBUG] Epoch 12 - {'loss': '6.451970', 'mae': '2.403603', 'lr': '0.000125'} 
2023-06-03 19:53:11 [DEBUG] Epoch 13 - {'loss': '6.451961', 'mae': '2.403582', 'lr': '0.000125'} 
2023-06-03 19:53:13 [DEBUG] Epoch 14 - {'loss': '6.452138', 'mae': '2.403623', 'lr': '0.000125'} 
2023-06-03 19:53:16 [DEBUG] Epoch 15 - {'loss': '6.452196', 'mae': '2.403637', 'lr': '0.000063'} 
2023-06-03 19:53:18 [DEBUG] Epoch 16 - {'loss': '6.451942', 'mae': '2.403596', 'lr': '0.000063'} 
2023-06-03 19:53:21 [DEBUG] Epoch 17 - {'loss': '6.451885', 'mae': '2.403578', 'lr': '0.000063'} 
2023-06-03 19:53:21 [INFO] Training finished, elapsed time: 47.08 seconds
2023-06-03 19:53:21 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:56:39 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:56:39
2023-06-03 19:56:39 [INFO] Model configurations:
2023-06-03 19:56:39 [INFO] name: WF_T_21
2023-06-03 19:56:39 [INFO] loss_function: mse
2023-06-03 19:56:39 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:56:39 [INFO] metric: mae
2023-06-03 19:56:39 [INFO] shuffle: True
2023-06-03 19:56:39 [INFO] epochs: 20
2023-06-03 19:56:39 [INFO] batch_size: 512
2023-06-03 19:56:39 [INFO] verbose: 2
2023-06-03 19:56:39 [INFO] TensorBoard_log_path: logs
2023-06-03 19:56:39 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:56:39 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:56:39 [INFO] EarlyStopping_patience: 15
2023-06-03 19:56:39 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:56:39 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:56:39 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:56:39 [INFO] Checkpoint_monitor: loss
2023-06-03 19:56:39 [INFO] save_best_only: True
2023-06-03 19:56:39 [INFO] log_path: logs/training.log
2023-06-03 19:56:39 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:56:39 [INFO] Training in progress
2023-06-03 19:56:40 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.
2023-06-03 19:56:43 [DEBUG] Epoch 0 - {'loss': '0.109681', 'mae': '0.225095', 'lr': '0.001000'} 
2023-06-03 19:56:45 [DEBUG] Epoch 1 - {'loss': '0.078180', 'mae': '0.190933', 'lr': '0.001000'} 
2023-06-03 19:56:48 [DEBUG] Epoch 2 - {'loss': '0.075555', 'mae': '0.185708', 'lr': '0.001000'} 
2023-06-03 19:56:50 [DEBUG] Epoch 3 - {'loss': '0.074657', 'mae': '0.184254', 'lr': '0.001000'} 
2023-06-03 19:56:52 [DEBUG] Epoch 4 - {'loss': '0.073627', 'mae': '0.182755', 'lr': '0.001000'} 
2023-06-03 19:56:55 [DEBUG] Epoch 5 - {'loss': '0.072685', 'mae': '0.181464', 'lr': '0.001000'} 
2023-06-03 19:56:57 [DEBUG] Epoch 6 - {'loss': '0.072055', 'mae': '0.180578', 'lr': '0.001000'} 
2023-06-03 19:57:00 [DEBUG] Epoch 7 - {'loss': '0.071585', 'mae': '0.179875', 'lr': '0.001000'} 
2023-06-03 19:57:02 [DEBUG] Epoch 8 - {'loss': '0.071224', 'mae': '0.179323', 'lr': '0.001000'} 
2023-06-03 19:57:05 [DEBUG] Epoch 9 - {'loss': '0.070829', 'mae': '0.178687', 'lr': '0.001000'} 
2023-06-03 19:57:07 [DEBUG] Epoch 10 - {'loss': '0.070335', 'mae': '0.177941', 'lr': '0.001000'} 
2023-06-03 19:57:10 [DEBUG] Epoch 11 - {'loss': '0.069822', 'mae': '0.177246', 'lr': '0.001000'} 
2023-06-03 19:57:12 [DEBUG] Epoch 12 - {'loss': '0.069431', 'mae': '0.176763', 'lr': '0.001000'} 
2023-06-03 19:57:14 [DEBUG] Epoch 13 - {'loss': '0.069068', 'mae': '0.176328', 'lr': '0.001000'} 
2023-06-03 19:57:17 [DEBUG] Epoch 14 - {'loss': '0.068832', 'mae': '0.176057', 'lr': '0.001000'} 
2023-06-03 19:57:19 [DEBUG] Epoch 15 - {'loss': '0.068645', 'mae': '0.175834', 'lr': '0.001000'} 
2023-06-03 19:57:22 [DEBUG] Epoch 16 - {'loss': '0.068496', 'mae': '0.175663', 'lr': '0.001000'} 
2023-06-03 19:57:24 [DEBUG] Epoch 17 - {'loss': '0.068317', 'mae': '0.175415', 'lr': '0.001000'} 
2023-06-03 19:57:27 [DEBUG] Epoch 18 - {'loss': '0.068221', 'mae': '0.175291', 'lr': '0.001000'} 
2023-06-03 19:57:29 [DEBUG] Epoch 19 - {'loss': '0.068100', 'mae': '0.175118', 'lr': '0.001000'} 
2023-06-03 19:57:29 [INFO] Training finished, elapsed time: 49.60 seconds
2023-06-03 19:57:29 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 19:59:55 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 19:59:55
2023-06-03 19:59:55 [INFO] Model configurations:
2023-06-03 19:59:55 [INFO] name: WF_T_21
2023-06-03 19:59:55 [INFO] loss_function: mse
2023-06-03 19:59:55 [INFO] optimizer: {'name': 'adam', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 19:59:55 [INFO] metric: mae
2023-06-03 19:59:55 [INFO] shuffle: True
2023-06-03 19:59:55 [INFO] epochs: 20
2023-06-03 19:59:55 [INFO] batch_size: 512
2023-06-03 19:59:55 [INFO] verbose: 2
2023-06-03 19:59:55 [INFO] TensorBoard_log_path: logs
2023-06-03 19:59:55 [INFO] TensorBoard_hist_freq: 1
2023-06-03 19:59:55 [INFO] EarlyStopping_monitor: loss
2023-06-03 19:59:55 [INFO] EarlyStopping_patience: 15
2023-06-03 19:59:55 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 19:59:55 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 19:59:55 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 19:59:55 [INFO] Checkpoint_monitor: loss
2023-06-03 19:59:55 [INFO] save_best_only: True
2023-06-03 19:59:55 [INFO] log_path: logs/training.log
2023-06-03 19:59:55 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 19:59:55 [INFO] Training in progress
2023-06-03 19:59:56 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.
2023-06-03 19:59:59 [DEBUG] Epoch 0 - {'loss': '0.092048', 'mae': '0.198504', 'lr': '0.001000'} 
2023-06-03 20:00:01 [DEBUG] Epoch 1 - {'loss': '0.070433', 'mae': '0.176124', 'lr': '0.001000'} 
2023-06-03 20:00:04 [DEBUG] Epoch 2 - {'loss': '0.068981', 'mae': '0.174225', 'lr': '0.001000'} 
2023-06-03 20:00:06 [DEBUG] Epoch 3 - {'loss': '0.068293', 'mae': '0.173444', 'lr': '0.001000'} 
2023-06-03 20:00:08 [DEBUG] Epoch 4 - {'loss': '0.067607', 'mae': '0.172567', 'lr': '0.001000'} 
2023-06-03 20:00:11 [DEBUG] Epoch 5 - {'loss': '0.066918', 'mae': '0.171671', 'lr': '0.001000'} 
2023-06-03 20:00:13 [DEBUG] Epoch 6 - {'loss': '0.066149', 'mae': '0.170404', 'lr': '0.001000'} 
2023-06-03 20:00:16 [DEBUG] Epoch 7 - {'loss': '0.065717', 'mae': '0.169743', 'lr': '0.001000'} 
2023-06-03 20:00:18 [DEBUG] Epoch 8 - {'loss': '0.065349', 'mae': '0.169088', 'lr': '0.001000'} 
2023-06-03 20:00:21 [DEBUG] Epoch 9 - {'loss': '0.065167', 'mae': '0.168799', 'lr': '0.001000'} 
2023-06-03 20:00:23 [DEBUG] Epoch 10 - {'loss': '0.064987', 'mae': '0.168473', 'lr': '0.001000'} 
2023-06-03 20:00:26 [DEBUG] Epoch 11 - {'loss': '0.064833', 'mae': '0.168224', 'lr': '0.001000'} 
2023-06-03 20:00:28 [DEBUG] Epoch 12 - {'loss': '0.064750', 'mae': '0.168093', 'lr': '0.001000'} 
2023-06-03 20:00:31 [DEBUG] Epoch 13 - {'loss': '0.064641', 'mae': '0.167913', 'lr': '0.001000'} 
2023-06-03 20:00:33 [DEBUG] Epoch 14 - {'loss': '0.064625', 'mae': '0.167903', 'lr': '0.001000'} 
2023-06-03 20:00:35 [DEBUG] Epoch 15 - {'loss': '0.064552', 'mae': '0.167752', 'lr': '0.001000'} 
2023-06-03 20:00:38 [DEBUG] Epoch 16 - {'loss': '0.064434', 'mae': '0.167524', 'lr': '0.001000'} 
2023-06-03 20:00:40 [DEBUG] Epoch 17 - {'loss': '0.064392', 'mae': '0.167456', 'lr': '0.001000'} 
2023-06-03 20:00:43 [DEBUG] Epoch 18 - {'loss': '0.064393', 'mae': '0.167470', 'lr': '0.001000'} 
2023-06-03 20:00:45 [DEBUG] Epoch 19 - {'loss': '0.064282', 'mae': '0.167273', 'lr': '0.001000'} 
2023-06-03 20:00:45 [INFO] Training finished, elapsed time: 49.87 seconds
2023-06-03 20:00:45 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:03:07 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:03:07
2023-06-03 20:03:07 [INFO] Model configurations:
2023-06-03 20:03:07 [INFO] name: WF_T_21
2023-06-03 20:03:07 [INFO] loss_function: mse
2023-06-03 20:03:07 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:03:07 [INFO] metric: mae
2023-06-03 20:03:07 [INFO] shuffle: True
2023-06-03 20:03:07 [INFO] epochs: 20
2023-06-03 20:03:07 [INFO] batch_size: 512
2023-06-03 20:03:07 [INFO] verbose: 1
2023-06-03 20:03:07 [INFO] TensorBoard_log_path: logs
2023-06-03 20:03:07 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:03:07 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:03:07 [INFO] EarlyStopping_patience: 15
2023-06-03 20:03:07 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:03:07 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:03:07 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:03:07 [INFO] Checkpoint_monitor: loss
2023-06-03 20:03:07 [INFO] save_best_only: True
2023-06-03 20:03:07 [INFO] log_path: logs/training.log
2023-06-03 20:03:07 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:03:07 [INFO] Training in progress
2023-06-03 20:03:08 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0057s). Check your callbacks.
2023-06-03 20:03:11 [DEBUG] Epoch 0 - {'loss': '0.114732', 'mae': '0.222382', 'lr': '0.001000'} 
2023-06-03 20:03:13 [DEBUG] Epoch 1 - {'loss': '0.075299', 'mae': '0.182524', 'lr': '0.001000'} 
2023-06-03 20:03:16 [DEBUG] Epoch 2 - {'loss': '0.072511', 'mae': '0.178763', 'lr': '0.001000'} 
2023-06-03 20:03:19 [DEBUG] Epoch 3 - {'loss': '0.070744', 'mae': '0.176174', 'lr': '0.001000'} 
2023-06-03 20:03:21 [DEBUG] Epoch 4 - {'loss': '0.069221', 'mae': '0.174126', 'lr': '0.001000'} 
2023-06-03 20:03:24 [DEBUG] Epoch 5 - {'loss': '0.068173', 'mae': '0.172857', 'lr': '0.001000'} 
2023-06-03 20:03:26 [DEBUG] Epoch 6 - {'loss': '0.067398', 'mae': '0.171931', 'lr': '0.001000'} 
2023-06-03 20:03:29 [DEBUG] Epoch 7 - {'loss': '0.066794', 'mae': '0.171159', 'lr': '0.001000'} 
2023-06-03 20:03:31 [DEBUG] Epoch 8 - {'loss': '0.066363', 'mae': '0.170610', 'lr': '0.001000'} 
2023-06-03 20:03:34 [DEBUG] Epoch 9 - {'loss': '0.065945', 'mae': '0.169971', 'lr': '0.001000'} 
2023-06-03 20:03:37 [DEBUG] Epoch 10 - {'loss': '0.065591', 'mae': '0.169391', 'lr': '0.001000'} 
2023-06-03 20:03:39 [DEBUG] Epoch 11 - {'loss': '0.065293', 'mae': '0.168878', 'lr': '0.001000'} 
2023-06-03 20:03:42 [DEBUG] Epoch 12 - {'loss': '0.065086', 'mae': '0.168510', 'lr': '0.001000'} 
2023-06-03 20:03:44 [DEBUG] Epoch 13 - {'loss': '0.064895', 'mae': '0.168176', 'lr': '0.001000'} 
2023-06-03 20:03:47 [DEBUG] Epoch 14 - {'loss': '0.064752', 'mae': '0.167896', 'lr': '0.001000'} 
2023-06-03 20:03:50 [DEBUG] Epoch 15 - {'loss': '0.064638', 'mae': '0.167736', 'lr': '0.001000'} 
2023-06-03 20:03:52 [DEBUG] Epoch 16 - {'loss': '0.064569', 'mae': '0.167619', 'lr': '0.001000'} 
2023-06-03 20:03:55 [DEBUG] Epoch 17 - {'loss': '0.064475', 'mae': '0.167422', 'lr': '0.001000'} 
2023-06-03 20:03:57 [DEBUG] Epoch 18 - {'loss': '0.064393', 'mae': '0.167308', 'lr': '0.001000'} 
2023-06-03 20:04:00 [DEBUG] Epoch 19 - {'loss': '0.064352', 'mae': '0.167246', 'lr': '0.001000'} 
2023-06-03 20:04:00 [INFO] Training finished, elapsed time: 52.38 seconds
2023-06-03 20:04:00 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:06:24 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:06:24
2023-06-03 20:06:24 [INFO] Model configurations:
2023-06-03 20:06:24 [INFO] name: WF_T_21
2023-06-03 20:06:24 [INFO] loss_function: mse
2023-06-03 20:06:24 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:06:24 [INFO] metric: mae
2023-06-03 20:06:24 [INFO] shuffle: True
2023-06-03 20:06:24 [INFO] epochs: 20
2023-06-03 20:06:24 [INFO] batch_size: 512
2023-06-03 20:06:24 [INFO] verbose: 1
2023-06-03 20:06:24 [INFO] TensorBoard_log_path: logs
2023-06-03 20:06:24 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:06:24 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:06:24 [INFO] EarlyStopping_patience: 15
2023-06-03 20:06:24 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:06:24 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:06:24 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:06:24 [INFO] Checkpoint_monitor: loss
2023-06-03 20:06:24 [INFO] save_best_only: True
2023-06-03 20:06:24 [INFO] log_path: logs/training.log
2023-06-03 20:06:24 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:06:24 [INFO] Training in progress
2023-06-03 20:06:25 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.
2023-06-03 20:06:28 [DEBUG] Epoch 0 - {'loss': '0.148978', 'mae': '0.249248', 'lr': '0.001000'} 
2023-06-03 20:06:30 [DEBUG] Epoch 1 - {'loss': '0.073418', 'mae': '0.189084', 'lr': '0.001000'} 
2023-06-03 20:06:33 [DEBUG] Epoch 2 - {'loss': '0.065042', 'mae': '0.180170', 'lr': '0.001000'} 
2023-06-03 20:06:35 [DEBUG] Epoch 3 - {'loss': '0.054130', 'mae': '0.166308', 'lr': '0.001000'} 
2023-06-03 20:06:38 [DEBUG] Epoch 4 - {'loss': '0.049213', 'mae': '0.160039', 'lr': '0.001000'} 
2023-06-03 20:06:40 [DEBUG] Epoch 5 - {'loss': '0.046941', 'mae': '0.157020', 'lr': '0.001000'} 
2023-06-03 20:06:43 [DEBUG] Epoch 6 - {'loss': '0.045158', 'mae': '0.154577', 'lr': '0.001000'} 
2023-06-03 20:06:45 [DEBUG] Epoch 7 - {'loss': '0.043602', 'mae': '0.152334', 'lr': '0.001000'} 
2023-06-03 20:06:48 [DEBUG] Epoch 8 - {'loss': '0.041949', 'mae': '0.149747', 'lr': '0.001000'} 
2023-06-03 20:06:50 [DEBUG] Epoch 9 - {'loss': '0.040290', 'mae': '0.146902', 'lr': '0.001000'} 
2023-06-03 20:06:53 [DEBUG] Epoch 10 - {'loss': '0.039259', 'mae': '0.145067', 'lr': '0.001000'} 
2023-06-03 20:06:55 [DEBUG] Epoch 11 - {'loss': '0.038773', 'mae': '0.144151', 'lr': '0.001000'} 
2023-06-03 20:06:58 [DEBUG] Epoch 12 - {'loss': '0.038354', 'mae': '0.143330', 'lr': '0.001000'} 
2023-06-03 20:07:01 [DEBUG] Epoch 13 - {'loss': '0.038096', 'mae': '0.142811', 'lr': '0.001000'} 
2023-06-03 20:07:03 [DEBUG] Epoch 14 - {'loss': '0.037871', 'mae': '0.142388', 'lr': '0.001000'} 
2023-06-03 20:07:06 [DEBUG] Epoch 15 - {'loss': '0.037647', 'mae': '0.141962', 'lr': '0.001000'} 
2023-06-03 20:07:08 [DEBUG] Epoch 16 - {'loss': '0.037466', 'mae': '0.141600', 'lr': '0.001000'} 
2023-06-03 20:07:11 [DEBUG] Epoch 17 - {'loss': '0.037329', 'mae': '0.141345', 'lr': '0.001000'} 
2023-06-03 20:07:13 [DEBUG] Epoch 18 - {'loss': '0.037176', 'mae': '0.141034', 'lr': '0.001000'} 
2023-06-03 20:07:16 [DEBUG] Epoch 19 - {'loss': '0.037095', 'mae': '0.140887', 'lr': '0.001000'} 
2023-06-03 20:07:16 [INFO] Training finished, elapsed time: 51.47 seconds
2023-06-03 20:07:16 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:08:47 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:08:47
2023-06-03 20:08:47 [INFO] Model configurations:
2023-06-03 20:08:47 [INFO] name: WF_T_21
2023-06-03 20:08:47 [INFO] loss_function: mse
2023-06-03 20:08:47 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:08:47 [INFO] metric: mae
2023-06-03 20:08:47 [INFO] shuffle: True
2023-06-03 20:08:47 [INFO] epochs: 20
2023-06-03 20:08:47 [INFO] batch_size: 512
2023-06-03 20:08:47 [INFO] verbose: 1
2023-06-03 20:08:47 [INFO] TensorBoard_log_path: logs
2023-06-03 20:08:47 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:08:47 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:08:47 [INFO] EarlyStopping_patience: 15
2023-06-03 20:08:47 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:08:47 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:08:47 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:08:47 [INFO] Checkpoint_monitor: loss
2023-06-03 20:08:47 [INFO] save_best_only: True
2023-06-03 20:08:47 [INFO] log_path: logs/training.log
2023-06-03 20:08:47 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:08:47 [INFO] Training in progress
2023-06-03 20:08:50 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0233s). Check your callbacks.
2023-06-03 20:09:02 [DEBUG] Epoch 0 - {'loss': '0.138055', 'mae': '0.219250', 'lr': '0.001000'} 
2023-06-03 20:09:15 [DEBUG] Epoch 1 - {'loss': '0.051396', 'mae': '0.161858', 'lr': '0.001000'} 
2023-06-03 20:09:28 [DEBUG] Epoch 2 - {'loss': '0.048720', 'mae': '0.158106', 'lr': '0.001000'} 
2023-06-03 20:09:40 [DEBUG] Epoch 3 - {'loss': '0.047314', 'mae': '0.156074', 'lr': '0.001000'} 
2023-06-03 20:09:53 [DEBUG] Epoch 4 - {'loss': '0.046467', 'mae': '0.154955', 'lr': '0.001000'} 
2023-06-03 20:10:06 [DEBUG] Epoch 5 - {'loss': '0.046045', 'mae': '0.154311', 'lr': '0.001000'} 
2023-06-03 20:10:19 [DEBUG] Epoch 6 - {'loss': '0.045596', 'mae': '0.153728', 'lr': '0.001000'} 
2023-06-03 20:10:31 [DEBUG] Epoch 7 - {'loss': '0.045545', 'mae': '0.153607', 'lr': '0.001000'} 
2023-06-03 20:10:44 [DEBUG] Epoch 8 - {'loss': '0.045188', 'mae': '0.153048', 'lr': '0.001000'} 
2023-06-03 20:10:57 [DEBUG] Epoch 9 - {'loss': '0.045138', 'mae': '0.152871', 'lr': '0.001000'} 
2023-06-03 20:11:09 [DEBUG] Epoch 10 - {'loss': '0.044900', 'mae': '0.152617', 'lr': '0.001000'} 
2023-06-03 20:11:22 [DEBUG] Epoch 11 - {'loss': '0.044775', 'mae': '0.152365', 'lr': '0.001000'} 
2023-06-03 20:11:35 [DEBUG] Epoch 12 - {'loss': '0.044781', 'mae': '0.152386', 'lr': '0.001000'} 
2023-06-03 20:11:48 [DEBUG] Epoch 13 - {'loss': '0.044596', 'mae': '0.152009', 'lr': '0.001000'} 
2023-06-03 20:12:00 [DEBUG] Epoch 14 - {'loss': '0.044543', 'mae': '0.151989', 'lr': '0.001000'} 
2023-06-03 20:12:13 [DEBUG] Epoch 15 - {'loss': '0.044390', 'mae': '0.151695', 'lr': '0.001000'} 
2023-06-03 20:12:26 [DEBUG] Epoch 16 - {'loss': '0.044329', 'mae': '0.151569', 'lr': '0.001000'} 
2023-06-03 20:12:38 [DEBUG] Epoch 17 - {'loss': '0.044279', 'mae': '0.151504', 'lr': '0.001000'} 
2023-06-03 20:12:51 [DEBUG] Epoch 18 - {'loss': '0.044326', 'mae': '0.151627', 'lr': '0.001000'} 
2023-06-03 20:13:04 [DEBUG] Epoch 19 - {'loss': '0.044198', 'mae': '0.151351', 'lr': '0.001000'} 
2023-06-03 20:13:04 [INFO] Training finished, elapsed time: 256.78 seconds
2023-06-03 20:13:04 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:17:23 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:17:23
2023-06-03 20:17:23 [INFO] Model configurations:
2023-06-03 20:17:23 [INFO] name: WF_T_21
2023-06-03 20:17:23 [INFO] loss_function: mse
2023-06-03 20:17:23 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:17:23 [INFO] metric: mae
2023-06-03 20:17:23 [INFO] shuffle: True
2023-06-03 20:17:23 [INFO] epochs: 20
2023-06-03 20:17:23 [INFO] batch_size: 512
2023-06-03 20:17:23 [INFO] verbose: 1
2023-06-03 20:17:23 [INFO] TensorBoard_log_path: logs
2023-06-03 20:17:23 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:17:23 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:17:23 [INFO] EarlyStopping_patience: 15
2023-06-03 20:17:23 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:17:23 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:17:23 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:17:23 [INFO] Checkpoint_monitor: loss
2023-06-03 20:17:23 [INFO] save_best_only: True
2023-06-03 20:17:23 [INFO] log_path: logs/training.log
2023-06-03 20:17:23 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:17:23 [INFO] Training in progress
2023-06-03 20:17:24 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-03 20:17:26 [DEBUG] Epoch 0 - {'loss': '0.209998', 'mae': '0.277004', 'lr': '0.001000'} 
2023-06-03 20:17:29 [DEBUG] Epoch 1 - {'loss': '0.048148', 'mae': '0.159247', 'lr': '0.001000'} 
2023-06-03 20:17:32 [DEBUG] Epoch 2 - {'loss': '0.038192', 'mae': '0.145520', 'lr': '0.001000'} 
2023-06-03 20:17:34 [DEBUG] Epoch 3 - {'loss': '0.032471', 'mae': '0.135802', 'lr': '0.001000'} 
2023-06-03 20:17:37 [DEBUG] Epoch 4 - {'loss': '0.029362', 'mae': '0.130093', 'lr': '0.001000'} 
2023-06-03 20:17:40 [DEBUG] Epoch 5 - {'loss': '0.027068', 'mae': '0.125870', 'lr': '0.001000'} 
2023-06-03 20:17:42 [DEBUG] Epoch 6 - {'loss': '0.025501', 'mae': '0.122906', 'lr': '0.001000'} 
2023-06-03 20:17:45 [DEBUG] Epoch 7 - {'loss': '0.024922', 'mae': '0.121906', 'lr': '0.001000'} 
2023-06-03 20:17:48 [DEBUG] Epoch 8 - {'loss': '0.024585', 'mae': '0.121282', 'lr': '0.001000'} 
2023-06-03 20:17:50 [DEBUG] Epoch 9 - {'loss': '0.024362', 'mae': '0.120884', 'lr': '0.001000'} 
2023-06-03 20:17:53 [DEBUG] Epoch 10 - {'loss': '0.024318', 'mae': '0.120915', 'lr': '0.001000'} 
2023-06-03 20:17:56 [DEBUG] Epoch 11 - {'loss': '0.024151', 'mae': '0.120567', 'lr': '0.001000'} 
2023-06-03 20:17:58 [DEBUG] Epoch 12 - {'loss': '0.024072', 'mae': '0.120418', 'lr': '0.001000'} 
2023-06-03 20:18:01 [DEBUG] Epoch 13 - {'loss': '0.023982', 'mae': '0.120217', 'lr': '0.001000'} 
2023-06-03 20:18:04 [DEBUG] Epoch 14 - {'loss': '0.023997', 'mae': '0.120275', 'lr': '0.001000'} 
2023-06-03 20:18:06 [DEBUG] Epoch 15 - {'loss': '0.023928', 'mae': '0.120152', 'lr': '0.001000'} 
2023-06-03 20:18:09 [DEBUG] Epoch 16 - {'loss': '0.023935', 'mae': '0.120186', 'lr': '0.001000'} 
2023-06-03 20:18:12 [DEBUG] Epoch 17 - {'loss': '0.023697', 'mae': '0.119551', 'lr': '0.000500'} 
2023-06-03 20:18:14 [DEBUG] Epoch 18 - {'loss': '0.023743', 'mae': '0.119684', 'lr': '0.000500'} 
2023-06-03 20:18:17 [DEBUG] Epoch 19 - {'loss': '0.023742', 'mae': '0.119699', 'lr': '0.000500'} 
2023-06-03 20:18:17 [INFO] Training finished, elapsed time: 54.09 seconds
2023-06-03 20:18:17 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:20:47 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:20:47
2023-06-03 20:20:47 [INFO] Model configurations:
2023-06-03 20:20:47 [INFO] name: WF_T_21
2023-06-03 20:20:47 [INFO] loss_function: mse
2023-06-03 20:20:47 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:20:47 [INFO] metric: mae
2023-06-03 20:20:47 [INFO] shuffle: True
2023-06-03 20:20:47 [INFO] epochs: 20
2023-06-03 20:20:47 [INFO] batch_size: 512
2023-06-03 20:20:47 [INFO] verbose: 1
2023-06-03 20:20:47 [INFO] TensorBoard_log_path: logs
2023-06-03 20:20:47 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:20:47 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:20:47 [INFO] EarlyStopping_patience: 15
2023-06-03 20:20:47 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:20:47 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:20:47 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:20:47 [INFO] Checkpoint_monitor: loss
2023-06-03 20:20:47 [INFO] save_best_only: True
2023-06-03 20:20:47 [INFO] log_path: logs/training.log
2023-06-03 20:20:47 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:20:48 [INFO] Training in progress
2023-06-03 20:20:48 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-03 20:20:51 [DEBUG] Epoch 0 - {'loss': '0.110879', 'mae': '0.235130', 'lr': '0.001000'} 
2023-06-03 20:20:54 [DEBUG] Epoch 1 - {'loss': '0.041439', 'mae': '0.153917', 'lr': '0.001000'} 
2023-06-03 20:20:56 [DEBUG] Epoch 2 - {'loss': '0.033144', 'mae': '0.139150', 'lr': '0.001000'} 
2023-06-03 20:20:59 [DEBUG] Epoch 3 - {'loss': '0.030568', 'mae': '0.134080', 'lr': '0.001000'} 
2023-06-03 20:21:02 [DEBUG] Epoch 4 - {'loss': '0.029162', 'mae': '0.131211', 'lr': '0.001000'} 
2023-06-03 20:21:04 [DEBUG] Epoch 5 - {'loss': '0.028393', 'mae': '0.129693', 'lr': '0.001000'} 
2023-06-03 20:21:07 [DEBUG] Epoch 6 - {'loss': '0.027850', 'mae': '0.128601', 'lr': '0.001000'} 
2023-06-03 20:21:10 [DEBUG] Epoch 7 - {'loss': '0.027415', 'mae': '0.127689', 'lr': '0.001000'} 
2023-06-03 20:21:12 [DEBUG] Epoch 8 - {'loss': '0.027089', 'mae': '0.126991', 'lr': '0.001000'} 
2023-06-03 20:21:15 [DEBUG] Epoch 9 - {'loss': '0.026807', 'mae': '0.126350', 'lr': '0.001000'} 
2023-06-03 20:21:18 [DEBUG] Epoch 10 - {'loss': '0.026582', 'mae': '0.125853', 'lr': '0.001000'} 
2023-06-03 20:21:20 [DEBUG] Epoch 11 - {'loss': '0.026361', 'mae': '0.125370', 'lr': '0.001000'} 
2023-06-03 20:21:23 [DEBUG] Epoch 12 - {'loss': '0.026212', 'mae': '0.125025', 'lr': '0.001000'} 
2023-06-03 20:21:26 [DEBUG] Epoch 13 - {'loss': '0.026099', 'mae': '0.124756', 'lr': '0.001000'} 
2023-06-03 20:21:28 [DEBUG] Epoch 14 - {'loss': '0.025995', 'mae': '0.124519', 'lr': '0.001000'} 
2023-06-03 20:21:31 [DEBUG] Epoch 15 - {'loss': '0.025952', 'mae': '0.124423', 'lr': '0.001000'} 
2023-06-03 20:21:34 [DEBUG] Epoch 16 - {'loss': '0.025868', 'mae': '0.124228', 'lr': '0.001000'} 
2023-06-03 20:21:36 [DEBUG] Epoch 17 - {'loss': '0.025831', 'mae': '0.124159', 'lr': '0.001000'} 
2023-06-03 20:21:39 [DEBUG] Epoch 18 - {'loss': '0.025740', 'mae': '0.123952', 'lr': '0.001000'} 
2023-06-03 20:21:42 [DEBUG] Epoch 19 - {'loss': '0.025714', 'mae': '0.123892', 'lr': '0.001000'} 
2023-06-03 20:21:42 [INFO] Training finished, elapsed time: 54.36 seconds
2023-06-03 20:21:42 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:23:19 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:23:19
2023-06-03 20:23:19 [INFO] Model configurations:
2023-06-03 20:23:19 [INFO] name: WF_T_21
2023-06-03 20:23:19 [INFO] loss_function: mse
2023-06-03 20:23:19 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:23:19 [INFO] metric: mae
2023-06-03 20:23:19 [INFO] shuffle: True
2023-06-03 20:23:19 [INFO] epochs: 20
2023-06-03 20:23:19 [INFO] batch_size: 512
2023-06-03 20:23:19 [INFO] verbose: 1
2023-06-03 20:23:19 [INFO] TensorBoard_log_path: logs
2023-06-03 20:23:19 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:23:19 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:23:19 [INFO] EarlyStopping_patience: 15
2023-06-03 20:23:19 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:23:19 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:23:19 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:23:19 [INFO] Checkpoint_monitor: loss
2023-06-03 20:23:19 [INFO] save_best_only: True
2023-06-03 20:23:19 [INFO] log_path: logs/training.log
2023-06-03 20:23:19 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:23:19 [INFO] Training in progress
2023-06-03 20:23:21 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0228s). Check your callbacks.
2023-06-03 20:23:34 [DEBUG] Epoch 0 - {'loss': '0.070152', 'mae': '0.186204', 'lr': '0.001000'} 
2023-06-03 20:23:46 [DEBUG] Epoch 1 - {'loss': '0.047842', 'mae': '0.157092', 'lr': '0.001000'} 
2023-06-03 20:23:59 [DEBUG] Epoch 2 - {'loss': '0.040349', 'mae': '0.146552', 'lr': '0.001000'} 
2023-06-03 20:24:12 [DEBUG] Epoch 3 - {'loss': '0.038999', 'mae': '0.144475', 'lr': '0.001000'} 
2023-06-03 20:24:24 [DEBUG] Epoch 4 - {'loss': '0.038515', 'mae': '0.143722', 'lr': '0.001000'} 
2023-06-03 20:24:37 [DEBUG] Epoch 5 - {'loss': '0.038020', 'mae': '0.142807', 'lr': '0.001000'} 
2023-06-03 20:24:49 [DEBUG] Epoch 6 - {'loss': '0.037703', 'mae': '0.142301', 'lr': '0.001000'} 
2023-06-03 20:25:02 [DEBUG] Epoch 7 - {'loss': '0.037484', 'mae': '0.141930', 'lr': '0.001000'} 
2023-06-03 20:25:14 [DEBUG] Epoch 8 - {'loss': '0.037243', 'mae': '0.141489', 'lr': '0.001000'} 
2023-06-03 20:25:27 [DEBUG] Epoch 9 - {'loss': '0.036972', 'mae': '0.140959', 'lr': '0.001000'} 
2023-06-03 20:25:39 [DEBUG] Epoch 10 - {'loss': '0.036733', 'mae': '0.140478', 'lr': '0.001000'} 
2023-06-03 20:25:52 [DEBUG] Epoch 11 - {'loss': '0.036790', 'mae': '0.140567', 'lr': '0.001000'} 
2023-06-03 20:26:04 [DEBUG] Epoch 12 - {'loss': '0.036608', 'mae': '0.140221', 'lr': '0.001000'} 
2023-06-03 20:26:17 [DEBUG] Epoch 13 - {'loss': '0.036461', 'mae': '0.139928', 'lr': '0.001000'} 
2023-06-03 20:26:29 [DEBUG] Epoch 14 - {'loss': '0.036512', 'mae': '0.140091', 'lr': '0.001000'} 
2023-06-03 20:26:42 [DEBUG] Epoch 15 - {'loss': '0.036464', 'mae': '0.139985', 'lr': '0.001000'} 
2023-06-03 20:26:54 [DEBUG] Epoch 16 - {'loss': '0.036194', 'mae': '0.139470', 'lr': '0.001000'} 
2023-06-03 20:27:07 [DEBUG] Epoch 17 - {'loss': '0.036161', 'mae': '0.139427', 'lr': '0.001000'} 
2023-06-03 20:27:20 [DEBUG] Epoch 18 - {'loss': '0.036058', 'mae': '0.139189', 'lr': '0.001000'} 
2023-06-03 20:27:32 [DEBUG] Epoch 19 - {'loss': '0.036021', 'mae': '0.139143', 'lr': '0.001000'} 
2023-06-03 20:27:32 [INFO] Training finished, elapsed time: 253.42 seconds
2023-06-03 20:27:32 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:30:03 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:30:03
2023-06-03 20:30:03 [INFO] Model configurations:
2023-06-03 20:30:03 [INFO] name: WF_T_21
2023-06-03 20:30:03 [INFO] loss_function: mse
2023-06-03 20:30:03 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:30:03 [INFO] metric: mae
2023-06-03 20:30:03 [INFO] shuffle: True
2023-06-03 20:30:03 [INFO] epochs: 20
2023-06-03 20:30:03 [INFO] batch_size: 512
2023-06-03 20:30:03 [INFO] verbose: 1
2023-06-03 20:30:03 [INFO] TensorBoard_log_path: logs
2023-06-03 20:30:03 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:30:03 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:30:03 [INFO] EarlyStopping_patience: 15
2023-06-03 20:30:03 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:30:03 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:30:03 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:30:03 [INFO] Checkpoint_monitor: loss
2023-06-03 20:30:03 [INFO] save_best_only: True
2023-06-03 20:30:03 [INFO] log_path: logs/training.log
2023-06-03 20:30:03 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:30:03 [INFO] Training in progress
2023-06-03 20:30:06 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0127s vs `on_train_batch_end` time: 0.0229s). Check your callbacks.
2023-06-03 20:30:19 [DEBUG] Epoch 0 - {'loss': '0.088879', 'mae': '0.204115', 'lr': '0.001000'} 
2023-06-03 20:30:31 [DEBUG] Epoch 1 - {'loss': '0.033468', 'mae': '0.141740', 'lr': '0.001000'} 
2023-06-03 20:30:44 [DEBUG] Epoch 2 - {'loss': '0.029142', 'mae': '0.132054', 'lr': '0.001000'} 
2023-06-03 20:30:57 [DEBUG] Epoch 3 - {'loss': '0.027025', 'mae': '0.127112', 'lr': '0.001000'} 
2023-06-03 20:31:09 [DEBUG] Epoch 4 - {'loss': '0.026083', 'mae': '0.124882', 'lr': '0.001000'} 
2023-06-03 20:31:22 [DEBUG] Epoch 5 - {'loss': '0.025483', 'mae': '0.123387', 'lr': '0.001000'} 
2023-06-03 20:31:35 [DEBUG] Epoch 6 - {'loss': '0.025198', 'mae': '0.122735', 'lr': '0.001000'} 
2023-06-03 20:31:47 [DEBUG] Epoch 7 - {'loss': '0.024877', 'mae': '0.121983', 'lr': '0.001000'} 
2023-06-03 20:32:00 [DEBUG] Epoch 8 - {'loss': '0.024593', 'mae': '0.121303', 'lr': '0.001000'} 
2023-06-03 20:32:13 [DEBUG] Epoch 9 - {'loss': '0.024516', 'mae': '0.121122', 'lr': '0.001000'} 
2023-06-03 20:32:25 [DEBUG] Epoch 10 - {'loss': '0.024335', 'mae': '0.120707', 'lr': '0.001000'} 
2023-06-03 20:32:38 [DEBUG] Epoch 11 - {'loss': '0.024208', 'mae': '0.120404', 'lr': '0.001000'} 
2023-06-03 20:32:51 [DEBUG] Epoch 12 - {'loss': '0.024075', 'mae': '0.120081', 'lr': '0.001000'} 
2023-06-03 20:33:03 [DEBUG] Epoch 13 - {'loss': '0.024047', 'mae': '0.120003', 'lr': '0.001000'} 
2023-06-03 20:33:16 [DEBUG] Epoch 14 - {'loss': '0.024018', 'mae': '0.119946', 'lr': '0.001000'} 
2023-06-03 20:33:29 [DEBUG] Epoch 15 - {'loss': '0.023990', 'mae': '0.119871', 'lr': '0.001000'} 
2023-06-03 20:33:42 [DEBUG] Epoch 16 - {'loss': '0.023750', 'mae': '0.119283', 'lr': '0.000500'} 
2023-06-03 20:33:54 [DEBUG] Epoch 17 - {'loss': '0.023658', 'mae': '0.119047', 'lr': '0.000500'} 
2023-06-03 20:34:07 [DEBUG] Epoch 18 - {'loss': '0.023613', 'mae': '0.118947', 'lr': '0.000500'} 
2023-06-03 20:34:20 [DEBUG] Epoch 19 - {'loss': '0.023566', 'mae': '0.118808', 'lr': '0.000500'} 
2023-06-03 20:34:20 [INFO] Training finished, elapsed time: 256.25 seconds
2023-06-03 20:34:20 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:37:07 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:37:07
2023-06-03 20:37:07 [INFO] Model configurations:
2023-06-03 20:37:07 [INFO] name: WF_T_21
2023-06-03 20:37:07 [INFO] loss_function: mse
2023-06-03 20:37:07 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:37:07 [INFO] metric: mae
2023-06-03 20:37:07 [INFO] shuffle: True
2023-06-03 20:37:07 [INFO] epochs: 20
2023-06-03 20:37:07 [INFO] batch_size: 512
2023-06-03 20:37:07 [INFO] verbose: 1
2023-06-03 20:37:07 [INFO] TensorBoard_log_path: logs
2023-06-03 20:37:07 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:37:07 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:37:07 [INFO] EarlyStopping_patience: 15
2023-06-03 20:37:07 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:37:07 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:37:07 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:37:07 [INFO] Checkpoint_monitor: loss
2023-06-03 20:37:07 [INFO] save_best_only: True
2023-06-03 20:37:07 [INFO] log_path: logs/training.log
2023-06-03 20:37:07 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:37:07 [INFO] Training in progress
2023-06-03 20:37:09 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0137s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.
2023-06-03 20:37:26 [DEBUG] Epoch 0 - {'loss': '0.048819', 'mae': '0.153151', 'lr': '0.001000'} 
2023-06-03 20:37:42 [DEBUG] Epoch 1 - {'loss': '0.027609', 'mae': '0.127941', 'lr': '0.001000'} 
2023-06-03 20:37:58 [DEBUG] Epoch 2 - {'loss': '0.026262', 'mae': '0.125090', 'lr': '0.001000'} 
2023-06-03 20:38:15 [DEBUG] Epoch 3 - {'loss': '0.025919', 'mae': '0.124429', 'lr': '0.001000'} 
2023-06-03 20:38:31 [DEBUG] Epoch 4 - {'loss': '0.025286', 'mae': '0.122958', 'lr': '0.001000'} 
2023-06-03 20:38:48 [DEBUG] Epoch 5 - {'loss': '0.025179', 'mae': '0.122741', 'lr': '0.001000'} 
2023-06-03 20:39:04 [DEBUG] Epoch 6 - {'loss': '0.025107', 'mae': '0.122593', 'lr': '0.001000'} 
2023-06-03 20:39:21 [DEBUG] Epoch 7 - {'loss': '0.024782', 'mae': '0.121759', 'lr': '0.001000'} 
2023-06-03 20:39:37 [DEBUG] Epoch 8 - {'loss': '0.024647', 'mae': '0.121429', 'lr': '0.001000'} 
2023-06-03 20:39:53 [DEBUG] Epoch 9 - {'loss': '0.024412', 'mae': '0.120862', 'lr': '0.001000'} 
2023-06-03 20:40:10 [DEBUG] Epoch 10 - {'loss': '0.024443', 'mae': '0.120962', 'lr': '0.001000'} 
2023-06-03 20:40:26 [DEBUG] Epoch 11 - {'loss': '0.024220', 'mae': '0.120384', 'lr': '0.001000'} 
2023-06-03 20:40:43 [DEBUG] Epoch 12 - {'loss': '0.024266', 'mae': '0.120556', 'lr': '0.001000'} 
2023-06-03 20:40:59 [DEBUG] Epoch 13 - {'loss': '0.024074', 'mae': '0.120055', 'lr': '0.001000'} 
2023-06-03 20:41:15 [DEBUG] Epoch 14 - {'loss': '0.024011', 'mae': '0.119903', 'lr': '0.001000'} 
2023-06-03 20:41:32 [DEBUG] Epoch 15 - {'loss': '0.024028', 'mae': '0.119938', 'lr': '0.001000'} 
2023-06-03 20:41:48 [DEBUG] Epoch 16 - {'loss': '0.023974', 'mae': '0.119844', 'lr': '0.001000'} 
2023-06-03 20:42:05 [DEBUG] Epoch 17 - {'loss': '0.023701', 'mae': '0.119135', 'lr': '0.000500'} 
2023-06-03 20:42:21 [DEBUG] Epoch 18 - {'loss': '0.023628', 'mae': '0.118972', 'lr': '0.000500'} 
2023-06-03 20:42:38 [DEBUG] Epoch 19 - {'loss': '0.023599', 'mae': '0.118874', 'lr': '0.000500'} 
2023-06-03 20:42:38 [INFO] Training finished, elapsed time: 330.40 seconds
2023-06-03 20:42:38 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-03 20:44:27 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/03 20:44:27
2023-06-03 20:44:27 [INFO] Model configurations:
2023-06-03 20:44:27 [INFO] name: WF_T_21
2023-06-03 20:44:27 [INFO] loss_function: mse
2023-06-03 20:44:27 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-03 20:44:27 [INFO] metric: mae
2023-06-03 20:44:27 [INFO] shuffle: True
2023-06-03 20:44:27 [INFO] epochs: 200
2023-06-03 20:44:27 [INFO] batch_size: 512
2023-06-03 20:44:27 [INFO] verbose: 1
2023-06-03 20:44:27 [INFO] TensorBoard_log_path: logs
2023-06-03 20:44:27 [INFO] TensorBoard_hist_freq: 1
2023-06-03 20:44:27 [INFO] EarlyStopping_monitor: loss
2023-06-03 20:44:27 [INFO] EarlyStopping_patience: 15
2023-06-03 20:44:27 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-03 20:44:27 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-03 20:44:27 [INFO] ReduceLROnPlateau_patience: 3
2023-06-03 20:44:27 [INFO] Checkpoint_monitor: loss
2023-06-03 20:44:27 [INFO] save_best_only: True
2023-06-03 20:44:27 [INFO] log_path: logs/training.log
2023-06-03 20:44:27 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-03 20:44:27 [INFO] Training in progress
2023-06-03 20:44:29 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.
2023-06-03 20:44:45 [DEBUG] Epoch 0 - {'loss': '0.045366', 'mae': '0.151012', 'lr': '0.001000'} 
2023-06-03 20:45:02 [DEBUG] Epoch 1 - {'loss': '0.026961', 'mae': '0.126601', 'lr': '0.001000'} 
2023-06-03 20:45:18 [DEBUG] Epoch 2 - {'loss': '0.025860', 'mae': '0.124233', 'lr': '0.001000'} 
2023-06-03 20:45:34 [DEBUG] Epoch 3 - {'loss': '0.025499', 'mae': '0.123475', 'lr': '0.001000'} 
2023-06-03 20:45:51 [DEBUG] Epoch 4 - {'loss': '0.025161', 'mae': '0.122676', 'lr': '0.001000'} 
2023-06-03 20:46:07 [DEBUG] Epoch 5 - {'loss': '0.025045', 'mae': '0.122402', 'lr': '0.001000'} 
2023-06-03 20:46:24 [DEBUG] Epoch 6 - {'loss': '0.024812', 'mae': '0.121851', 'lr': '0.001000'} 
2023-06-03 20:46:40 [DEBUG] Epoch 7 - {'loss': '0.024771', 'mae': '0.121752', 'lr': '0.001000'} 
2023-06-03 20:46:57 [DEBUG] Epoch 8 - {'loss': '0.024538', 'mae': '0.121177', 'lr': '0.001000'} 
2023-06-03 20:47:13 [DEBUG] Epoch 9 - {'loss': '0.024348', 'mae': '0.120703', 'lr': '0.001000'} 
2023-06-03 20:47:29 [DEBUG] Epoch 10 - {'loss': '0.024359', 'mae': '0.120775', 'lr': '0.001000'} 
2023-06-03 20:47:46 [DEBUG] Epoch 11 - {'loss': '0.024293', 'mae': '0.120600', 'lr': '0.001000'} 
2023-06-03 20:48:02 [DEBUG] Epoch 12 - {'loss': '0.024246', 'mae': '0.120501', 'lr': '0.001000'} 
2023-06-03 20:48:19 [DEBUG] Epoch 13 - {'loss': '0.024130', 'mae': '0.120218', 'lr': '0.001000'} 
2023-06-03 20:48:35 [DEBUG] Epoch 14 - {'loss': '0.024168', 'mae': '0.120337', 'lr': '0.001000'} 
2023-06-03 20:48:51 [DEBUG] Epoch 15 - {'loss': '0.023990', 'mae': '0.119880', 'lr': '0.001000'} 
2023-06-03 20:49:08 [DEBUG] Epoch 16 - {'loss': '0.023955', 'mae': '0.119783', 'lr': '0.001000'} 
2023-06-03 20:49:24 [DEBUG] Epoch 17 - {'loss': '0.023927', 'mae': '0.119724', 'lr': '0.001000'} 
2023-06-03 20:49:41 [DEBUG] Epoch 18 - {'loss': '0.023941', 'mae': '0.119765', 'lr': '0.001000'} 
2023-06-03 20:49:57 [DEBUG] Epoch 19 - {'loss': '0.023635', 'mae': '0.118980', 'lr': '0.000500'} 
2023-06-03 20:50:14 [DEBUG] Epoch 20 - {'loss': '0.023628', 'mae': '0.118983', 'lr': '0.000500'} 
2023-06-03 20:50:30 [DEBUG] Epoch 21 - {'loss': '0.023608', 'mae': '0.118935', 'lr': '0.000500'} 
2023-06-03 20:50:47 [DEBUG] Epoch 22 - {'loss': '0.023524', 'mae': '0.118698', 'lr': '0.000500'} 
2023-06-03 20:51:03 [DEBUG] Epoch 23 - {'loss': '0.023522', 'mae': '0.118723', 'lr': '0.000500'} 
2023-06-03 20:51:20 [DEBUG] Epoch 24 - {'loss': '0.023598', 'mae': '0.118879', 'lr': '0.000500'} 
2023-06-03 20:51:36 [DEBUG] Epoch 25 - {'loss': '0.023575', 'mae': '0.118845', 'lr': '0.000500'} 
2023-06-03 20:51:53 [DEBUG] Epoch 26 - {'loss': '0.023445', 'mae': '0.118523', 'lr': '0.000250'} 
2023-06-03 20:52:09 [DEBUG] Epoch 27 - {'loss': '0.023393', 'mae': '0.118387', 'lr': '0.000250'} 
2023-06-03 20:52:26 [DEBUG] Epoch 28 - {'loss': '0.023376', 'mae': '0.118341', 'lr': '0.000250'} 
2023-06-03 20:52:42 [DEBUG] Epoch 29 - {'loss': '0.023391', 'mae': '0.118375', 'lr': '0.000250'} 
2023-06-03 20:52:59 [DEBUG] Epoch 30 - {'loss': '0.023359', 'mae': '0.118292', 'lr': '0.000250'} 
2023-06-03 20:53:15 [DEBUG] Epoch 31 - {'loss': '0.023307', 'mae': '0.118164', 'lr': '0.000125'} 
2023-06-03 20:53:32 [DEBUG] Epoch 32 - {'loss': '0.023293', 'mae': '0.118115', 'lr': '0.000125'} 
2023-06-03 20:53:48 [DEBUG] Epoch 33 - {'loss': '0.023297', 'mae': '0.118117', 'lr': '0.000125'} 
2023-06-03 20:54:05 [DEBUG] Epoch 34 - {'loss': '0.023274', 'mae': '0.118068', 'lr': '0.000125'} 
2023-06-03 20:54:21 [DEBUG] Epoch 35 - {'loss': '0.023297', 'mae': '0.118130', 'lr': '0.000125'} 
2023-06-03 20:54:38 [DEBUG] Epoch 36 - {'loss': '0.023254', 'mae': '0.118015', 'lr': '0.000063'} 
2023-06-03 20:54:55 [DEBUG] Epoch 37 - {'loss': '0.023226', 'mae': '0.117915', 'lr': '0.000063'} 
2023-06-03 20:55:11 [DEBUG] Epoch 38 - {'loss': '0.023272', 'mae': '0.118063', 'lr': '0.000063'} 
2023-06-03 20:55:28 [DEBUG] Epoch 39 - {'loss': '0.023218', 'mae': '0.117897', 'lr': '0.000031'} 
2023-06-04 19:26:57 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 19:26:57
2023-06-04 19:26:57 [INFO] Model configurations:
2023-06-04 19:26:57 [INFO] name: WF_T_21
2023-06-04 19:26:57 [INFO] loss_function: mse
2023-06-04 19:26:57 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 19:26:57 [INFO] metric: mae
2023-06-04 19:26:57 [INFO] shuffle: True
2023-06-04 19:26:57 [INFO] epochs: 200
2023-06-04 19:26:57 [INFO] batch_size: 512
2023-06-04 19:26:57 [INFO] verbose: 1
2023-06-04 19:26:57 [INFO] TensorBoard_log_path: logs
2023-06-04 19:26:57 [INFO] TensorBoard_hist_freq: 1
2023-06-04 19:26:57 [INFO] EarlyStopping_monitor: loss
2023-06-04 19:26:57 [INFO] EarlyStopping_patience: 15
2023-06-04 19:26:57 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 19:26:57 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 19:26:57 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 19:26:57 [INFO] Checkpoint_monitor: loss
2023-06-04 19:26:57 [INFO] save_best_only: True
2023-06-04 19:26:57 [INFO] log_path: logs/training.log
2023-06-04 19:26:57 [INFO] save_model_path: models/best_epoch_WT_F_21_.h5
2023-06-04 19:26:57 [INFO] Training in progress
2023-06-04 19:26:58 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.
2023-06-04 19:27:00 [DEBUG] Epoch 0 - {'loss': '0.084093', 'mae': '0.203584', 'lr': '0.001000'} 
2023-06-04 19:27:03 [DEBUG] Epoch 1 - {'loss': '0.037519', 'mae': '0.145929', 'lr': '0.001000'} 
2023-06-04 19:27:06 [DEBUG] Epoch 2 - {'loss': '0.031776', 'mae': '0.136210', 'lr': '0.001000'} 
2023-06-04 19:27:08 [DEBUG] Epoch 3 - {'loss': '0.029273', 'mae': '0.131418', 'lr': '0.001000'} 
2023-06-04 19:27:11 [DEBUG] Epoch 4 - {'loss': '0.028014', 'mae': '0.128830', 'lr': '0.001000'} 
2023-06-04 19:27:14 [DEBUG] Epoch 5 - {'loss': '0.027409', 'mae': '0.127630', 'lr': '0.001000'} 
2023-06-04 19:27:16 [DEBUG] Epoch 6 - {'loss': '0.026948', 'mae': '0.126647', 'lr': '0.001000'} 
2023-06-04 19:27:19 [DEBUG] Epoch 7 - {'loss': '0.026632', 'mae': '0.125928', 'lr': '0.001000'} 
2023-06-04 19:27:22 [DEBUG] Epoch 8 - {'loss': '0.026350', 'mae': '0.125291', 'lr': '0.001000'} 
2023-06-04 19:27:24 [DEBUG] Epoch 9 - {'loss': '0.026132', 'mae': '0.124788', 'lr': '0.001000'} 
2023-06-04 19:27:27 [DEBUG] Epoch 10 - {'loss': '0.025895', 'mae': '0.124228', 'lr': '0.001000'} 
2023-06-04 19:27:30 [DEBUG] Epoch 11 - {'loss': '0.025785', 'mae': '0.124008', 'lr': '0.001000'} 
2023-06-04 19:27:33 [DEBUG] Epoch 12 - {'loss': '0.025635', 'mae': '0.123647', 'lr': '0.001000'} 
2023-06-04 19:27:35 [DEBUG] Epoch 13 - {'loss': '0.025561', 'mae': '0.123497', 'lr': '0.001000'} 
2023-06-04 19:27:38 [DEBUG] Epoch 14 - {'loss': '0.025395', 'mae': '0.123099', 'lr': '0.001000'} 
2023-06-04 19:27:41 [DEBUG] Epoch 15 - {'loss': '0.025290', 'mae': '0.122855', 'lr': '0.001000'} 
2023-06-04 19:27:43 [DEBUG] Epoch 16 - {'loss': '0.025193', 'mae': '0.122621', 'lr': '0.001000'} 
2023-06-04 19:27:46 [DEBUG] Epoch 17 - {'loss': '0.025123', 'mae': '0.122474', 'lr': '0.001000'} 
2023-06-04 19:27:49 [DEBUG] Epoch 18 - {'loss': '0.025052', 'mae': '0.122285', 'lr': '0.001000'} 
2023-06-04 19:27:51 [DEBUG] Epoch 19 - {'loss': '0.025009', 'mae': '0.122215', 'lr': '0.001000'} 
2023-06-04 19:27:54 [DEBUG] Epoch 20 - {'loss': '0.024937', 'mae': '0.122045', 'lr': '0.001000'} 
2023-06-04 19:27:57 [DEBUG] Epoch 21 - {'loss': '0.024873', 'mae': '0.121892', 'lr': '0.001000'} 
2023-06-04 19:27:59 [DEBUG] Epoch 22 - {'loss': '0.024844', 'mae': '0.121833', 'lr': '0.001000'} 
2023-06-04 19:28:02 [DEBUG] Epoch 23 - {'loss': '0.024809', 'mae': '0.121763', 'lr': '0.001000'} 
2023-06-04 19:28:05 [DEBUG] Epoch 24 - {'loss': '0.024743', 'mae': '0.121609', 'lr': '0.001000'} 
2023-06-04 19:28:07 [DEBUG] Epoch 25 - {'loss': '0.024723', 'mae': '0.121565', 'lr': '0.001000'} 
2023-06-04 19:28:10 [DEBUG] Epoch 26 - {'loss': '0.024708', 'mae': '0.121542', 'lr': '0.001000'} 
2023-06-04 19:28:13 [DEBUG] Epoch 27 - {'loss': '0.024689', 'mae': '0.121504', 'lr': '0.001000'} 
2023-06-04 19:28:16 [DEBUG] Epoch 28 - {'loss': '0.024550', 'mae': '0.121131', 'lr': '0.000500'} 
2023-06-04 19:28:18 [DEBUG] Epoch 29 - {'loss': '0.024535', 'mae': '0.121116', 'lr': '0.000500'} 
2023-06-04 19:28:21 [DEBUG] Epoch 30 - {'loss': '0.024512', 'mae': '0.121063', 'lr': '0.000500'} 
2023-06-04 19:28:24 [DEBUG] Epoch 31 - {'loss': '0.024499', 'mae': '0.121046', 'lr': '0.000500'} 
2023-06-04 19:28:26 [DEBUG] Epoch 32 - {'loss': '0.024463', 'mae': '0.120953', 'lr': '0.000250'} 
2023-06-04 19:28:29 [DEBUG] Epoch 33 - {'loss': '0.024457', 'mae': '0.120947', 'lr': '0.000250'} 
2023-06-04 19:28:32 [DEBUG] Epoch 34 - {'loss': '0.024443', 'mae': '0.120902', 'lr': '0.000250'} 
2023-06-04 19:28:34 [DEBUG] Epoch 35 - {'loss': '0.024425', 'mae': '0.120869', 'lr': '0.000250'} 
2023-06-04 19:28:37 [DEBUG] Epoch 36 - {'loss': '0.024408', 'mae': '0.120822', 'lr': '0.000250'} 
2023-06-04 19:28:40 [DEBUG] Epoch 37 - {'loss': '0.024401', 'mae': '0.120820', 'lr': '0.000250'} 
2023-06-04 19:28:42 [DEBUG] Epoch 38 - {'loss': '0.024390', 'mae': '0.120800', 'lr': '0.000125'} 
2023-06-04 19:28:45 [DEBUG] Epoch 39 - {'loss': '0.024375', 'mae': '0.120752', 'lr': '0.000125'} 
2023-06-04 19:28:48 [DEBUG] Epoch 40 - {'loss': '0.024376', 'mae': '0.120764', 'lr': '0.000125'} 
2023-06-04 19:28:50 [DEBUG] Epoch 41 - {'loss': '0.024361', 'mae': '0.120733', 'lr': '0.000063'} 
2023-06-04 19:28:53 [DEBUG] Epoch 42 - {'loss': '0.024366', 'mae': '0.120749', 'lr': '0.000063'} 
2023-06-04 19:28:56 [DEBUG] Epoch 43 - {'loss': '0.024352', 'mae': '0.120705', 'lr': '0.000063'} 
2023-06-04 19:28:58 [DEBUG] Epoch 44 - {'loss': '0.024347', 'mae': '0.120694', 'lr': '0.000031'} 
2023-06-04 19:29:01 [DEBUG] Epoch 45 - {'loss': '0.024348', 'mae': '0.120692', 'lr': '0.000031'} 
2023-06-04 19:29:04 [DEBUG] Epoch 46 - {'loss': '0.024351', 'mae': '0.120694', 'lr': '0.000031'} 
2023-06-04 19:29:06 [DEBUG] Epoch 47 - {'loss': '0.024339', 'mae': '0.120686', 'lr': '0.000016'} 
2023-06-04 19:29:09 [DEBUG] Epoch 48 - {'loss': '0.024342', 'mae': '0.120690', 'lr': '0.000016'} 
2023-06-04 19:29:12 [DEBUG] Epoch 49 - {'loss': '0.024342', 'mae': '0.120695', 'lr': '0.000016'} 
2023-06-04 19:29:14 [DEBUG] Epoch 50 - {'loss': '0.024343', 'mae': '0.120693', 'lr': '0.000016'} 
2023-06-04 19:29:17 [DEBUG] Epoch 51 - {'loss': '0.024334', 'mae': '0.120676', 'lr': '0.000008'} 
2023-06-04 19:29:20 [DEBUG] Epoch 52 - {'loss': '0.024340', 'mae': '0.120689', 'lr': '0.000008'} 
2023-06-04 19:29:22 [DEBUG] Epoch 53 - {'loss': '0.024341', 'mae': '0.120695', 'lr': '0.000008'} 
2023-06-04 19:29:25 [DEBUG] Epoch 54 - {'loss': '0.024331', 'mae': '0.120660', 'lr': '0.000004'} 
2023-06-04 19:29:28 [DEBUG] Epoch 55 - {'loss': '0.024346', 'mae': '0.120699', 'lr': '0.000004'} 
2023-06-04 19:29:30 [DEBUG] Epoch 56 - {'loss': '0.024338', 'mae': '0.120682', 'lr': '0.000004'} 
2023-06-04 19:29:33 [DEBUG] Epoch 57 - {'loss': '0.024332', 'mae': '0.120665', 'lr': '0.000002'} 
2023-06-04 19:29:36 [DEBUG] Epoch 58 - {'loss': '0.024340', 'mae': '0.120671', 'lr': '0.000002'} 
2023-06-04 19:29:38 [DEBUG] Epoch 59 - {'loss': '0.024338', 'mae': '0.120681', 'lr': '0.000002'} 
2023-06-04 19:29:41 [DEBUG] Epoch 60 - {'loss': '0.024335', 'mae': '0.120672', 'lr': '0.000001'} 
2023-06-04 19:29:44 [DEBUG] Epoch 61 - {'loss': '0.024331', 'mae': '0.120661', 'lr': '0.000001'} 
2023-06-04 19:29:46 [DEBUG] Epoch 62 - {'loss': '0.024330', 'mae': '0.120661', 'lr': '0.000001'} 
2023-06-04 19:29:49 [DEBUG] Epoch 63 - {'loss': '0.024331', 'mae': '0.120666', 'lr': '0.000000'} 
2023-06-04 19:29:52 [DEBUG] Epoch 64 - {'loss': '0.024326', 'mae': '0.120655', 'lr': '0.000000'} 
2023-06-04 19:29:54 [DEBUG] Epoch 65 - {'loss': '0.024332', 'mae': '0.120656', 'lr': '0.000000'} 
2023-06-04 19:29:57 [DEBUG] Epoch 66 - {'loss': '0.024334', 'mae': '0.120666', 'lr': '0.000000'} 
2023-06-04 19:30:00 [DEBUG] Epoch 67 - {'loss': '0.024334', 'mae': '0.120663', 'lr': '0.000000'} 
2023-06-04 19:30:02 [DEBUG] Epoch 68 - {'loss': '0.024337', 'mae': '0.120683', 'lr': '0.000000'} 
2023-06-04 19:30:05 [DEBUG] Epoch 69 - {'loss': '0.024333', 'mae': '0.120664', 'lr': '0.000000'} 
2023-06-04 19:30:08 [DEBUG] Epoch 70 - {'loss': '0.024336', 'mae': '0.120675', 'lr': '0.000000'} 
2023-06-04 19:30:10 [DEBUG] Epoch 71 - {'loss': '0.024343', 'mae': '0.120697', 'lr': '0.000000'} 
2023-06-04 19:30:13 [DEBUG] Epoch 72 - {'loss': '0.024331', 'mae': '0.120657', 'lr': '0.000000'} 
2023-06-04 19:30:15 [DEBUG] Epoch 73 - {'loss': '0.024338', 'mae': '0.120673', 'lr': '0.000000'} 
2023-06-04 19:30:18 [DEBUG] Epoch 74 - {'loss': '0.024334', 'mae': '0.120663', 'lr': '0.000000'} 
2023-06-04 19:30:21 [DEBUG] Epoch 75 - {'loss': '0.024340', 'mae': '0.120681', 'lr': '0.000000'} 
2023-06-04 19:30:23 [DEBUG] Epoch 76 - {'loss': '0.024330', 'mae': '0.120667', 'lr': '0.000000'} 
2023-06-04 19:30:26 [DEBUG] Epoch 77 - {'loss': '0.024331', 'mae': '0.120643', 'lr': '0.000000'} 
2023-06-04 19:30:29 [DEBUG] Epoch 78 - {'loss': '0.024333', 'mae': '0.120668', 'lr': '0.000000'} 
2023-06-04 19:30:31 [DEBUG] Epoch 79 - {'loss': '0.024342', 'mae': '0.120690', 'lr': '0.000000'} 
2023-06-04 19:30:31 [INFO] Training finished, elapsed time: 214.48 seconds
2023-06-04 19:30:31 [INFO] model is saved in 'models/best_epoch_WT_F_21_.h5'

2023-06-04 19:40:44 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 19:40:44
2023-06-04 19:40:44 [INFO] Model configurations:
2023-06-04 19:40:44 [INFO] name: WF_T_21
2023-06-04 19:40:44 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 19:40:44 [INFO] loss_function: mse
2023-06-04 19:40:44 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 19:40:44 [INFO] metric: mae
2023-06-04 19:40:44 [INFO] shuffle: True
2023-06-04 19:40:44 [INFO] epochs: 20
2023-06-04 19:40:44 [INFO] batch_size: 512
2023-06-04 19:40:44 [INFO] verbose: 1
2023-06-04 19:40:44 [INFO] TensorBoard_log_path: logs
2023-06-04 19:40:44 [INFO] TensorBoard_hist_freq: 1
2023-06-04 19:40:44 [INFO] EarlyStopping_monitor: loss
2023-06-04 19:40:44 [INFO] EarlyStopping_patience: 15
2023-06-04 19:40:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 19:40:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 19:40:44 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 19:40:44 [INFO] Checkpoint_monitor: loss
2023-06-04 19:40:44 [INFO] save_best_only: True
2023-06-04 19:40:44 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 19:40:44 [INFO] log_path: logs/training.log
2023-06-04 19:40:44 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 19:40:44 [INFO] Training in progress
2023-06-04 19:40:44 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.
2023-06-04 19:40:47 [DEBUG] Epoch 0 - {'loss': '0.113467', 'mae': '0.234448', 'lr': '0.001000'} 
2023-06-04 19:40:50 [DEBUG] Epoch 1 - {'loss': '0.042487', 'mae': '0.154389', 'lr': '0.001000'} 
2023-06-04 19:40:53 [DEBUG] Epoch 2 - {'loss': '0.033650', 'mae': '0.139508', 'lr': '0.001000'} 
2023-06-04 19:40:55 [DEBUG] Epoch 3 - {'loss': '0.030426', 'mae': '0.133482', 'lr': '0.001000'} 
2023-06-04 19:40:58 [DEBUG] Epoch 4 - {'loss': '0.028718', 'mae': '0.130032', 'lr': '0.001000'} 
2023-06-04 19:41:01 [DEBUG] Epoch 5 - {'loss': '0.027721', 'mae': '0.127989', 'lr': '0.001000'} 
2023-06-04 19:41:03 [DEBUG] Epoch 6 - {'loss': '0.027027', 'mae': '0.126625', 'lr': '0.001000'} 
2023-06-04 19:41:06 [DEBUG] Epoch 7 - {'loss': '0.026493', 'mae': '0.125511', 'lr': '0.001000'} 
2023-06-04 19:41:09 [DEBUG] Epoch 8 - {'loss': '0.026148', 'mae': '0.124723', 'lr': '0.001000'} 
2023-06-04 19:41:11 [DEBUG] Epoch 9 - {'loss': '0.025867', 'mae': '0.124088', 'lr': '0.001000'} 
2023-06-04 19:41:14 [DEBUG] Epoch 10 - {'loss': '0.025607', 'mae': '0.123489', 'lr': '0.001000'} 
2023-06-04 19:41:17 [DEBUG] Epoch 11 - {'loss': '0.025433', 'mae': '0.123092', 'lr': '0.001000'} 
2023-06-04 19:41:19 [DEBUG] Epoch 12 - {'loss': '0.025290', 'mae': '0.122789', 'lr': '0.001000'} 
2023-06-04 19:41:22 [DEBUG] Epoch 13 - {'loss': '0.025156', 'mae': '0.122491', 'lr': '0.001000'} 
2023-06-04 19:41:25 [DEBUG] Epoch 14 - {'loss': '0.025052', 'mae': '0.122244', 'lr': '0.001000'} 
2023-06-04 19:41:27 [DEBUG] Epoch 15 - {'loss': '0.024958', 'mae': '0.122032', 'lr': '0.001000'} 
2023-06-04 19:41:30 [DEBUG] Epoch 16 - {'loss': '0.024916', 'mae': '0.121954', 'lr': '0.001000'} 
2023-06-04 19:41:33 [DEBUG] Epoch 17 - {'loss': '0.024838', 'mae': '0.121766', 'lr': '0.001000'} 
2023-06-04 19:41:35 [DEBUG] Epoch 18 - {'loss': '0.024800', 'mae': '0.121693', 'lr': '0.001000'} 
2023-06-04 19:41:38 [DEBUG] Epoch 19 - {'loss': '0.024741', 'mae': '0.121554', 'lr': '0.001000'} 
2023-06-04 19:41:38 [INFO] Training finished, elapsed time: 54.38 seconds
2023-06-04 19:41:38 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 19:44:27 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 19:44:27
2023-06-04 19:44:27 [INFO] Model configurations:
2023-06-04 19:44:27 [INFO] name: WF_T_21
2023-06-04 19:44:27 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 19:44:27 [INFO] loss_function: mse
2023-06-04 19:44:27 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 19:44:27 [INFO] metric: mae
2023-06-04 19:44:27 [INFO] shuffle: True
2023-06-04 19:44:27 [INFO] epochs: 200
2023-06-04 19:44:27 [INFO] batch_size: 512
2023-06-04 19:44:27 [INFO] verbose: 1
2023-06-04 19:44:27 [INFO] TensorBoard_log_path: logs
2023-06-04 19:44:27 [INFO] TensorBoard_hist_freq: 1
2023-06-04 19:44:27 [INFO] EarlyStopping_monitor: loss
2023-06-04 19:44:27 [INFO] EarlyStopping_patience: 15
2023-06-04 19:44:27 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 19:44:27 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 19:44:27 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 19:44:27 [INFO] Checkpoint_monitor: loss
2023-06-04 19:44:27 [INFO] save_best_only: True
2023-06-04 19:44:27 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 19:44:27 [INFO] log_path: logs/training.log
2023-06-04 19:44:27 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 19:44:27 [INFO] Training in progress
2023-06-04 19:44:28 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 19:44:31 [DEBUG] Epoch 0 - {'loss': '0.118284', 'mae': '0.240063', 'lr': '0.001000'} 
2023-06-04 19:44:34 [DEBUG] Epoch 1 - {'loss': '0.045615', 'mae': '0.159615', 'lr': '0.001000'} 
2023-06-04 19:44:36 [DEBUG] Epoch 2 - {'loss': '0.035142', 'mae': '0.142359', 'lr': '0.001000'} 
2023-06-04 19:44:39 [DEBUG] Epoch 3 - {'loss': '0.031035', 'mae': '0.134907', 'lr': '0.001000'} 
2023-06-04 19:44:42 [DEBUG] Epoch 4 - {'loss': '0.029069', 'mae': '0.130955', 'lr': '0.001000'} 
2023-06-04 19:44:44 [DEBUG] Epoch 5 - {'loss': '0.027916', 'mae': '0.128571', 'lr': '0.001000'} 
2023-06-04 19:44:47 [DEBUG] Epoch 6 - {'loss': '0.027259', 'mae': '0.127258', 'lr': '0.001000'} 
2023-06-04 19:44:50 [DEBUG] Epoch 7 - {'loss': '0.026758', 'mae': '0.126193', 'lr': '0.001000'} 
2023-06-04 19:44:52 [DEBUG] Epoch 8 - {'loss': '0.026454', 'mae': '0.125526', 'lr': '0.001000'} 
2023-06-04 19:44:55 [DEBUG] Epoch 9 - {'loss': '0.026225', 'mae': '0.125006', 'lr': '0.001000'} 
2023-06-04 19:44:58 [DEBUG] Epoch 10 - {'loss': '0.025964', 'mae': '0.124381', 'lr': '0.001000'} 
2023-06-04 19:45:00 [DEBUG] Epoch 11 - {'loss': '0.025785', 'mae': '0.123962', 'lr': '0.001000'} 
2023-06-04 19:45:03 [DEBUG] Epoch 12 - {'loss': '0.025635', 'mae': '0.123600', 'lr': '0.001000'} 
2023-06-04 19:45:06 [DEBUG] Epoch 13 - {'loss': '0.025554', 'mae': '0.123431', 'lr': '0.001000'} 
2023-06-04 19:45:08 [DEBUG] Epoch 14 - {'loss': '0.025427', 'mae': '0.123125', 'lr': '0.001000'} 
2023-06-04 19:45:11 [DEBUG] Epoch 15 - {'loss': '0.025336', 'mae': '0.122887', 'lr': '0.001000'} 
2023-06-04 19:45:14 [DEBUG] Epoch 16 - {'loss': '0.025230', 'mae': '0.122647', 'lr': '0.001000'} 
2023-06-04 19:45:16 [DEBUG] Epoch 17 - {'loss': '0.025130', 'mae': '0.122404', 'lr': '0.001000'} 
2023-06-04 19:45:19 [DEBUG] Epoch 18 - {'loss': '0.025104', 'mae': '0.122366', 'lr': '0.001000'} 
2023-06-04 19:45:22 [DEBUG] Epoch 19 - {'loss': '0.025019', 'mae': '0.122159', 'lr': '0.001000'} 
2023-06-04 19:45:25 [DEBUG] Epoch 20 - {'loss': '0.024976', 'mae': '0.122063', 'lr': '0.001000'} 
2023-06-04 19:45:27 [DEBUG] Epoch 21 - {'loss': '0.024923', 'mae': '0.121951', 'lr': '0.001000'} 
2023-06-04 19:45:30 [DEBUG] Epoch 22 - {'loss': '0.024881', 'mae': '0.121859', 'lr': '0.001000'} 
2023-06-04 19:45:33 [DEBUG] Epoch 23 - {'loss': '0.024854', 'mae': '0.121813', 'lr': '0.001000'} 
2023-06-04 19:45:35 [DEBUG] Epoch 24 - {'loss': '0.024811', 'mae': '0.121710', 'lr': '0.001000'} 
2023-06-04 19:45:38 [DEBUG] Epoch 25 - {'loss': '0.024768', 'mae': '0.121618', 'lr': '0.001000'} 
2023-06-04 19:45:41 [DEBUG] Epoch 26 - {'loss': '0.024744', 'mae': '0.121574', 'lr': '0.001000'} 
2023-06-04 19:45:43 [DEBUG] Epoch 27 - {'loss': '0.024716', 'mae': '0.121512', 'lr': '0.001000'} 
2023-06-04 19:45:46 [DEBUG] Epoch 28 - {'loss': '0.024688', 'mae': '0.121465', 'lr': '0.001000'} 
2023-06-04 19:45:49 [DEBUG] Epoch 29 - {'loss': '0.024640', 'mae': '0.121341', 'lr': '0.001000'} 
2023-06-04 19:45:51 [DEBUG] Epoch 30 - {'loss': '0.024645', 'mae': '0.121381', 'lr': '0.001000'} 
2023-06-04 19:45:54 [DEBUG] Epoch 31 - {'loss': '0.024603', 'mae': '0.121301', 'lr': '0.001000'} 
2023-06-04 19:45:57 [DEBUG] Epoch 32 - {'loss': '0.024561', 'mae': '0.121173', 'lr': '0.001000'} 
2023-06-04 19:45:59 [DEBUG] Epoch 33 - {'loss': '0.024480', 'mae': '0.120972', 'lr': '0.000500'} 
2023-06-04 19:46:02 [DEBUG] Epoch 34 - {'loss': '0.024468', 'mae': '0.120958', 'lr': '0.000500'} 
2023-06-04 19:46:05 [DEBUG] Epoch 35 - {'loss': '0.024450', 'mae': '0.120908', 'lr': '0.000500'} 
2023-06-04 19:46:08 [DEBUG] Epoch 36 - {'loss': '0.024444', 'mae': '0.120910', 'lr': '0.000500'} 
2023-06-04 19:46:10 [DEBUG] Epoch 37 - {'loss': '0.024399', 'mae': '0.120800', 'lr': '0.000250'} 
2023-06-04 19:46:13 [DEBUG] Epoch 38 - {'loss': '0.024397', 'mae': '0.120806', 'lr': '0.000250'} 
2023-06-04 19:46:16 [DEBUG] Epoch 39 - {'loss': '0.024369', 'mae': '0.120742', 'lr': '0.000250'} 
2023-06-04 19:46:18 [DEBUG] Epoch 40 - {'loss': '0.024380', 'mae': '0.120769', 'lr': '0.000250'} 
2023-06-04 19:46:21 [DEBUG] Epoch 41 - {'loss': '0.024360', 'mae': '0.120718', 'lr': '0.000250'} 
2023-06-04 19:46:24 [DEBUG] Epoch 42 - {'loss': '0.024350', 'mae': '0.120707', 'lr': '0.000250'} 
2023-06-04 19:46:26 [DEBUG] Epoch 43 - {'loss': '0.024337', 'mae': '0.120673', 'lr': '0.000125'} 
2023-06-04 19:46:29 [DEBUG] Epoch 44 - {'loss': '0.024337', 'mae': '0.120679', 'lr': '0.000125'} 
2023-06-04 19:46:32 [DEBUG] Epoch 45 - {'loss': '0.024324', 'mae': '0.120657', 'lr': '0.000125'} 
2023-06-04 19:46:34 [DEBUG] Epoch 46 - {'loss': '0.024332', 'mae': '0.120678', 'lr': '0.000063'} 
2023-06-04 19:46:37 [DEBUG] Epoch 47 - {'loss': '0.024319', 'mae': '0.120635', 'lr': '0.000063'} 
2023-06-04 19:46:40 [DEBUG] Epoch 48 - {'loss': '0.024304', 'mae': '0.120606', 'lr': '0.000063'} 
2023-06-04 19:46:42 [DEBUG] Epoch 49 - {'loss': '0.024308', 'mae': '0.120615', 'lr': '0.000031'} 
2023-06-04 19:46:45 [DEBUG] Epoch 50 - {'loss': '0.024297', 'mae': '0.120595', 'lr': '0.000031'} 
2023-06-04 19:46:48 [DEBUG] Epoch 51 - {'loss': '0.024305', 'mae': '0.120598', 'lr': '0.000031'} 
2023-06-04 19:46:50 [DEBUG] Epoch 52 - {'loss': '0.024298', 'mae': '0.120595', 'lr': '0.000016'} 
2023-06-04 19:46:53 [DEBUG] Epoch 53 - {'loss': '0.024296', 'mae': '0.120578', 'lr': '0.000016'} 
2023-06-04 19:46:56 [DEBUG] Epoch 54 - {'loss': '0.024284', 'mae': '0.120563', 'lr': '0.000016'} 
2023-06-04 19:46:58 [DEBUG] Epoch 55 - {'loss': '0.024299', 'mae': '0.120609', 'lr': '0.000008'} 
2023-06-04 19:47:01 [DEBUG] Epoch 56 - {'loss': '0.024292', 'mae': '0.120580', 'lr': '0.000008'} 
2023-06-04 19:47:04 [DEBUG] Epoch 57 - {'loss': '0.024292', 'mae': '0.120584', 'lr': '0.000008'} 
2023-06-04 19:47:06 [DEBUG] Epoch 58 - {'loss': '0.024292', 'mae': '0.120586', 'lr': '0.000004'} 
2023-06-04 19:47:09 [DEBUG] Epoch 59 - {'loss': '0.024292', 'mae': '0.120573', 'lr': '0.000004'} 
2023-06-04 19:47:12 [DEBUG] Epoch 60 - {'loss': '0.024284', 'mae': '0.120556', 'lr': '0.000004'} 
2023-06-04 19:47:14 [DEBUG] Epoch 61 - {'loss': '0.024295', 'mae': '0.120586', 'lr': '0.000002'} 
2023-06-04 19:47:17 [DEBUG] Epoch 62 - {'loss': '0.024291', 'mae': '0.120585', 'lr': '0.000002'} 
2023-06-04 19:47:20 [DEBUG] Epoch 63 - {'loss': '0.024283', 'mae': '0.120555', 'lr': '0.000002'} 
2023-06-04 19:47:22 [DEBUG] Epoch 64 - {'loss': '0.024284', 'mae': '0.120571', 'lr': '0.000001'} 
2023-06-04 19:47:25 [DEBUG] Epoch 65 - {'loss': '0.024290', 'mae': '0.120581', 'lr': '0.000001'} 
2023-06-04 19:47:28 [DEBUG] Epoch 66 - {'loss': '0.024287', 'mae': '0.120574', 'lr': '0.000001'} 
2023-06-04 19:47:30 [DEBUG] Epoch 67 - {'loss': '0.024289', 'mae': '0.120559', 'lr': '0.000000'} 
2023-06-04 19:47:33 [DEBUG] Epoch 68 - {'loss': '0.024296', 'mae': '0.120591', 'lr': '0.000000'} 
2023-06-04 19:47:36 [DEBUG] Epoch 69 - {'loss': '0.024298', 'mae': '0.120610', 'lr': '0.000000'} 
2023-06-04 19:47:38 [DEBUG] Epoch 70 - {'loss': '0.024284', 'mae': '0.120557', 'lr': '0.000000'} 
2023-06-04 19:47:41 [DEBUG] Epoch 71 - {'loss': '0.024287', 'mae': '0.120583', 'lr': '0.000000'} 
2023-06-04 19:47:44 [DEBUG] Epoch 72 - {'loss': '0.024291', 'mae': '0.120582', 'lr': '0.000000'} 
2023-06-04 19:47:46 [DEBUG] Epoch 73 - {'loss': '0.024290', 'mae': '0.120581', 'lr': '0.000000'} 
2023-06-04 19:47:49 [DEBUG] Epoch 74 - {'loss': '0.024293', 'mae': '0.120587', 'lr': '0.000000'} 
2023-06-04 19:47:52 [DEBUG] Epoch 75 - {'loss': '0.024287', 'mae': '0.120571', 'lr': '0.000000'} 
2023-06-04 19:47:54 [DEBUG] Epoch 76 - {'loss': '0.024286', 'mae': '0.120566', 'lr': '0.000000'} 
2023-06-04 19:47:57 [DEBUG] Epoch 77 - {'loss': '0.024289', 'mae': '0.120584', 'lr': '0.000000'} 
2023-06-04 19:48:00 [DEBUG] Epoch 78 - {'loss': '0.024285', 'mae': '0.120564', 'lr': '0.000000'} 
2023-06-04 19:48:00 [INFO] Training finished, elapsed time: 212.39 seconds
2023-06-04 19:48:00 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 19:52:41 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 19:52:41
2023-06-04 19:52:41 [INFO] Model configurations:
2023-06-04 19:52:41 [INFO] name: WF_T_21
2023-06-04 19:52:41 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 19:52:41 [INFO] loss_function: mse
2023-06-04 19:52:41 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 19:52:41 [INFO] metric: mae
2023-06-04 19:52:41 [INFO] shuffle: True
2023-06-04 19:52:41 [INFO] epochs: 200
2023-06-04 19:52:41 [INFO] batch_size: 512
2023-06-04 19:52:41 [INFO] verbose: 1
2023-06-04 19:52:41 [INFO] TensorBoard_log_path: logs
2023-06-04 19:52:41 [INFO] TensorBoard_hist_freq: 1
2023-06-04 19:52:41 [INFO] EarlyStopping_monitor: loss
2023-06-04 19:52:41 [INFO] EarlyStopping_patience: 15
2023-06-04 19:52:41 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 19:52:41 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 19:52:41 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 19:52:41 [INFO] Checkpoint_monitor: loss
2023-06-04 19:52:41 [INFO] save_best_only: True
2023-06-04 19:52:41 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 19:52:41 [INFO] log_path: logs/training.log
2023-06-04 19:52:41 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 19:52:41 [INFO] Training in progress
2023-06-04 19:52:42 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 19:52:45 [DEBUG] Epoch 0 - {'loss': '0.207727', 'mae': '0.317074', 'lr': '0.001000'} 
2023-06-04 19:52:48 [DEBUG] Epoch 1 - {'loss': '0.063236', 'mae': '0.186683', 'lr': '0.001000'} 
2023-06-04 19:52:50 [DEBUG] Epoch 2 - {'loss': '0.041790', 'mae': '0.152976', 'lr': '0.001000'} 
2023-06-04 19:52:53 [DEBUG] Epoch 3 - {'loss': '0.034301', 'mae': '0.140341', 'lr': '0.001000'} 
2023-06-04 19:52:56 [DEBUG] Epoch 4 - {'loss': '0.031175', 'mae': '0.134719', 'lr': '0.001000'} 
2023-06-04 19:52:58 [DEBUG] Epoch 5 - {'loss': '0.029199', 'mae': '0.130963', 'lr': '0.001000'} 
2023-06-04 19:53:01 [DEBUG] Epoch 6 - {'loss': '0.027981', 'mae': '0.128578', 'lr': '0.001000'} 
2023-06-04 19:53:04 [DEBUG] Epoch 7 - {'loss': '0.027305', 'mae': '0.127253', 'lr': '0.001000'} 
2023-06-04 19:53:07 [DEBUG] Epoch 8 - {'loss': '0.026884', 'mae': '0.126378', 'lr': '0.001000'} 
2023-06-04 19:53:09 [DEBUG] Epoch 9 - {'loss': '0.026554', 'mae': '0.125663', 'lr': '0.001000'} 
2023-06-04 19:53:12 [DEBUG] Epoch 10 - {'loss': '0.026294', 'mae': '0.125086', 'lr': '0.001000'} 
2023-06-04 19:53:15 [DEBUG] Epoch 11 - {'loss': '0.026087', 'mae': '0.124638', 'lr': '0.001000'} 
2023-06-04 19:53:17 [DEBUG] Epoch 12 - {'loss': '0.025864', 'mae': '0.124136', 'lr': '0.001000'} 
2023-06-04 19:53:20 [DEBUG] Epoch 13 - {'loss': '0.025701', 'mae': '0.123760', 'lr': '0.001000'} 
2023-06-04 19:53:23 [DEBUG] Epoch 14 - {'loss': '0.025552', 'mae': '0.123426', 'lr': '0.001000'} 
2023-06-04 19:53:26 [DEBUG] Epoch 15 - {'loss': '0.025414', 'mae': '0.123104', 'lr': '0.001000'} 
2023-06-04 19:53:28 [DEBUG] Epoch 16 - {'loss': '0.025308', 'mae': '0.122867', 'lr': '0.001000'} 
2023-06-04 19:53:31 [DEBUG] Epoch 17 - {'loss': '0.025201', 'mae': '0.122601', 'lr': '0.001000'} 
2023-06-04 19:53:34 [DEBUG] Epoch 18 - {'loss': '0.025159', 'mae': '0.122508', 'lr': '0.001000'} 
2023-06-04 19:53:36 [DEBUG] Epoch 19 - {'loss': '0.025071', 'mae': '0.122303', 'lr': '0.001000'} 
2023-06-04 19:53:39 [DEBUG] Epoch 20 - {'loss': '0.025050', 'mae': '0.122265', 'lr': '0.001000'} 
2023-06-04 19:53:42 [DEBUG] Epoch 21 - {'loss': '0.024974', 'mae': '0.122094', 'lr': '0.001000'} 
2023-06-04 19:53:45 [DEBUG] Epoch 22 - {'loss': '0.024940', 'mae': '0.122021', 'lr': '0.001000'} 
2023-06-04 19:53:47 [DEBUG] Epoch 23 - {'loss': '0.024895', 'mae': '0.121920', 'lr': '0.001000'} 
2023-06-04 19:53:50 [DEBUG] Epoch 24 - {'loss': '0.024830', 'mae': '0.121771', 'lr': '0.001000'} 
2023-06-04 19:53:53 [DEBUG] Epoch 25 - {'loss': '0.024813', 'mae': '0.121736', 'lr': '0.001000'} 
2023-06-04 19:53:55 [DEBUG] Epoch 26 - {'loss': '0.024752', 'mae': '0.121598', 'lr': '0.001000'} 
2023-06-04 19:53:58 [DEBUG] Epoch 27 - {'loss': '0.024741', 'mae': '0.121593', 'lr': '0.001000'} 
2023-06-04 19:54:01 [DEBUG] Epoch 28 - {'loss': '0.024632', 'mae': '0.121295', 'lr': '0.000500'} 
2023-06-04 19:54:04 [DEBUG] Epoch 29 - {'loss': '0.024615', 'mae': '0.121266', 'lr': '0.000500'} 
2023-06-04 19:54:06 [DEBUG] Epoch 30 - {'loss': '0.024591', 'mae': '0.121214', 'lr': '0.000500'} 
2023-06-04 19:54:09 [DEBUG] Epoch 31 - {'loss': '0.024569', 'mae': '0.121168', 'lr': '0.000500'} 
2023-06-04 19:54:12 [DEBUG] Epoch 32 - {'loss': '0.024539', 'mae': '0.121094', 'lr': '0.000250'} 
2023-06-04 19:54:14 [DEBUG] Epoch 33 - {'loss': '0.024525', 'mae': '0.121071', 'lr': '0.000250'} 
2023-06-04 19:54:17 [DEBUG] Epoch 34 - {'loss': '0.024512', 'mae': '0.121035', 'lr': '0.000250'} 
2023-06-04 19:54:20 [DEBUG] Epoch 35 - {'loss': '0.024497', 'mae': '0.121012', 'lr': '0.000250'} 
2023-06-04 19:54:23 [DEBUG] Epoch 36 - {'loss': '0.024490', 'mae': '0.121008', 'lr': '0.000250'} 
2023-06-04 19:54:25 [DEBUG] Epoch 37 - {'loss': '0.024470', 'mae': '0.120962', 'lr': '0.000125'} 
2023-06-04 19:54:28 [DEBUG] Epoch 38 - {'loss': '0.024459', 'mae': '0.120922', 'lr': '0.000125'} 
2023-06-04 19:54:31 [DEBUG] Epoch 39 - {'loss': '0.024445', 'mae': '0.120894', 'lr': '0.000125'} 
2023-06-04 19:54:33 [DEBUG] Epoch 40 - {'loss': '0.024437', 'mae': '0.120882', 'lr': '0.000063'} 
2023-06-04 19:54:36 [DEBUG] Epoch 41 - {'loss': '0.024442', 'mae': '0.120886', 'lr': '0.000063'} 
2023-06-04 19:54:39 [DEBUG] Epoch 42 - {'loss': '0.024435', 'mae': '0.120856', 'lr': '0.000063'} 
2023-06-04 19:54:42 [DEBUG] Epoch 43 - {'loss': '0.024428', 'mae': '0.120849', 'lr': '0.000031'} 
2023-06-04 19:54:44 [DEBUG] Epoch 44 - {'loss': '0.024432', 'mae': '0.120865', 'lr': '0.000031'} 
2023-06-04 19:54:47 [DEBUG] Epoch 45 - {'loss': '0.024426', 'mae': '0.120855', 'lr': '0.000031'} 
2023-06-04 19:54:50 [DEBUG] Epoch 46 - {'loss': '0.024421', 'mae': '0.120835', 'lr': '0.000016'} 
2023-06-04 19:54:52 [DEBUG] Epoch 47 - {'loss': '0.024413', 'mae': '0.120827', 'lr': '0.000016'} 
2023-06-04 19:54:55 [DEBUG] Epoch 48 - {'loss': '0.024416', 'mae': '0.120838', 'lr': '0.000016'} 
2023-06-04 19:54:58 [DEBUG] Epoch 49 - {'loss': '0.024417', 'mae': '0.120850', 'lr': '0.000016'} 
2023-06-04 19:55:01 [DEBUG] Epoch 50 - {'loss': '0.024421', 'mae': '0.120847', 'lr': '0.000008'} 
2023-06-04 19:55:03 [DEBUG] Epoch 51 - {'loss': '0.024405', 'mae': '0.120802', 'lr': '0.000008'} 
2023-06-04 19:55:06 [DEBUG] Epoch 52 - {'loss': '0.024416', 'mae': '0.120830', 'lr': '0.000008'} 
2023-06-04 19:55:09 [DEBUG] Epoch 53 - {'loss': '0.024408', 'mae': '0.120804', 'lr': '0.000004'} 
2023-06-04 19:55:11 [DEBUG] Epoch 54 - {'loss': '0.024403', 'mae': '0.120800', 'lr': '0.000004'} 
2023-06-04 19:55:14 [DEBUG] Epoch 55 - {'loss': '0.024407', 'mae': '0.120810', 'lr': '0.000004'} 
2023-06-04 19:55:17 [DEBUG] Epoch 56 - {'loss': '0.024413', 'mae': '0.120820', 'lr': '0.000002'} 
2023-06-04 19:55:19 [DEBUG] Epoch 57 - {'loss': '0.024406', 'mae': '0.120811', 'lr': '0.000002'} 
2023-06-04 19:55:22 [DEBUG] Epoch 58 - {'loss': '0.024406', 'mae': '0.120818', 'lr': '0.000002'} 
2023-06-04 19:55:25 [DEBUG] Epoch 59 - {'loss': '0.024398', 'mae': '0.120787', 'lr': '0.000001'} 
2023-06-04 19:55:28 [DEBUG] Epoch 60 - {'loss': '0.024409', 'mae': '0.120820', 'lr': '0.000001'} 
2023-06-04 19:55:30 [DEBUG] Epoch 61 - {'loss': '0.024412', 'mae': '0.120816', 'lr': '0.000001'} 
2023-06-04 19:55:33 [DEBUG] Epoch 62 - {'loss': '0.024404', 'mae': '0.120802', 'lr': '0.000000'} 
2023-06-04 19:55:36 [DEBUG] Epoch 63 - {'loss': '0.024401', 'mae': '0.120797', 'lr': '0.000000'} 
2023-06-04 19:55:38 [DEBUG] Epoch 64 - {'loss': '0.024403', 'mae': '0.120808', 'lr': '0.000000'} 
2023-06-04 19:55:41 [DEBUG] Epoch 65 - {'loss': '0.024408', 'mae': '0.120809', 'lr': '0.000000'} 
2023-06-04 19:55:44 [DEBUG] Epoch 66 - {'loss': '0.024399', 'mae': '0.120787', 'lr': '0.000000'} 
2023-06-04 19:55:46 [DEBUG] Epoch 67 - {'loss': '0.024402', 'mae': '0.120801', 'lr': '0.000000'} 
2023-06-04 19:55:49 [DEBUG] Epoch 68 - {'loss': '0.024410', 'mae': '0.120824', 'lr': '0.000000'} 
2023-06-04 19:55:52 [DEBUG] Epoch 69 - {'loss': '0.024410', 'mae': '0.120825', 'lr': '0.000000'} 
2023-06-04 19:55:55 [DEBUG] Epoch 70 - {'loss': '0.024401', 'mae': '0.120794', 'lr': '0.000000'} 
2023-06-04 19:55:57 [DEBUG] Epoch 71 - {'loss': '0.024410', 'mae': '0.120811', 'lr': '0.000000'} 
2023-06-04 19:56:00 [DEBUG] Epoch 72 - {'loss': '0.024409', 'mae': '0.120820', 'lr': '0.000000'} 
2023-06-04 19:56:03 [DEBUG] Epoch 73 - {'loss': '0.024415', 'mae': '0.120824', 'lr': '0.000000'} 
2023-06-04 19:56:05 [DEBUG] Epoch 74 - {'loss': '0.024407', 'mae': '0.120807', 'lr': '0.000000'} 
2023-06-04 19:56:05 [INFO] Training finished, elapsed time: 203.94 seconds
2023-06-04 19:56:05 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:14:48 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:14:48
2023-06-04 20:14:48 [INFO] Model configurations:
2023-06-04 20:14:48 [INFO] name: WF_T_21
2023-06-04 20:14:48 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:14:48 [INFO] loss_function: mse
2023-06-04 20:14:48 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:14:48 [INFO] metric: mae
2023-06-04 20:14:48 [INFO] shuffle: True
2023-06-04 20:14:48 [INFO] epochs: 2
2023-06-04 20:14:48 [INFO] batch_size: 512
2023-06-04 20:14:48 [INFO] verbose: 1
2023-06-04 20:14:48 [INFO] TensorBoard_log_path: logs
2023-06-04 20:14:48 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:14:48 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:14:48 [INFO] EarlyStopping_patience: 15
2023-06-04 20:14:48 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:14:48 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:14:48 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:14:48 [INFO] Checkpoint_monitor: loss
2023-06-04 20:14:48 [INFO] save_best_only: True
2023-06-04 20:14:48 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:14:48 [INFO] log_path: logs/training.log
2023-06-04 20:14:48 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:14:48 [INFO] Training in progress
2023-06-04 20:14:49 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:14:52 [DEBUG] Epoch 0 - {'loss': '0.103430', 'mae': '0.224206', 'lr': '0.001000'} 
2023-06-04 20:14:54 [DEBUG] Epoch 1 - {'loss': '0.039776', 'mae': '0.149938', 'lr': '0.001000'} 
2023-06-04 20:14:54 [INFO] Training finished, elapsed time: 6.10 seconds
2023-06-04 20:14:54 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:15:52 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:15:52
2023-06-04 20:15:52 [INFO] Model configurations:
2023-06-04 20:15:52 [INFO] name: WF_T_21
2023-06-04 20:15:52 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:15:52 [INFO] loss_function: mse
2023-06-04 20:15:52 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:15:52 [INFO] metric: mae
2023-06-04 20:15:52 [INFO] shuffle: True
2023-06-04 20:15:52 [INFO] epochs: 2
2023-06-04 20:15:52 [INFO] batch_size: 512
2023-06-04 20:15:52 [INFO] verbose: 1
2023-06-04 20:15:52 [INFO] TensorBoard_log_path: logs
2023-06-04 20:15:52 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:15:52 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:15:52 [INFO] EarlyStopping_patience: 15
2023-06-04 20:15:52 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:15:52 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:15:52 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:15:52 [INFO] Checkpoint_monitor: loss
2023-06-04 20:15:52 [INFO] save_best_only: True
2023-06-04 20:15:52 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:15:52 [INFO] log_path: logs/training.log
2023-06-04 20:15:52 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:15:52 [INFO] Training in progress
2023-06-04 20:15:53 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:15:56 [DEBUG] Epoch 0 - {'loss': '0.156641', 'mae': '0.269797', 'lr': '0.001000'} 
2023-06-04 20:15:58 [DEBUG] Epoch 1 - {'loss': '0.053899', 'mae': '0.170114', 'lr': '0.001000'} 
2023-06-04 20:15:58 [INFO] Training finished, elapsed time: 6.09 seconds
2023-06-04 20:15:58 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:17:26 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:17:26
2023-06-04 20:17:26 [INFO] Model configurations:
2023-06-04 20:17:26 [INFO] name: WF_T_21
2023-06-04 20:17:26 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:17:26 [INFO] loss_function: mse
2023-06-04 20:17:26 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:17:26 [INFO] metric: mae
2023-06-04 20:17:26 [INFO] shuffle: True
2023-06-04 20:17:26 [INFO] epochs: 2
2023-06-04 20:17:26 [INFO] batch_size: 512
2023-06-04 20:17:26 [INFO] verbose: 1
2023-06-04 20:17:26 [INFO] TensorBoard_log_path: logs
2023-06-04 20:17:26 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:17:26 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:17:26 [INFO] EarlyStopping_patience: 15
2023-06-04 20:17:26 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:17:26 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:17:26 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:17:26 [INFO] Checkpoint_monitor: loss
2023-06-04 20:17:26 [INFO] save_best_only: True
2023-06-04 20:17:26 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:17:26 [INFO] log_path: logs/training.log
2023-06-04 20:17:26 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:17:26 [INFO] Training in progress
2023-06-04 20:17:27 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:17:29 [DEBUG] Epoch 0 - {'loss': '0.088046', 'mae': '0.208287', 'lr': '0.001000'} 
2023-06-04 20:17:32 [DEBUG] Epoch 1 - {'loss': '0.039405', 'mae': '0.148807', 'lr': '0.001000'} 
2023-06-04 20:17:32 [INFO] Training finished, elapsed time: 6.11 seconds
2023-06-04 20:17:32 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:24:48 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:24:48
2023-06-04 20:24:48 [INFO] Model configurations:
2023-06-04 20:24:48 [INFO] name: WF_T_21
2023-06-04 20:24:48 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:24:48 [INFO] loss_function: mse
2023-06-04 20:24:48 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:24:48 [INFO] metric: mae
2023-06-04 20:24:48 [INFO] shuffle: True
2023-06-04 20:24:48 [INFO] epochs: 5
2023-06-04 20:24:48 [INFO] batch_size: 512
2023-06-04 20:24:48 [INFO] verbose: 1
2023-06-04 20:24:48 [INFO] TensorBoard_log_path: logs
2023-06-04 20:24:48 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:24:48 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:24:48 [INFO] EarlyStopping_patience: 15
2023-06-04 20:24:48 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:24:48 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:24:48 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:24:48 [INFO] Checkpoint_monitor: loss
2023-06-04 20:24:48 [INFO] save_best_only: True
2023-06-04 20:24:48 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:24:48 [INFO] log_path: logs/training.log
2023-06-04 20:24:48 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:24:48 [INFO] Training in progress
2023-06-04 20:24:49 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 20:24:52 [DEBUG] Epoch 0 - {'loss': '0.166135', 'mae': '0.280790', 'lr': '0.001000'} 
2023-06-04 20:24:55 [DEBUG] Epoch 1 - {'loss': '0.054375', 'mae': '0.175039', 'lr': '0.001000'} 
2023-06-04 20:24:57 [DEBUG] Epoch 2 - {'loss': '0.036469', 'mae': '0.145638', 'lr': '0.001000'} 
2023-06-04 20:25:00 [DEBUG] Epoch 3 - {'loss': '0.031614', 'mae': '0.136133', 'lr': '0.001000'} 
2023-06-04 20:25:03 [DEBUG] Epoch 4 - {'loss': '0.029418', 'mae': '0.131594', 'lr': '0.001000'} 
2023-06-04 20:25:03 [INFO] Training finished, elapsed time: 14.24 seconds
2023-06-04 20:25:03 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:27:14 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:27:14
2023-06-04 20:27:14 [INFO] Model configurations:
2023-06-04 20:27:14 [INFO] name: WF_T_21
2023-06-04 20:27:14 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:27:14 [INFO] loss_function: mse
2023-06-04 20:27:14 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:27:14 [INFO] metric: mae
2023-06-04 20:27:14 [INFO] shuffle: True
2023-06-04 20:27:14 [INFO] epochs: 5
2023-06-04 20:27:14 [INFO] batch_size: 512
2023-06-04 20:27:14 [INFO] verbose: 1
2023-06-04 20:27:14 [INFO] TensorBoard_log_path: logs
2023-06-04 20:27:14 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:27:14 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:27:14 [INFO] EarlyStopping_patience: 15
2023-06-04 20:27:14 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:27:14 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:27:14 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:27:14 [INFO] Checkpoint_monitor: loss
2023-06-04 20:27:14 [INFO] save_best_only: True
2023-06-04 20:27:14 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:27:14 [INFO] log_path: logs/training.log
2023-06-04 20:27:14 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:27:14 [INFO] Training in progress
2023-06-04 20:27:14 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:27:17 [DEBUG] Epoch 0 - {'loss': '0.150846', 'mae': '0.269250', 'lr': '0.001000'} 
2023-06-04 20:27:20 [DEBUG] Epoch 1 - {'loss': '0.048054', 'mae': '0.163970', 'lr': '0.001000'} 
2023-06-04 20:27:23 [DEBUG] Epoch 2 - {'loss': '0.035388', 'mae': '0.142783', 'lr': '0.001000'} 
2023-06-04 20:27:25 [DEBUG] Epoch 3 - {'loss': '0.031440', 'mae': '0.135509', 'lr': '0.001000'} 
2023-06-04 20:27:28 [DEBUG] Epoch 4 - {'loss': '0.029393', 'mae': '0.131438', 'lr': '0.001000'} 
2023-06-04 20:27:28 [INFO] Training finished, elapsed time: 14.26 seconds
2023-06-04 20:27:28 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:28:06 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:28:06
2023-06-04 20:28:06 [INFO] Model configurations:
2023-06-04 20:28:06 [INFO] name: WF_T_21
2023-06-04 20:28:06 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:28:06 [INFO] loss_function: mse
2023-06-04 20:28:06 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:28:06 [INFO] metric: mae
2023-06-04 20:28:06 [INFO] shuffle: True
2023-06-04 20:28:06 [INFO] epochs: 5
2023-06-04 20:28:06 [INFO] batch_size: 512
2023-06-04 20:28:06 [INFO] verbose: 1
2023-06-04 20:28:06 [INFO] TensorBoard_log_path: logs
2023-06-04 20:28:06 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:28:06 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:28:06 [INFO] EarlyStopping_patience: 15
2023-06-04 20:28:06 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:28:06 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:28:06 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:28:06 [INFO] Checkpoint_monitor: loss
2023-06-04 20:28:06 [INFO] save_best_only: True
2023-06-04 20:28:06 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:28:06 [INFO] log_path: logs/training.log
2023-06-04 20:28:06 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:28:06 [INFO] Training in progress
2023-06-04 20:28:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.
2023-06-04 20:28:09 [DEBUG] Epoch 0 - {'loss': '0.140108', 'mae': '0.258534', 'lr': '0.001000'} 
2023-06-04 20:28:12 [DEBUG] Epoch 1 - {'loss': '0.050424', 'mae': '0.165730', 'lr': '0.001000'} 
2023-06-04 20:28:15 [DEBUG] Epoch 2 - {'loss': '0.040594', 'mae': '0.149827', 'lr': '0.001000'} 
2023-06-04 20:28:17 [DEBUG] Epoch 3 - {'loss': '0.033523', 'mae': '0.138974', 'lr': '0.001000'} 
2023-06-04 20:28:20 [DEBUG] Epoch 4 - {'loss': '0.030683', 'mae': '0.134014', 'lr': '0.001000'} 
2023-06-04 20:28:20 [INFO] Training finished, elapsed time: 14.11 seconds
2023-06-04 20:28:20 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:31:29 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:31:29
2023-06-04 20:31:29 [INFO] Model configurations:
2023-06-04 20:31:29 [INFO] name: WF_T_21
2023-06-04 20:31:29 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:31:29 [INFO] loss_function: mse
2023-06-04 20:31:29 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:31:29 [INFO] metric: mae
2023-06-04 20:31:29 [INFO] shuffle: True
2023-06-04 20:31:29 [INFO] epochs: 5
2023-06-04 20:31:29 [INFO] batch_size: 512
2023-06-04 20:31:29 [INFO] verbose: 1
2023-06-04 20:31:29 [INFO] TensorBoard_log_path: logs
2023-06-04 20:31:29 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:31:29 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:31:29 [INFO] EarlyStopping_patience: 15
2023-06-04 20:31:29 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:31:29 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:31:29 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:31:29 [INFO] Checkpoint_monitor: loss
2023-06-04 20:31:29 [INFO] save_best_only: True
2023-06-04 20:31:29 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:31:29 [INFO] log_path: logs/training.log
2023-06-04 20:31:29 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:31:30 [INFO] Training in progress
2023-06-04 20:31:30 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0057s). Check your callbacks.
2023-06-04 20:31:33 [DEBUG] Epoch 0 - {'loss': '0.135149', 'mae': '0.255175', 'lr': '0.001000'} 
2023-06-04 20:31:36 [DEBUG] Epoch 1 - {'loss': '0.046377', 'mae': '0.161042', 'lr': '0.001000'} 
2023-06-04 20:31:38 [DEBUG] Epoch 2 - {'loss': '0.034621', 'mae': '0.141355', 'lr': '0.001000'} 
2023-06-04 20:31:41 [DEBUG] Epoch 3 - {'loss': '0.031100', 'mae': '0.134745', 'lr': '0.001000'} 
2023-06-04 20:31:44 [DEBUG] Epoch 4 - {'loss': '0.029256', 'mae': '0.131087', 'lr': '0.001000'} 
2023-06-04 20:31:44 [INFO] Training finished, elapsed time: 14.25 seconds
2023-06-04 20:31:44 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:32:02 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:32:02
2023-06-04 20:32:02 [INFO] Model configurations:
2023-06-04 20:32:02 [INFO] name: WF_T_21
2023-06-04 20:32:02 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:32:02 [INFO] loss_function: mse
2023-06-04 20:32:02 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:32:02 [INFO] metric: mae
2023-06-04 20:32:02 [INFO] shuffle: True
2023-06-04 20:32:02 [INFO] epochs: 5
2023-06-04 20:32:02 [INFO] batch_size: 512
2023-06-04 20:32:02 [INFO] verbose: 1
2023-06-04 20:32:02 [INFO] TensorBoard_log_path: logs
2023-06-04 20:32:02 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:32:02 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:32:02 [INFO] EarlyStopping_patience: 15
2023-06-04 20:32:02 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:32:02 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:32:02 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:32:02 [INFO] Checkpoint_monitor: loss
2023-06-04 20:32:02 [INFO] save_best_only: True
2023-06-04 20:32:02 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:32:02 [INFO] log_path: logs/training.log
2023-06-04 20:32:02 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:32:02 [INFO] Training in progress
2023-06-04 20:32:02 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 20:32:05 [DEBUG] Epoch 0 - {'loss': '0.102305', 'mae': '0.225268', 'lr': '0.001000'} 
2023-06-04 20:32:08 [DEBUG] Epoch 1 - {'loss': '0.042116', 'mae': '0.154073', 'lr': '0.001000'} 
2023-06-04 20:32:10 [DEBUG] Epoch 2 - {'loss': '0.033471', 'mae': '0.139401', 'lr': '0.001000'} 
2023-06-04 20:32:13 [DEBUG] Epoch 3 - {'loss': '0.030258', 'mae': '0.133344', 'lr': '0.001000'} 
2023-06-04 20:32:16 [DEBUG] Epoch 4 - {'loss': '0.028603', 'mae': '0.129945', 'lr': '0.001000'} 
2023-06-04 20:32:16 [INFO] Training finished, elapsed time: 14.09 seconds
2023-06-04 20:32:16 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:33:31 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:33:31
2023-06-04 20:33:31 [INFO] Model configurations:
2023-06-04 20:33:31 [INFO] name: WF_T_21
2023-06-04 20:33:31 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:33:31 [INFO] loss_function: mse
2023-06-04 20:33:31 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:33:31 [INFO] metric: mae
2023-06-04 20:33:31 [INFO] shuffle: True
2023-06-04 20:33:31 [INFO] epochs: 5
2023-06-04 20:33:31 [INFO] batch_size: 512
2023-06-04 20:33:31 [INFO] verbose: 1
2023-06-04 20:33:31 [INFO] TensorBoard_log_path: logs
2023-06-04 20:33:31 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:33:31 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:33:31 [INFO] EarlyStopping_patience: 15
2023-06-04 20:33:31 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:33:31 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:33:31 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:33:31 [INFO] Checkpoint_monitor: loss
2023-06-04 20:33:31 [INFO] save_best_only: True
2023-06-04 20:33:31 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:33:31 [INFO] log_path: logs/training.log
2023-06-04 20:33:31 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:33:31 [INFO] Training in progress
2023-06-04 20:33:32 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 20:33:34 [DEBUG] Epoch 0 - {'loss': '0.123125', 'mae': '0.243291', 'lr': '0.001000'} 
2023-06-04 20:33:37 [DEBUG] Epoch 1 - {'loss': '0.046510', 'mae': '0.159808', 'lr': '0.001000'} 
2023-06-04 20:33:40 [DEBUG] Epoch 2 - {'loss': '0.035846', 'mae': '0.142875', 'lr': '0.001000'} 
2023-06-04 20:33:42 [DEBUG] Epoch 3 - {'loss': '0.031517', 'mae': '0.135562', 'lr': '0.001000'} 
2023-06-04 20:33:45 [DEBUG] Epoch 4 - {'loss': '0.029445', 'mae': '0.131632', 'lr': '0.001000'} 
2023-06-04 20:33:45 [INFO] Training finished, elapsed time: 14.22 seconds
2023-06-04 20:33:45 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:47:24 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:47:24
2023-06-04 20:47:24 [INFO] Model configurations:
2023-06-04 20:47:24 [INFO] name: WF_T_21
2023-06-04 20:47:24 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:47:24 [INFO] loss_function: mse
2023-06-04 20:47:24 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:47:24 [INFO] metric: mae
2023-06-04 20:47:24 [INFO] shuffle: True
2023-06-04 20:47:24 [INFO] epochs: 5
2023-06-04 20:47:24 [INFO] batch_size: 512
2023-06-04 20:47:24 [INFO] verbose: 1
2023-06-04 20:47:24 [INFO] TensorBoard_log_path: logs
2023-06-04 20:47:24 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:47:24 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:47:24 [INFO] EarlyStopping_patience: 15
2023-06-04 20:47:24 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:47:24 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:47:24 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:47:24 [INFO] Checkpoint_monitor: loss
2023-06-04 20:47:24 [INFO] save_best_only: True
2023-06-04 20:47:24 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:47:24 [INFO] log_path: logs/training.log
2023-06-04 20:47:24 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:47:24 [INFO] Training in progress
2023-06-04 20:47:24 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:47:27 [DEBUG] Epoch 0 - {'loss': '0.188811', 'mae': '0.299013', 'lr': '0.001000'} 
2023-06-04 20:47:30 [DEBUG] Epoch 1 - {'loss': '0.052000', 'mae': '0.170391', 'lr': '0.001000'} 
2023-06-04 20:47:32 [DEBUG] Epoch 2 - {'loss': '0.035179', 'mae': '0.142714', 'lr': '0.001000'} 
2023-06-04 20:47:35 [DEBUG] Epoch 3 - {'loss': '0.031434', 'mae': '0.135490', 'lr': '0.001000'} 
2023-06-04 20:47:38 [DEBUG] Epoch 4 - {'loss': '0.029592', 'mae': '0.131773', 'lr': '0.001000'} 
2023-06-04 20:47:38 [INFO] Training finished, elapsed time: 14.16 seconds
2023-06-04 20:47:38 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:48:45 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:48:45
2023-06-04 20:48:45 [INFO] Model configurations:
2023-06-04 20:48:45 [INFO] name: WF_T_21
2023-06-04 20:48:45 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:48:45 [INFO] loss_function: mse
2023-06-04 20:48:45 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:48:45 [INFO] metric: mae
2023-06-04 20:48:45 [INFO] shuffle: True
2023-06-04 20:48:45 [INFO] epochs: 5
2023-06-04 20:48:45 [INFO] batch_size: 512
2023-06-04 20:48:45 [INFO] verbose: 1
2023-06-04 20:48:45 [INFO] TensorBoard_log_path: logs
2023-06-04 20:48:45 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:48:45 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:48:45 [INFO] EarlyStopping_patience: 15
2023-06-04 20:48:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:48:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:48:45 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:48:45 [INFO] Checkpoint_monitor: loss
2023-06-04 20:48:45 [INFO] save_best_only: True
2023-06-04 20:48:45 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:48:45 [INFO] log_path: logs/training.log
2023-06-04 20:48:45 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:48:45 [INFO] Training in progress
2023-06-04 20:48:45 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.
2023-06-04 20:48:48 [DEBUG] Epoch 0 - {'loss': '0.079822', 'mae': '0.199054', 'lr': '0.001000'} 
2023-06-04 20:48:51 [DEBUG] Epoch 1 - {'loss': '0.037842', 'mae': '0.146090', 'lr': '0.001000'} 
2023-06-04 20:48:54 [DEBUG] Epoch 2 - {'loss': '0.031868', 'mae': '0.136221', 'lr': '0.001000'} 
2023-06-04 20:48:56 [DEBUG] Epoch 3 - {'loss': '0.029463', 'mae': '0.131654', 'lr': '0.001000'} 
2023-06-04 20:48:59 [DEBUG] Epoch 4 - {'loss': '0.028131', 'mae': '0.128906', 'lr': '0.001000'} 
2023-06-04 20:48:59 [INFO] Training finished, elapsed time: 14.24 seconds
2023-06-04 20:48:59 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:50:07 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:50:07
2023-06-04 20:50:07 [INFO] Model configurations:
2023-06-04 20:50:07 [INFO] name: WF_T_21
2023-06-04 20:50:07 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:50:07 [INFO] loss_function: mse
2023-06-04 20:50:07 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:50:07 [INFO] metric: mae
2023-06-04 20:50:07 [INFO] shuffle: True
2023-06-04 20:50:07 [INFO] epochs: 5
2023-06-04 20:50:07 [INFO] batch_size: 512
2023-06-04 20:50:07 [INFO] verbose: 1
2023-06-04 20:50:07 [INFO] TensorBoard_log_path: logs
2023-06-04 20:50:07 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:50:07 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:50:07 [INFO] EarlyStopping_patience: 15
2023-06-04 20:50:07 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:50:07 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:50:07 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:50:07 [INFO] Checkpoint_monitor: loss
2023-06-04 20:50:07 [INFO] save_best_only: True
2023-06-04 20:50:07 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:50:07 [INFO] log_path: logs/training.log
2023-06-04 20:50:07 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:50:07 [INFO] Training in progress
2023-06-04 20:50:08 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:50:11 [DEBUG] Epoch 0 - {'loss': '0.147478', 'mae': '0.264483', 'lr': '0.001000'} 
2023-06-04 20:50:13 [DEBUG] Epoch 1 - {'loss': '0.049673', 'mae': '0.165963', 'lr': '0.001000'} 
2023-06-04 20:50:16 [DEBUG] Epoch 2 - {'loss': '0.037292', 'mae': '0.145991', 'lr': '0.001000'} 
2023-06-04 20:50:19 [DEBUG] Epoch 3 - {'loss': '0.031962', 'mae': '0.136443', 'lr': '0.001000'} 
2023-06-04 20:50:21 [DEBUG] Epoch 4 - {'loss': '0.029652', 'mae': '0.131900', 'lr': '0.001000'} 
2023-06-04 20:50:21 [INFO] Training finished, elapsed time: 14.11 seconds
2023-06-04 20:50:21 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:51:43 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:51:43
2023-06-04 20:51:43 [INFO] Model configurations:
2023-06-04 20:51:43 [INFO] name: WF_T_21
2023-06-04 20:51:43 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:51:43 [INFO] loss_function: mse
2023-06-04 20:51:43 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:51:43 [INFO] metric: mae
2023-06-04 20:51:43 [INFO] shuffle: True
2023-06-04 20:51:43 [INFO] epochs: 5
2023-06-04 20:51:43 [INFO] batch_size: 512
2023-06-04 20:51:43 [INFO] verbose: 1
2023-06-04 20:51:43 [INFO] TensorBoard_log_path: logs
2023-06-04 20:51:43 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:51:43 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:51:43 [INFO] EarlyStopping_patience: 15
2023-06-04 20:51:43 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:51:43 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:51:43 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:51:43 [INFO] Checkpoint_monitor: loss
2023-06-04 20:51:43 [INFO] save_best_only: True
2023-06-04 20:51:43 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:51:43 [INFO] log_path: logs/training.log
2023-06-04 20:51:43 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:51:43 [INFO] Training in progress
2023-06-04 20:51:44 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:51:47 [DEBUG] Epoch 0 - {'loss': '0.163216', 'mae': '0.283079', 'lr': '0.001000'} 
2023-06-04 20:51:49 [DEBUG] Epoch 1 - {'loss': '0.054422', 'mae': '0.176435', 'lr': '0.001000'} 
2023-06-04 20:51:52 [DEBUG] Epoch 2 - {'loss': '0.036075', 'mae': '0.145534', 'lr': '0.001000'} 
2023-06-04 20:51:55 [DEBUG] Epoch 3 - {'loss': '0.030991', 'mae': '0.134945', 'lr': '0.001000'} 
2023-06-04 20:51:57 [DEBUG] Epoch 4 - {'loss': '0.029005', 'mae': '0.130702', 'lr': '0.001000'} 
2023-06-04 20:51:57 [INFO] Training finished, elapsed time: 14.21 seconds
2023-06-04 20:51:57 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:54:14 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:54:14
2023-06-04 20:54:14 [INFO] Model configurations:
2023-06-04 20:54:14 [INFO] name: WF_T_21
2023-06-04 20:54:14 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:54:14 [INFO] loss_function: mse
2023-06-04 20:54:14 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:54:14 [INFO] metric: mae
2023-06-04 20:54:14 [INFO] shuffle: True
2023-06-04 20:54:14 [INFO] epochs: 2
2023-06-04 20:54:14 [INFO] batch_size: 512
2023-06-04 20:54:14 [INFO] verbose: 1
2023-06-04 20:54:14 [INFO] TensorBoard_log_path: logs
2023-06-04 20:54:14 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:54:14 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:54:14 [INFO] EarlyStopping_patience: 15
2023-06-04 20:54:14 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:54:14 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:54:14 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:54:14 [INFO] Checkpoint_monitor: loss
2023-06-04 20:54:14 [INFO] save_best_only: True
2023-06-04 20:54:14 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:54:14 [INFO] log_path: logs/training.log
2023-06-04 20:54:14 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:54:14 [INFO] Training in progress
2023-06-04 20:54:15 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:54:18 [DEBUG] Epoch 0 - {'loss': '0.162729', 'mae': '0.279991', 'lr': '0.001000'} 
2023-06-04 20:54:21 [DEBUG] Epoch 1 - {'loss': '0.053348', 'mae': '0.174100', 'lr': '0.001000'} 
2023-06-04 20:54:21 [INFO] Training finished, elapsed time: 6.22 seconds
2023-06-04 20:54:21 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:54:56 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:54:56
2023-06-04 20:54:56 [INFO] Model configurations:
2023-06-04 20:54:56 [INFO] name: WF_T_21
2023-06-04 20:54:56 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:54:56 [INFO] loss_function: mse
2023-06-04 20:54:56 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:54:56 [INFO] metric: mae
2023-06-04 20:54:56 [INFO] shuffle: True
2023-06-04 20:54:56 [INFO] epochs: 2
2023-06-04 20:54:56 [INFO] batch_size: 512
2023-06-04 20:54:56 [INFO] verbose: 1
2023-06-04 20:54:56 [INFO] TensorBoard_log_path: logs
2023-06-04 20:54:56 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:54:56 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:54:56 [INFO] EarlyStopping_patience: 15
2023-06-04 20:54:56 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:54:56 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:54:56 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:54:56 [INFO] Checkpoint_monitor: loss
2023-06-04 20:54:56 [INFO] save_best_only: True
2023-06-04 20:54:56 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:54:56 [INFO] log_path: logs/training.log
2023-06-04 20:54:56 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:54:57 [INFO] Training in progress
2023-06-04 20:54:57 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 20:55:00 [DEBUG] Epoch 0 - {'loss': '0.114341', 'mae': '0.234901', 'lr': '0.001000'} 
2023-06-04 20:55:03 [DEBUG] Epoch 1 - {'loss': '0.042840', 'mae': '0.154802', 'lr': '0.001000'} 
2023-06-04 20:55:03 [INFO] Training finished, elapsed time: 6.13 seconds
2023-06-04 20:55:03 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:55:33 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:55:33
2023-06-04 20:55:33 [INFO] Model configurations:
2023-06-04 20:55:33 [INFO] name: WF_T_21
2023-06-04 20:55:33 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:55:33 [INFO] loss_function: mse
2023-06-04 20:55:33 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:55:33 [INFO] metric: mae
2023-06-04 20:55:33 [INFO] shuffle: True
2023-06-04 20:55:33 [INFO] epochs: 2
2023-06-04 20:55:33 [INFO] batch_size: 512
2023-06-04 20:55:33 [INFO] verbose: 1
2023-06-04 20:55:33 [INFO] TensorBoard_log_path: logs
2023-06-04 20:55:33 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:55:33 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:55:33 [INFO] EarlyStopping_patience: 15
2023-06-04 20:55:33 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:55:33 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:55:33 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:55:33 [INFO] Checkpoint_monitor: loss
2023-06-04 20:55:33 [INFO] save_best_only: True
2023-06-04 20:55:33 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:55:33 [INFO] log_path: logs/training.log
2023-06-04 20:55:33 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:55:33 [INFO] Training in progress
2023-06-04 20:55:34 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.
2023-06-04 20:55:37 [DEBUG] Epoch 0 - {'loss': '0.150364', 'mae': '0.272762', 'lr': '0.001000'} 
2023-06-04 20:55:40 [DEBUG] Epoch 1 - {'loss': '0.056212', 'mae': '0.178162', 'lr': '0.001000'} 
2023-06-04 20:55:40 [INFO] Training finished, elapsed time: 6.10 seconds
2023-06-04 20:55:40 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:58:06 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:58:06
2023-06-04 20:58:06 [INFO] Model configurations:
2023-06-04 20:58:06 [INFO] name: WF_T_21
2023-06-04 20:58:06 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:58:06 [INFO] loss_function: mse
2023-06-04 20:58:06 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:58:06 [INFO] metric: mae
2023-06-04 20:58:06 [INFO] shuffle: True
2023-06-04 20:58:06 [INFO] epochs: 2
2023-06-04 20:58:06 [INFO] batch_size: 512
2023-06-04 20:58:06 [INFO] verbose: 1
2023-06-04 20:58:06 [INFO] TensorBoard_log_path: logs
2023-06-04 20:58:06 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:58:06 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:58:06 [INFO] EarlyStopping_patience: 15
2023-06-04 20:58:06 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:58:06 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:58:06 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:58:06 [INFO] Checkpoint_monitor: loss
2023-06-04 20:58:06 [INFO] save_best_only: True
2023-06-04 20:58:06 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:58:06 [INFO] log_path: logs/training.log
2023-06-04 20:58:06 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:58:06 [INFO] Training in progress
2023-06-04 20:58:07 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.
2023-06-04 20:58:10 [DEBUG] Epoch 0 - {'loss': '0.166176', 'mae': '0.284847', 'lr': '0.001000'} 
2023-06-04 20:58:12 [DEBUG] Epoch 1 - {'loss': '0.056418', 'mae': '0.178123', 'lr': '0.001000'} 
2023-06-04 20:58:12 [INFO] Training finished, elapsed time: 6.18 seconds
2023-06-04 20:58:12 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 20:59:24 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 20:59:24
2023-06-04 20:59:24 [INFO] Model configurations:
2023-06-04 20:59:24 [INFO] name: WF_T_21
2023-06-04 20:59:24 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 20:59:24 [INFO] loss_function: mse
2023-06-04 20:59:24 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 20:59:24 [INFO] metric: mae
2023-06-04 20:59:24 [INFO] shuffle: True
2023-06-04 20:59:24 [INFO] epochs: 2
2023-06-04 20:59:24 [INFO] batch_size: 512
2023-06-04 20:59:24 [INFO] verbose: 1
2023-06-04 20:59:24 [INFO] TensorBoard_log_path: logs
2023-06-04 20:59:24 [INFO] TensorBoard_hist_freq: 1
2023-06-04 20:59:24 [INFO] EarlyStopping_monitor: loss
2023-06-04 20:59:24 [INFO] EarlyStopping_patience: 15
2023-06-04 20:59:24 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 20:59:24 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 20:59:24 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 20:59:24 [INFO] Checkpoint_monitor: loss
2023-06-04 20:59:24 [INFO] save_best_only: True
2023-06-04 20:59:24 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 20:59:24 [INFO] log_path: logs/training.log
2023-06-04 20:59:24 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 20:59:25 [INFO] Training in progress
2023-06-04 20:59:25 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.
2023-06-04 20:59:28 [DEBUG] Epoch 0 - {'loss': '0.146468', 'mae': '0.267107', 'lr': '0.001000'} 
2023-06-04 20:59:31 [DEBUG] Epoch 1 - {'loss': '0.048796', 'mae': '0.166107', 'lr': '0.001000'} 
2023-06-04 20:59:31 [INFO] Training finished, elapsed time: 6.11 seconds
2023-06-04 20:59:31 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:00:02 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:00:02
2023-06-04 21:00:02 [INFO] Model configurations:
2023-06-04 21:00:02 [INFO] name: WF_T_21
2023-06-04 21:00:02 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:00:02 [INFO] loss_function: mse
2023-06-04 21:00:02 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:00:02 [INFO] metric: mae
2023-06-04 21:00:02 [INFO] shuffle: True
2023-06-04 21:00:02 [INFO] epochs: 2
2023-06-04 21:00:02 [INFO] batch_size: 512
2023-06-04 21:00:02 [INFO] verbose: 1
2023-06-04 21:00:02 [INFO] TensorBoard_log_path: logs
2023-06-04 21:00:02 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:00:02 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:00:02 [INFO] EarlyStopping_patience: 15
2023-06-04 21:00:02 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:00:02 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:00:02 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:00:02 [INFO] Checkpoint_monitor: loss
2023-06-04 21:00:02 [INFO] save_best_only: True
2023-06-04 21:00:02 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:00:02 [INFO] log_path: logs/training.log
2023-06-04 21:00:02 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:00:02 [INFO] Training in progress
2023-06-04 21:00:03 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 21:00:05 [DEBUG] Epoch 0 - {'loss': '0.153497', 'mae': '0.268979', 'lr': '0.001000'} 
2023-06-04 21:00:08 [DEBUG] Epoch 1 - {'loss': '0.047468', 'mae': '0.164149', 'lr': '0.001000'} 
2023-06-04 21:00:08 [INFO] Training finished, elapsed time: 6.20 seconds
2023-06-04 21:00:08 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:00:51 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:00:51
2023-06-04 21:00:51 [INFO] Model configurations:
2023-06-04 21:00:51 [INFO] name: WF_T_21
2023-06-04 21:00:51 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:00:51 [INFO] loss_function: mse
2023-06-04 21:00:51 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:00:51 [INFO] metric: mae
2023-06-04 21:00:51 [INFO] shuffle: True
2023-06-04 21:00:51 [INFO] epochs: 2
2023-06-04 21:00:51 [INFO] batch_size: 512
2023-06-04 21:00:51 [INFO] verbose: 1
2023-06-04 21:00:51 [INFO] TensorBoard_log_path: logs
2023-06-04 21:00:51 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:00:51 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:00:51 [INFO] EarlyStopping_patience: 15
2023-06-04 21:00:51 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:00:51 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:00:51 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:00:51 [INFO] Checkpoint_monitor: loss
2023-06-04 21:00:51 [INFO] save_best_only: True
2023-06-04 21:00:51 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:00:51 [INFO] log_path: logs/training.log
2023-06-04 21:00:51 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:00:51 [INFO] Training in progress
2023-06-04 21:00:52 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 21:00:54 [DEBUG] Epoch 0 - {'loss': '0.141173', 'mae': '0.261610', 'lr': '0.001000'} 
2023-06-04 21:00:57 [DEBUG] Epoch 1 - {'loss': '0.048141', 'mae': '0.162864', 'lr': '0.001000'} 
2023-06-04 21:00:57 [INFO] Training finished, elapsed time: 6.10 seconds
2023-06-04 21:00:57 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:01:15 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:01:15
2023-06-04 21:01:15 [INFO] Model configurations:
2023-06-04 21:01:15 [INFO] name: WF_T_21
2023-06-04 21:01:15 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:01:15 [INFO] loss_function: mse
2023-06-04 21:01:15 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:01:15 [INFO] metric: mae
2023-06-04 21:01:15 [INFO] shuffle: True
2023-06-04 21:01:15 [INFO] epochs: 2
2023-06-04 21:01:15 [INFO] batch_size: 512
2023-06-04 21:01:15 [INFO] verbose: 1
2023-06-04 21:01:15 [INFO] TensorBoard_log_path: logs
2023-06-04 21:01:15 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:01:15 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:01:15 [INFO] EarlyStopping_patience: 15
2023-06-04 21:01:15 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:01:15 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:01:15 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:01:15 [INFO] Checkpoint_monitor: loss
2023-06-04 21:01:15 [INFO] save_best_only: True
2023-06-04 21:01:15 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:01:15 [INFO] log_path: logs/training.log
2023-06-04 21:01:15 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:01:15 [INFO] Training in progress
2023-06-04 21:01:16 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 21:01:19 [DEBUG] Epoch 0 - {'loss': '0.180996', 'mae': '0.287831', 'lr': '0.001000'} 
2023-06-04 21:01:21 [DEBUG] Epoch 1 - {'loss': '0.055589', 'mae': '0.174215', 'lr': '0.001000'} 
2023-06-04 21:01:21 [INFO] Training finished, elapsed time: 6.03 seconds
2023-06-04 21:01:21 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:01:43 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:01:43
2023-06-04 21:01:43 [INFO] Model configurations:
2023-06-04 21:01:43 [INFO] name: WF_T_21
2023-06-04 21:01:43 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:01:43 [INFO] loss_function: mse
2023-06-04 21:01:43 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:01:43 [INFO] metric: mae
2023-06-04 21:01:43 [INFO] shuffle: True
2023-06-04 21:01:43 [INFO] epochs: 2
2023-06-04 21:01:43 [INFO] batch_size: 512
2023-06-04 21:01:43 [INFO] verbose: 1
2023-06-04 21:01:43 [INFO] TensorBoard_log_path: logs
2023-06-04 21:01:43 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:01:43 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:01:43 [INFO] EarlyStopping_patience: 15
2023-06-04 21:01:43 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:01:43 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:01:43 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:01:43 [INFO] Checkpoint_monitor: loss
2023-06-04 21:01:43 [INFO] save_best_only: True
2023-06-04 21:01:43 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:01:43 [INFO] log_path: logs/training.log
2023-06-04 21:01:43 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:01:43 [INFO] Training in progress
2023-06-04 21:01:43 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.
2023-06-04 21:01:46 [DEBUG] Epoch 0 - {'loss': '0.200190', 'mae': '0.303520', 'lr': '0.001000'} 
2023-06-04 21:01:49 [DEBUG] Epoch 1 - {'loss': '0.062520', 'mae': '0.185772', 'lr': '0.001000'} 
2023-06-04 21:01:49 [INFO] Training finished, elapsed time: 6.13 seconds
2023-06-04 21:01:49 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:03:12 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:03:12
2023-06-04 21:03:12 [INFO] Model configurations:
2023-06-04 21:03:12 [INFO] name: WF_T_21
2023-06-04 21:03:12 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:03:12 [INFO] loss_function: mse
2023-06-04 21:03:12 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:03:12 [INFO] metric: mae
2023-06-04 21:03:12 [INFO] shuffle: True
2023-06-04 21:03:12 [INFO] epochs: 2
2023-06-04 21:03:12 [INFO] batch_size: 512
2023-06-04 21:03:12 [INFO] verbose: 1
2023-06-04 21:03:12 [INFO] TensorBoard_log_path: logs
2023-06-04 21:03:12 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:03:12 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:03:12 [INFO] EarlyStopping_patience: 15
2023-06-04 21:03:12 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:03:12 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:03:12 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:03:12 [INFO] Checkpoint_monitor: loss
2023-06-04 21:03:12 [INFO] save_best_only: True
2023-06-04 21:03:12 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:03:12 [INFO] log_path: logs/training.log
2023-06-04 21:03:12 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:03:12 [INFO] Training in progress
2023-06-04 21:03:13 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.
2023-06-04 21:03:16 [DEBUG] Epoch 0 - {'loss': '0.212709', 'mae': '0.309323', 'lr': '0.001000'} 
2023-06-04 21:03:19 [DEBUG] Epoch 1 - {'loss': '0.064234', 'mae': '0.189795', 'lr': '0.001000'} 
2023-06-04 21:03:19 [INFO] Training finished, elapsed time: 6.30 seconds
2023-06-04 21:03:19 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:03:39 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:03:39
2023-06-04 21:03:39 [INFO] Model configurations:
2023-06-04 21:03:39 [INFO] name: WF_T_21
2023-06-04 21:03:39 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:03:39 [INFO] loss_function: mse
2023-06-04 21:03:39 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:03:39 [INFO] metric: mae
2023-06-04 21:03:39 [INFO] shuffle: True
2023-06-04 21:03:39 [INFO] epochs: 2
2023-06-04 21:03:39 [INFO] batch_size: 512
2023-06-04 21:03:39 [INFO] verbose: 1
2023-06-04 21:03:39 [INFO] TensorBoard_log_path: logs
2023-06-04 21:03:39 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:03:39 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:03:39 [INFO] EarlyStopping_patience: 15
2023-06-04 21:03:39 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:03:39 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:03:39 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:03:39 [INFO] Checkpoint_monitor: loss
2023-06-04 21:03:39 [INFO] save_best_only: True
2023-06-04 21:03:39 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:03:39 [INFO] log_path: logs/training.log
2023-06-04 21:03:39 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:03:40 [INFO] Training in progress
2023-06-04 21:03:40 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 21:03:43 [DEBUG] Epoch 0 - {'loss': '0.198769', 'mae': '0.301155', 'lr': '0.001000'} 
2023-06-04 21:03:46 [DEBUG] Epoch 1 - {'loss': '0.059041', 'mae': '0.180927', 'lr': '0.001000'} 
2023-06-04 21:03:46 [INFO] Training finished, elapsed time: 6.19 seconds
2023-06-04 21:03:46 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:04:31 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:04:31
2023-06-04 21:04:31 [INFO] Model configurations:
2023-06-04 21:04:31 [INFO] name: WF_T_21
2023-06-04 21:04:31 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:04:31 [INFO] loss_function: mse
2023-06-04 21:04:31 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:04:31 [INFO] metric: mae
2023-06-04 21:04:31 [INFO] shuffle: True
2023-06-04 21:04:31 [INFO] epochs: 2
2023-06-04 21:04:31 [INFO] batch_size: 512
2023-06-04 21:04:31 [INFO] verbose: 1
2023-06-04 21:04:31 [INFO] TensorBoard_log_path: logs
2023-06-04 21:04:31 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:04:31 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:04:31 [INFO] EarlyStopping_patience: 15
2023-06-04 21:04:31 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:04:31 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:04:31 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:04:31 [INFO] Checkpoint_monitor: loss
2023-06-04 21:04:31 [INFO] save_best_only: True
2023-06-04 21:04:31 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:04:31 [INFO] log_path: logs/training.log
2023-06-04 21:04:31 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:04:31 [INFO] Training in progress
2023-06-04 21:04:32 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 21:04:35 [DEBUG] Epoch 0 - {'loss': '0.094713', 'mae': '0.216756', 'lr': '0.001000'} 
2023-06-04 21:04:38 [DEBUG] Epoch 1 - {'loss': '0.039441', 'mae': '0.149655', 'lr': '0.001000'} 
2023-06-04 21:04:38 [INFO] Training finished, elapsed time: 6.23 seconds
2023-06-04 21:04:38 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:06:59 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:06:59
2023-06-04 21:06:59 [INFO] Model configurations:
2023-06-04 21:06:59 [INFO] name: WF_T_21
2023-06-04 21:06:59 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:06:59 [INFO] loss_function: mse
2023-06-04 21:06:59 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:06:59 [INFO] metric: mae
2023-06-04 21:06:59 [INFO] shuffle: True
2023-06-04 21:06:59 [INFO] epochs: 2
2023-06-04 21:06:59 [INFO] batch_size: 512
2023-06-04 21:06:59 [INFO] verbose: 1
2023-06-04 21:06:59 [INFO] TensorBoard_log_path: logs
2023-06-04 21:06:59 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:06:59 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:06:59 [INFO] EarlyStopping_patience: 15
2023-06-04 21:06:59 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:06:59 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:06:59 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:06:59 [INFO] Checkpoint_monitor: loss
2023-06-04 21:06:59 [INFO] save_best_only: True
2023-06-04 21:06:59 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:06:59 [INFO] log_path: logs/training.log
2023-06-04 21:06:59 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:06:59 [INFO] Training in progress
2023-06-04 21:07:00 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.
2023-06-04 21:07:02 [DEBUG] Epoch 0 - {'loss': '0.178963', 'mae': '0.294327', 'lr': '0.001000'} 
2023-06-04 21:07:05 [DEBUG] Epoch 1 - {'loss': '0.067625', 'mae': '0.196147', 'lr': '0.001000'} 
2023-06-04 21:07:05 [INFO] Training finished, elapsed time: 6.16 seconds
2023-06-04 21:07:05 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:08:18 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:08:18
2023-06-04 21:08:18 [INFO] Model configurations:
2023-06-04 21:08:18 [INFO] name: WF_T_21
2023-06-04 21:08:18 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:08:18 [INFO] loss_function: mse
2023-06-04 21:08:18 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:08:18 [INFO] metric: mae
2023-06-04 21:08:18 [INFO] shuffle: True
2023-06-04 21:08:18 [INFO] epochs: 2
2023-06-04 21:08:18 [INFO] batch_size: 512
2023-06-04 21:08:18 [INFO] verbose: 1
2023-06-04 21:08:18 [INFO] TensorBoard_log_path: logs
2023-06-04 21:08:18 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:08:18 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:08:18 [INFO] EarlyStopping_patience: 15
2023-06-04 21:08:18 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:08:18 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:08:18 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:08:18 [INFO] Checkpoint_monitor: loss
2023-06-04 21:08:18 [INFO] save_best_only: True
2023-06-04 21:08:18 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:08:18 [INFO] log_path: logs/training.log
2023-06-04 21:08:18 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:08:18 [INFO] Training in progress
2023-06-04 21:08:19 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 21:08:21 [DEBUG] Epoch 0 - {'loss': '0.239431', 'mae': '0.336470', 'lr': '0.001000'} 
2023-06-04 21:08:24 [DEBUG] Epoch 1 - {'loss': '0.066602', 'mae': '0.190698', 'lr': '0.001000'} 
2023-06-04 21:08:24 [INFO] Training finished, elapsed time: 6.12 seconds
2023-06-04 21:08:24 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:10:23 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:10:23
2023-06-04 21:10:23 [INFO] Model configurations:
2023-06-04 21:10:23 [INFO] name: WF_T_21
2023-06-04 21:10:23 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:10:23 [INFO] loss_function: mse
2023-06-04 21:10:23 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:10:23 [INFO] metric: mae
2023-06-04 21:10:23 [INFO] shuffle: True
2023-06-04 21:10:23 [INFO] epochs: 2
2023-06-04 21:10:23 [INFO] batch_size: 512
2023-06-04 21:10:23 [INFO] verbose: 1
2023-06-04 21:10:23 [INFO] TensorBoard_log_path: logs
2023-06-04 21:10:23 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:10:23 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:10:23 [INFO] EarlyStopping_patience: 15
2023-06-04 21:10:23 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:10:23 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:10:23 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:10:23 [INFO] Checkpoint_monitor: loss
2023-06-04 21:10:23 [INFO] save_best_only: True
2023-06-04 21:10:23 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:10:23 [INFO] log_path: logs/training.log
2023-06-04 21:10:23 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:10:23 [INFO] Training in progress
2023-06-04 21:10:24 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 21:10:27 [DEBUG] Epoch 0 - {'loss': '0.112163', 'mae': '0.235703', 'lr': '0.001000'} 
2023-06-04 21:10:29 [DEBUG] Epoch 1 - {'loss': '0.043994', 'mae': '0.156568', 'lr': '0.001000'} 
2023-06-04 21:10:29 [INFO] Training finished, elapsed time: 6.09 seconds
2023-06-04 21:10:29 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:10:46 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:10:46
2023-06-04 21:10:46 [INFO] Model configurations:
2023-06-04 21:10:46 [INFO] name: WF_T_21
2023-06-04 21:10:46 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:10:46 [INFO] loss_function: mse
2023-06-04 21:10:46 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:10:46 [INFO] metric: mae
2023-06-04 21:10:46 [INFO] shuffle: True
2023-06-04 21:10:46 [INFO] epochs: 2
2023-06-04 21:10:46 [INFO] batch_size: 512
2023-06-04 21:10:46 [INFO] verbose: 1
2023-06-04 21:10:46 [INFO] TensorBoard_log_path: logs
2023-06-04 21:10:46 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:10:46 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:10:46 [INFO] EarlyStopping_patience: 15
2023-06-04 21:10:46 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:10:46 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:10:46 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:10:46 [INFO] Checkpoint_monitor: loss
2023-06-04 21:10:46 [INFO] save_best_only: True
2023-06-04 21:10:46 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:10:46 [INFO] log_path: logs/training.log
2023-06-04 21:10:46 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:10:46 [INFO] Training in progress
2023-06-04 21:10:47 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 0.0057s). Check your callbacks.
2023-06-04 21:10:50 [DEBUG] Epoch 0 - {'loss': '0.084279', 'mae': '0.204447', 'lr': '0.001000'} 
2023-06-04 21:10:53 [DEBUG] Epoch 1 - {'loss': '0.036129', 'mae': '0.144232', 'lr': '0.001000'} 
2023-06-04 21:10:53 [INFO] Training finished, elapsed time: 6.17 seconds
2023-06-04 21:10:53 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:13:08 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:13:08
2023-06-04 21:13:08 [INFO] Model configurations:
2023-06-04 21:13:08 [INFO] name: WF_T_21
2023-06-04 21:13:08 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:13:08 [INFO] loss_function: mse
2023-06-04 21:13:08 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:13:08 [INFO] metric: mae
2023-06-04 21:13:08 [INFO] shuffle: True
2023-06-04 21:13:08 [INFO] epochs: 2
2023-06-04 21:13:08 [INFO] batch_size: 512
2023-06-04 21:13:08 [INFO] verbose: 1
2023-06-04 21:13:08 [INFO] TensorBoard_log_path: logs
2023-06-04 21:13:08 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:13:08 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:13:08 [INFO] EarlyStopping_patience: 15
2023-06-04 21:13:08 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:13:08 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:13:08 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:13:08 [INFO] Checkpoint_monitor: loss
2023-06-04 21:13:08 [INFO] save_best_only: True
2023-06-04 21:13:08 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:13:08 [INFO] log_path: logs/training.log
2023-06-04 21:13:08 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:13:08 [INFO] Training in progress
2023-06-04 21:13:09 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 21:13:12 [DEBUG] Epoch 0 - {'loss': '0.136846', 'mae': '0.258895', 'lr': '0.001000'} 
2023-06-04 21:13:14 [DEBUG] Epoch 1 - {'loss': '0.049153', 'mae': '0.163743', 'lr': '0.001000'} 
2023-06-04 21:13:14 [INFO] Training finished, elapsed time: 6.10 seconds
2023-06-04 21:13:14 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:14:47 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:14:47
2023-06-04 21:14:47 [INFO] Model configurations:
2023-06-04 21:14:47 [INFO] name: WF_T_21
2023-06-04 21:14:47 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:14:47 [INFO] loss_function: mse
2023-06-04 21:14:47 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:14:47 [INFO] metric: mae
2023-06-04 21:14:47 [INFO] shuffle: True
2023-06-04 21:14:47 [INFO] epochs: 2
2023-06-04 21:14:47 [INFO] batch_size: 512
2023-06-04 21:14:47 [INFO] verbose: 1
2023-06-04 21:14:47 [INFO] TensorBoard_log_path: logs
2023-06-04 21:14:47 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:14:47 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:14:47 [INFO] EarlyStopping_patience: 15
2023-06-04 21:14:47 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:14:47 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:14:47 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:14:47 [INFO] Checkpoint_monitor: loss
2023-06-04 21:14:47 [INFO] save_best_only: True
2023-06-04 21:14:47 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:14:47 [INFO] log_path: logs/training.log
2023-06-04 21:14:47 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:14:47 [INFO] Training in progress
2023-06-04 21:14:48 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.
2023-06-04 21:14:51 [DEBUG] Epoch 0 - {'loss': '0.222541', 'mae': '0.327578', 'lr': '0.001000'} 
2023-06-04 21:14:54 [DEBUG] Epoch 1 - {'loss': '0.070694', 'mae': '0.197238', 'lr': '0.001000'} 
2023-06-04 21:14:54 [INFO] Training finished, elapsed time: 6.08 seconds
2023-06-04 21:14:54 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:16:51 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:16:51
2023-06-04 21:16:51 [INFO] Model configurations:
2023-06-04 21:16:51 [INFO] name: WF_T_21
2023-06-04 21:16:51 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:16:51 [INFO] loss_function: mse
2023-06-04 21:16:51 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:16:51 [INFO] metric: mae
2023-06-04 21:16:51 [INFO] shuffle: True
2023-06-04 21:16:51 [INFO] epochs: 2
2023-06-04 21:16:51 [INFO] batch_size: 512
2023-06-04 21:16:51 [INFO] verbose: 1
2023-06-04 21:16:51 [INFO] TensorBoard_log_path: logs
2023-06-04 21:16:51 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:16:51 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:16:51 [INFO] EarlyStopping_patience: 15
2023-06-04 21:16:51 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:16:51 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:16:51 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:16:51 [INFO] Checkpoint_monitor: loss
2023-06-04 21:16:51 [INFO] save_best_only: True
2023-06-04 21:16:51 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:16:51 [INFO] log_path: logs/training.log
2023-06-04 21:16:51 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:16:51 [INFO] Training in progress
2023-06-04 21:16:52 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-04 21:16:55 [DEBUG] Epoch 0 - {'loss': '0.123014', 'mae': '0.250738', 'lr': '0.001000'} 
2023-06-04 21:16:57 [DEBUG] Epoch 1 - {'loss': '0.050076', 'mae': '0.169971', 'lr': '0.001000'} 
2023-06-04 21:16:57 [INFO] Training finished, elapsed time: 6.13 seconds
2023-06-04 21:16:57 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:19:37 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:19:37
2023-06-04 21:19:37 [INFO] Model configurations:
2023-06-04 21:19:37 [INFO] name: WF_T_21
2023-06-04 21:19:37 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:19:37 [INFO] loss_function: mse
2023-06-04 21:19:37 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:19:37 [INFO] metric: mae
2023-06-04 21:19:37 [INFO] shuffle: True
2023-06-04 21:19:37 [INFO] epochs: 20
2023-06-04 21:19:37 [INFO] batch_size: 512
2023-06-04 21:19:37 [INFO] verbose: 1
2023-06-04 21:19:37 [INFO] TensorBoard_log_path: logs
2023-06-04 21:19:37 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:19:37 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:19:37 [INFO] EarlyStopping_patience: 15
2023-06-04 21:19:37 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:19:37 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:19:37 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:19:37 [INFO] Checkpoint_monitor: loss
2023-06-04 21:19:37 [INFO] save_best_only: True
2023-06-04 21:19:37 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:19:37 [INFO] log_path: logs/training.log
2023-06-04 21:19:37 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:19:37 [INFO] Training in progress
2023-06-04 21:19:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.
2023-06-04 21:19:41 [DEBUG] Epoch 0 - {'loss': '0.117868', 'mae': '0.240337', 'lr': '0.001000'} 
2023-06-04 21:19:44 [DEBUG] Epoch 1 - {'loss': '0.042930', 'mae': '0.155840', 'lr': '0.001000'} 
2023-06-04 21:19:47 [DEBUG] Epoch 2 - {'loss': '0.033514', 'mae': '0.139545', 'lr': '0.001000'} 
2023-06-04 21:19:50 [DEBUG] Epoch 3 - {'loss': '0.030525', 'mae': '0.133816', 'lr': '0.001000'} 
2023-06-04 21:19:53 [DEBUG] Epoch 4 - {'loss': '0.028787', 'mae': '0.130332', 'lr': '0.001000'} 
2023-06-04 21:19:56 [DEBUG] Epoch 5 - {'loss': '0.027731', 'mae': '0.128187', 'lr': '0.001000'} 
2023-06-04 21:19:59 [DEBUG] Epoch 6 - {'loss': '0.027180', 'mae': '0.127096', 'lr': '0.001000'} 
2023-06-04 21:20:02 [DEBUG] Epoch 7 - {'loss': '0.026791', 'mae': '0.126264', 'lr': '0.001000'} 
2023-06-04 21:20:05 [DEBUG] Epoch 8 - {'loss': '0.026452', 'mae': '0.125546', 'lr': '0.001000'} 
2023-06-04 21:20:08 [DEBUG] Epoch 9 - {'loss': '0.026214', 'mae': '0.124997', 'lr': '0.001000'} 
2023-06-04 21:20:11 [DEBUG] Epoch 10 - {'loss': '0.026027', 'mae': '0.124587', 'lr': '0.001000'} 
2023-06-04 21:20:14 [DEBUG] Epoch 11 - {'loss': '0.025771', 'mae': '0.123978', 'lr': '0.001000'} 
2023-06-04 21:20:17 [DEBUG] Epoch 12 - {'loss': '0.025635', 'mae': '0.123672', 'lr': '0.001000'} 
2023-06-04 21:20:20 [DEBUG] Epoch 13 - {'loss': '0.025482', 'mae': '0.123310', 'lr': '0.001000'} 
2023-06-04 21:20:24 [DEBUG] Epoch 14 - {'loss': '0.025381', 'mae': '0.123081', 'lr': '0.001000'} 
2023-06-04 21:20:27 [DEBUG] Epoch 15 - {'loss': '0.025260', 'mae': '0.122789', 'lr': '0.001000'} 
2023-06-04 21:20:30 [DEBUG] Epoch 16 - {'loss': '0.025175', 'mae': '0.122594', 'lr': '0.001000'} 
2023-06-04 21:20:33 [DEBUG] Epoch 17 - {'loss': '0.025087', 'mae': '0.122386', 'lr': '0.001000'} 
2023-06-04 21:20:36 [DEBUG] Epoch 18 - {'loss': '0.025028', 'mae': '0.122245', 'lr': '0.001000'} 
2023-06-04 21:20:39 [DEBUG] Epoch 19 - {'loss': '0.024981', 'mae': '0.122152', 'lr': '0.001000'} 
2023-06-04 21:20:39 [INFO] Training finished, elapsed time: 62.18 seconds
2023-06-04 21:20:39 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-04 21:26:12 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/04 21:26:12
2023-06-04 21:26:12 [INFO] Model configurations:
2023-06-04 21:26:12 [INFO] name: WF_T_21
2023-06-04 21:26:12 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'resnet'}, 'base_nn_amps': {'name': 'resnet'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-04 21:26:12 [INFO] loss_function: mse
2023-06-04 21:26:12 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-04 21:26:12 [INFO] metric: mae
2023-06-04 21:26:12 [INFO] shuffle: True
2023-06-04 21:26:12 [INFO] epochs: 200
2023-06-04 21:26:12 [INFO] batch_size: 512
2023-06-04 21:26:12 [INFO] verbose: 1
2023-06-04 21:26:12 [INFO] TensorBoard_log_path: logs
2023-06-04 21:26:12 [INFO] TensorBoard_hist_freq: 1
2023-06-04 21:26:12 [INFO] EarlyStopping_monitor: loss
2023-06-04 21:26:12 [INFO] EarlyStopping_patience: 15
2023-06-04 21:26:12 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-04 21:26:12 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-04 21:26:12 [INFO] ReduceLROnPlateau_patience: 3
2023-06-04 21:26:12 [INFO] Checkpoint_monitor: loss
2023-06-04 21:26:12 [INFO] save_best_only: True
2023-06-04 21:26:12 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-04 21:26:12 [INFO] log_path: logs/training.log
2023-06-04 21:26:12 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-04 21:26:12 [INFO] Training in progress
2023-06-04 21:26:14 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0138s vs `on_train_batch_end` time: 0.0249s). Check your callbacks.
2023-06-04 21:26:30 [DEBUG] Epoch 0 - {'loss': '0.061409', 'mae': '0.175328', 'lr': '0.001000'} 
2023-06-04 21:26:47 [DEBUG] Epoch 1 - {'loss': '0.029104', 'mae': '0.131998', 'lr': '0.001000'} 
2023-06-04 21:27:03 [DEBUG] Epoch 2 - {'loss': '0.026820', 'mae': '0.126650', 'lr': '0.001000'} 
2023-06-06 19:47:22 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/06 19:47:22
2023-06-06 19:47:22 [INFO] Model configurations:
2023-06-06 19:47:22 [INFO] name: WF_T_21
2023-06-06 19:47:22 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'resnet'}, 'base_nn_amps': {'name': 'resnet'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-06 19:47:22 [INFO] loss_function: mse
2023-06-06 19:47:22 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-06 19:47:22 [INFO] metric: mae
2023-06-06 19:47:22 [INFO] shuffle: True
2023-06-06 19:47:22 [INFO] epochs: 200
2023-06-06 19:47:22 [INFO] batch_size: 512
2023-06-06 19:47:22 [INFO] verbose: 1
2023-06-06 19:47:22 [INFO] TensorBoard_log_path: logs
2023-06-06 19:47:22 [INFO] TensorBoard_hist_freq: 1
2023-06-06 19:47:22 [INFO] EarlyStopping_monitor: loss
2023-06-06 19:47:22 [INFO] EarlyStopping_patience: 15
2023-06-06 19:47:22 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-06 19:47:22 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-06 19:47:22 [INFO] ReduceLROnPlateau_patience: 3
2023-06-06 19:47:22 [INFO] Checkpoint_monitor: loss
2023-06-06 19:47:22 [INFO] save_best_only: True
2023-06-06 19:47:22 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-06 19:47:22 [INFO] log_path: logs/training.log
2023-06-06 19:47:22 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-06 19:47:22 [INFO] Training in progress
2023-06-06 19:47:25 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0135s vs `on_train_batch_end` time: 0.0240s). Check your callbacks.
2023-06-06 19:47:41 [DEBUG] Epoch 0 - {'loss': '0.042095', 'mae': '0.145692', 'lr': '0.001000'} 
2023-06-06 19:47:57 [DEBUG] Epoch 1 - {'loss': '0.027317', 'mae': '0.127239', 'lr': '0.001000'} 
2023-06-06 19:48:14 [DEBUG] Epoch 2 - {'loss': '0.026116', 'mae': '0.124725', 'lr': '0.001000'} 
2023-06-06 19:48:30 [DEBUG] Epoch 3 - {'loss': '0.025505', 'mae': '0.123390', 'lr': '0.001000'} 
2023-06-06 19:48:47 [DEBUG] Epoch 4 - {'loss': '0.025260', 'mae': '0.122846', 'lr': '0.001000'} 
2023-06-06 19:49:03 [DEBUG] Epoch 5 - {'loss': '0.024991', 'mae': '0.122237', 'lr': '0.001000'} 
2023-06-06 19:49:20 [DEBUG] Epoch 6 - {'loss': '0.024872', 'mae': '0.121931', 'lr': '0.001000'} 
2023-06-06 19:49:36 [DEBUG] Epoch 7 - {'loss': '0.024684', 'mae': '0.121500', 'lr': '0.001000'} 
2023-06-06 19:49:53 [DEBUG] Epoch 8 - {'loss': '0.024619', 'mae': '0.121355', 'lr': '0.001000'} 
2023-06-06 19:50:09 [DEBUG] Epoch 9 - {'loss': '0.024502', 'mae': '0.121092', 'lr': '0.001000'} 
2023-06-06 19:50:26 [DEBUG] Epoch 10 - {'loss': '0.024364', 'mae': '0.120739', 'lr': '0.001000'} 
2023-06-06 19:50:42 [DEBUG] Epoch 11 - {'loss': '0.024343', 'mae': '0.120751', 'lr': '0.001000'} 
2023-06-06 19:50:58 [DEBUG] Epoch 12 - {'loss': '0.024224', 'mae': '0.120417', 'lr': '0.001000'} 
2023-06-06 19:51:15 [DEBUG] Epoch 13 - {'loss': '0.024119', 'mae': '0.120166', 'lr': '0.001000'} 
2023-06-06 19:51:31 [DEBUG] Epoch 14 - {'loss': '0.024061', 'mae': '0.120032', 'lr': '0.001000'} 
2023-06-06 19:51:48 [DEBUG] Epoch 15 - {'loss': '0.024012', 'mae': '0.119900', 'lr': '0.001000'} 
2023-06-06 19:52:04 [DEBUG] Epoch 16 - {'loss': '0.024014', 'mae': '0.119920', 'lr': '0.001000'} 
2023-06-06 19:52:21 [DEBUG] Epoch 17 - {'loss': '0.023913', 'mae': '0.119645', 'lr': '0.001000'} 
2023-06-06 19:52:37 [DEBUG] Epoch 18 - {'loss': '0.023871', 'mae': '0.119559', 'lr': '0.001000'} 
2023-06-06 19:52:54 [DEBUG] Epoch 19 - {'loss': '0.023825', 'mae': '0.119439', 'lr': '0.001000'} 
2023-06-06 19:53:10 [DEBUG] Epoch 20 - {'loss': '0.023862', 'mae': '0.119532', 'lr': '0.001000'} 
2023-06-06 19:53:26 [DEBUG] Epoch 21 - {'loss': '0.023832', 'mae': '0.119456', 'lr': '0.001000'} 
2023-06-06 19:53:43 [DEBUG] Epoch 22 - {'loss': '0.023545', 'mae': '0.118745', 'lr': '0.000500'} 
2023-06-06 19:53:59 [DEBUG] Epoch 23 - {'loss': '0.023547', 'mae': '0.118739', 'lr': '0.000500'} 
2023-06-06 19:54:16 [DEBUG] Epoch 24 - {'loss': '0.023486', 'mae': '0.118571', 'lr': '0.000500'} 
2023-06-06 19:55:44 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/06 19:55:44
2023-06-06 19:55:44 [INFO] Model configurations:
2023-06-06 19:55:44 [INFO] name: WF_T_21
2023-06-06 19:55:44 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'resnet'}, 'base_nn_amps': {'name': 'resnet'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-06 19:55:44 [INFO] loss_function: mse
2023-06-06 19:55:44 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-06 19:55:44 [INFO] metric: mae
2023-06-06 19:55:44 [INFO] shuffle: True
2023-06-06 19:55:44 [INFO] epochs: 200
2023-06-06 19:55:44 [INFO] batch_size: 512
2023-06-06 19:55:44 [INFO] verbose: 1
2023-06-06 19:55:44 [INFO] TensorBoard_log_path: logs
2023-06-06 19:55:44 [INFO] TensorBoard_hist_freq: 1
2023-06-06 19:55:44 [INFO] EarlyStopping_monitor: loss
2023-06-06 19:55:44 [INFO] EarlyStopping_patience: 15
2023-06-06 19:55:44 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-06 19:55:44 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-06 19:55:44 [INFO] ReduceLROnPlateau_patience: 3
2023-06-06 19:55:44 [INFO] Checkpoint_monitor: loss
2023-06-06 19:55:44 [INFO] save_best_only: True
2023-06-06 19:55:44 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-06 19:55:44 [INFO] log_path: logs/training.log
2023-06-06 19:55:44 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-06 19:55:44 [INFO] Training in progress
2023-06-06 19:55:46 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0137s vs `on_train_batch_end` time: 0.0252s). Check your callbacks.
2023-06-06 19:56:02 [DEBUG] Epoch 0 - {'loss': '0.126009', 'mae': '0.236426', 'lr': '0.001000'} 
2023-06-06 19:56:19 [DEBUG] Epoch 1 - {'loss': '0.035127', 'mae': '0.144830', 'lr': '0.001000'} 
2023-06-06 19:56:35 [DEBUG] Epoch 2 - {'loss': '0.028387', 'mae': '0.130352', 'lr': '0.001000'} 
2023-06-06 19:56:52 [DEBUG] Epoch 3 - {'loss': '0.026875', 'mae': '0.126844', 'lr': '0.001000'} 
2023-06-06 19:57:08 [DEBUG] Epoch 4 - {'loss': '0.026262', 'mae': '0.125447', 'lr': '0.001000'} 
2023-06-06 19:57:25 [DEBUG] Epoch 5 - {'loss': '0.025885', 'mae': '0.124567', 'lr': '0.001000'} 
2023-06-06 19:57:34 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/06 19:57:34
2023-06-06 19:57:34 [INFO] Model configurations:
2023-06-06 19:57:34 [INFO] name: WF_T_21
2023-06-06 19:57:34 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'resnet'}, 'base_nn_amps': {'name': 'resnet'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-06 19:57:34 [INFO] loss_function: mse
2023-06-06 19:57:34 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-06 19:57:34 [INFO] metric: mae
2023-06-06 19:57:34 [INFO] shuffle: True
2023-06-06 19:57:34 [INFO] epochs: 200
2023-06-06 19:57:34 [INFO] batch_size: 512
2023-06-06 19:57:34 [INFO] verbose: 1
2023-06-06 19:57:34 [INFO] TensorBoard_log_path: logs
2023-06-06 19:57:34 [INFO] TensorBoard_hist_freq: 1
2023-06-06 19:57:34 [INFO] EarlyStopping_monitor: loss
2023-06-06 19:57:34 [INFO] EarlyStopping_patience: 15
2023-06-06 19:57:34 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-06 19:57:34 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-06 19:57:34 [INFO] ReduceLROnPlateau_patience: 3
2023-06-06 19:57:34 [INFO] Checkpoint_monitor: loss
2023-06-06 19:57:34 [INFO] save_best_only: True
2023-06-06 19:57:34 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-06 19:57:34 [INFO] log_path: logs/training.log
2023-06-06 19:57:34 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-06 19:57:35 [INFO] Training in progress
2023-06-06 19:57:37 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0137s vs `on_train_batch_end` time: 0.0246s). Check your callbacks.
2023-06-06 19:57:53 [DEBUG] Epoch 1 - {'loss': '0.064981', 'mae': '0.170286', 'lr': '0.001000'} 
2023-06-06 19:58:09 [DEBUG] Epoch 2 - {'loss': '0.028349', 'mae': '0.129833', 'lr': '0.001000'} 
2023-06-06 19:58:25 [DEBUG] Epoch 3 - {'loss': '0.026639', 'mae': '0.126045', 'lr': '0.001000'} 
2023-06-06 19:58:42 [DEBUG] Epoch 4 - {'loss': '0.025863', 'mae': '0.124309', 'lr': '0.001000'} 
2023-06-06 19:58:58 [DEBUG] Epoch 5 - {'loss': '0.025581', 'mae': '0.123674', 'lr': '0.001000'} 
2023-06-06 19:59:14 [DEBUG] Epoch 6 - {'loss': '0.025545', 'mae': '0.123649', 'lr': '0.001000'} 
2023-06-06 19:59:30 [DEBUG] Epoch 7 - {'loss': '0.025374', 'mae': '0.123209', 'lr': '0.001000'} 
2023-06-06 19:59:47 [DEBUG] Epoch 8 - {'loss': '0.024963', 'mae': '0.122167', 'lr': '0.001000'} 
2023-06-06 20:00:03 [DEBUG] Epoch 9 - {'loss': '0.024703', 'mae': '0.121542', 'lr': '0.001000'} 
2023-06-06 20:00:19 [DEBUG] Epoch 10 - {'loss': '0.024607', 'mae': '0.121295', 'lr': '0.001000'} 
2023-06-06 20:00:36 [DEBUG] Epoch 11 - {'loss': '0.024423', 'mae': '0.120864', 'lr': '0.001000'} 
2023-06-06 20:00:52 [DEBUG] Epoch 12 - {'loss': '0.024290', 'mae': '0.120558', 'lr': '0.001000'} 
2023-06-06 20:01:08 [DEBUG] Epoch 13 - {'loss': '0.024198', 'mae': '0.120350', 'lr': '0.001000'} 
2023-06-06 20:01:24 [DEBUG] Epoch 14 - {'loss': '0.024225', 'mae': '0.120421', 'lr': '0.001000'} 
2023-06-06 20:01:41 [DEBUG] Epoch 15 - {'loss': '0.024134', 'mae': '0.120210', 'lr': '0.001000'} 
2023-06-06 20:01:57 [DEBUG] Epoch 16 - {'loss': '0.024017', 'mae': '0.119930', 'lr': '0.001000'} 
2023-06-06 20:02:13 [DEBUG] Epoch 17 - {'loss': '0.023949', 'mae': '0.119756', 'lr': '0.001000'} 
2023-06-06 20:02:29 [DEBUG] Epoch 18 - {'loss': '0.023971', 'mae': '0.119830', 'lr': '0.001000'} 
2023-06-06 20:02:46 [DEBUG] Epoch 19 - {'loss': '0.023924', 'mae': '0.119712', 'lr': '0.001000'} 
2023-06-06 20:03:02 [DEBUG] Epoch 20 - {'loss': '0.023635', 'mae': '0.118988', 'lr': '0.000500'} 
2023-06-06 20:03:18 [DEBUG] Epoch 21 - {'loss': '0.023643', 'mae': '0.119013', 'lr': '0.000500'} 
2023-06-06 20:03:34 [DEBUG] Epoch 22 - {'loss': '0.023634', 'mae': '0.118982', 'lr': '0.000500'} 
2023-06-06 20:03:51 [DEBUG] Epoch 23 - {'loss': '0.023604', 'mae': '0.118921', 'lr': '0.000500'} 
2023-06-06 20:04:07 [DEBUG] Epoch 24 - {'loss': '0.023456', 'mae': '0.118523', 'lr': '0.000250'} 
2023-06-06 20:04:24 [DEBUG] Epoch 25 - {'loss': '0.023450', 'mae': '0.118499', 'lr': '0.000250'} 
2023-06-06 20:04:40 [DEBUG] Epoch 26 - {'loss': '0.023402', 'mae': '0.118398', 'lr': '0.000250'} 
2023-06-06 20:04:57 [DEBUG] Epoch 27 - {'loss': '0.023412', 'mae': '0.118407', 'lr': '0.000250'} 
2023-06-06 20:05:13 [DEBUG] Epoch 28 - {'loss': '0.023358', 'mae': '0.118266', 'lr': '0.000125'} 
2023-06-06 20:05:29 [DEBUG] Epoch 29 - {'loss': '0.023325', 'mae': '0.118187', 'lr': '0.000125'} 
2023-06-06 20:05:46 [DEBUG] Epoch 30 - {'loss': '0.023327', 'mae': '0.118177', 'lr': '0.000125'} 
2023-06-06 20:06:02 [DEBUG] Epoch 31 - {'loss': '0.023324', 'mae': '0.118180', 'lr': '0.000125'} 
2023-06-06 20:06:18 [DEBUG] Epoch 32 - {'loss': '0.023315', 'mae': '0.118160', 'lr': '0.000125'} 
2023-06-06 20:06:35 [DEBUG] Epoch 33 - {'loss': '0.023294', 'mae': '0.118104', 'lr': '0.000063'} 
2023-06-06 20:06:51 [DEBUG] Epoch 34 - {'loss': '0.023270', 'mae': '0.118036', 'lr': '0.000063'} 
2023-06-06 20:07:07 [DEBUG] Epoch 35 - {'loss': '0.023275', 'mae': '0.118054', 'lr': '0.000063'} 
2023-06-06 20:07:23 [DEBUG] Epoch 36 - {'loss': '0.023265', 'mae': '0.118017', 'lr': '0.000031'} 
2023-06-06 20:07:40 [DEBUG] Epoch 37 - {'loss': '0.023258', 'mae': '0.118004', 'lr': '0.000031'} 
2023-06-06 20:07:56 [DEBUG] Epoch 38 - {'loss': '0.023249', 'mae': '0.117985', 'lr': '0.000031'} 
2023-06-06 20:08:12 [DEBUG] Epoch 39 - {'loss': '0.023259', 'mae': '0.118012', 'lr': '0.000016'} 
2023-06-06 20:08:28 [DEBUG] Epoch 40 - {'loss': '0.023229', 'mae': '0.117928', 'lr': '0.000016'} 
2023-06-06 20:08:45 [DEBUG] Epoch 41 - {'loss': '0.023245', 'mae': '0.117971', 'lr': '0.000016'} 
2023-06-06 20:09:01 [DEBUG] Epoch 42 - {'loss': '0.023235', 'mae': '0.117942', 'lr': '0.000008'} 
2023-06-06 20:09:17 [DEBUG] Epoch 43 - {'loss': '0.023244', 'mae': '0.117979', 'lr': '0.000008'} 
2023-06-06 20:09:33 [DEBUG] Epoch 44 - {'loss': '0.023236', 'mae': '0.117944', 'lr': '0.000008'} 
2023-06-06 20:09:49 [DEBUG] Epoch 45 - {'loss': '0.023226', 'mae': '0.117921', 'lr': '0.000004'} 
2023-06-06 20:10:05 [DEBUG] Epoch 46 - {'loss': '0.023226', 'mae': '0.117920', 'lr': '0.000004'} 
2023-06-06 20:10:22 [DEBUG] Epoch 47 - {'loss': '0.023236', 'mae': '0.117939', 'lr': '0.000004'} 
2023-06-06 20:10:38 [DEBUG] Epoch 48 - {'loss': '0.023232', 'mae': '0.117954', 'lr': '0.000002'} 
2023-06-06 20:10:54 [DEBUG] Epoch 49 - {'loss': '0.023225', 'mae': '0.117925', 'lr': '0.000002'} 
2023-06-06 20:11:10 [DEBUG] Epoch 50 - {'loss': '0.023232', 'mae': '0.117945', 'lr': '0.000002'} 
2023-06-06 20:11:26 [DEBUG] Epoch 51 - {'loss': '0.023229', 'mae': '0.117930', 'lr': '0.000001'} 
2023-06-06 20:11:43 [DEBUG] Epoch 52 - {'loss': '0.023237', 'mae': '0.117944', 'lr': '0.000001'} 
2023-06-06 20:11:59 [DEBUG] Epoch 53 - {'loss': '0.023233', 'mae': '0.117930', 'lr': '0.000001'} 
2023-06-06 20:12:15 [DEBUG] Epoch 54 - {'loss': '0.023223', 'mae': '0.117926', 'lr': '0.000000'} 
2023-06-06 20:12:31 [DEBUG] Epoch 55 - {'loss': '0.023240', 'mae': '0.117942', 'lr': '0.000000'} 
2023-06-06 20:12:48 [DEBUG] Epoch 56 - {'loss': '0.023222', 'mae': '0.117918', 'lr': '0.000000'} 
2023-06-06 20:13:04 [DEBUG] Epoch 57 - {'loss': '0.023233', 'mae': '0.117930', 'lr': '0.000000'} 
2023-06-06 20:13:20 [DEBUG] Epoch 58 - {'loss': '0.023225', 'mae': '0.117919', 'lr': '0.000000'} 
2023-06-06 20:13:36 [DEBUG] Epoch 59 - {'loss': '0.023227', 'mae': '0.117921', 'lr': '0.000000'} 
2023-06-06 20:13:52 [DEBUG] Epoch 60 - {'loss': '0.023209', 'mae': '0.117866', 'lr': '0.000000'} 
2023-06-06 20:14:08 [DEBUG] Epoch 61 - {'loss': '0.023219', 'mae': '0.117904', 'lr': '0.000000'} 
2023-06-06 20:14:25 [DEBUG] Epoch 62 - {'loss': '0.023222', 'mae': '0.117905', 'lr': '0.000000'} 
2023-06-06 20:14:41 [DEBUG] Epoch 63 - {'loss': '0.023221', 'mae': '0.117907', 'lr': '0.000000'} 
2023-06-06 20:14:57 [DEBUG] Epoch 64 - {'loss': '0.023226', 'mae': '0.117920', 'lr': '0.000000'} 
2023-06-06 20:15:13 [DEBUG] Epoch 65 - {'loss': '0.023223', 'mae': '0.117911', 'lr': '0.000000'} 
2023-06-06 20:15:29 [DEBUG] Epoch 66 - {'loss': '0.023233', 'mae': '0.117938', 'lr': '0.000000'} 
2023-06-06 20:15:45 [DEBUG] Epoch 67 - {'loss': '0.023218', 'mae': '0.117900', 'lr': '0.000000'} 
2023-06-06 20:16:02 [DEBUG] Epoch 68 - {'loss': '0.023215', 'mae': '0.117882', 'lr': '0.000000'} 
2023-06-06 20:16:18 [DEBUG] Epoch 69 - {'loss': '0.023236', 'mae': '0.117942', 'lr': '0.000000'} 
2023-06-06 20:16:34 [DEBUG] Epoch 70 - {'loss': '0.023232', 'mae': '0.117929', 'lr': '0.000000'} 
2023-06-06 20:16:50 [DEBUG] Epoch 71 - {'loss': '0.023233', 'mae': '0.117943', 'lr': '0.000000'} 
2023-06-06 20:17:06 [DEBUG] Epoch 72 - {'loss': '0.023220', 'mae': '0.117906', 'lr': '0.000000'} 
2023-06-06 20:17:23 [DEBUG] Epoch 73 - {'loss': '0.023229', 'mae': '0.117921', 'lr': '0.000000'} 
2023-06-06 20:17:39 [DEBUG] Epoch 74 - {'loss': '0.023226', 'mae': '0.117915', 'lr': '0.000000'} 
2023-06-06 20:17:55 [DEBUG] Epoch 75 - {'loss': '0.023225', 'mae': '0.117906', 'lr': '0.000000'} 
2023-06-06 20:17:55 [INFO] Training finished, elapsed time: 1220.28 seconds
2023-06-06 20:17:55 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-06 21:15:45 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/06 21:15:45
2023-06-06 21:15:45 [INFO] Model configurations:
2023-06-06 21:15:45 [INFO] name: WF_T_21
2023-06-06 21:15:45 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-06 21:15:45 [INFO] loss_function: mse
2023-06-06 21:15:45 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-06 21:15:45 [INFO] metric: mae
2023-06-06 21:15:45 [INFO] shuffle: True
2023-06-06 21:15:45 [INFO] epochs: 200
2023-06-06 21:15:45 [INFO] batch_size: 512
2023-06-06 21:15:45 [INFO] verbose: 1
2023-06-06 21:15:45 [INFO] TensorBoard_log_path: logs
2023-06-06 21:15:45 [INFO] TensorBoard_hist_freq: 1
2023-06-06 21:15:45 [INFO] EarlyStopping_monitor: loss
2023-06-06 21:15:45 [INFO] EarlyStopping_patience: 15
2023-06-06 21:15:45 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-06 21:15:45 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-06 21:15:45 [INFO] ReduceLROnPlateau_patience: 3
2023-06-06 21:15:45 [INFO] Checkpoint_monitor: loss
2023-06-06 21:15:45 [INFO] save_best_only: True
2023-06-06 21:15:45 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-06 21:15:45 [INFO] log_path: logs/training.log
2023-06-06 21:15:45 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-06 21:15:45 [INFO] Training in progress
2023-06-06 21:15:46 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.
2023-06-06 21:15:49 [DEBUG] Epoch 1 - {'loss': '0.172793', 'mae': '0.290078', 'lr': '0.001000'} 
2023-06-06 21:15:52 [DEBUG] Epoch 2 - {'loss': '0.058999', 'mae': '0.181275', 'lr': '0.001000'} 
2023-06-06 21:15:54 [DEBUG] Epoch 3 - {'loss': '0.039307', 'mae': '0.150195', 'lr': '0.001000'} 
2023-06-06 21:15:57 [DEBUG] Epoch 4 - {'loss': '0.032159', 'mae': '0.137155', 'lr': '0.001000'} 
2023-06-06 21:16:00 [DEBUG] Epoch 5 - {'loss': '0.029579', 'mae': '0.131897', 'lr': '0.001000'} 
2023-06-06 21:16:02 [DEBUG] Epoch 6 - {'loss': '0.028169', 'mae': '0.128983', 'lr': '0.001000'} 
2023-06-06 21:16:05 [DEBUG] Epoch 7 - {'loss': '0.027309', 'mae': '0.127226', 'lr': '0.001000'} 
2023-06-06 21:16:08 [DEBUG] Epoch 8 - {'loss': '0.026781', 'mae': '0.126091', 'lr': '0.001000'} 
2023-06-06 21:16:10 [DEBUG] Epoch 9 - {'loss': '0.026419', 'mae': '0.125273', 'lr': '0.001000'} 
2023-06-06 21:16:13 [DEBUG] Epoch 10 - {'loss': '0.026199', 'mae': '0.124793', 'lr': '0.001000'} 
2023-06-06 21:16:16 [DEBUG] Epoch 11 - {'loss': '0.026012', 'mae': '0.124352', 'lr': '0.001000'} 
2023-06-06 21:16:19 [DEBUG] Epoch 12 - {'loss': '0.025842', 'mae': '0.123977', 'lr': '0.001000'} 
2023-06-06 21:16:21 [DEBUG] Epoch 13 - {'loss': '0.025609', 'mae': '0.123521', 'lr': '0.001000'} 
2023-06-06 21:16:24 [DEBUG] Epoch 14 - {'loss': '0.025439', 'mae': '0.123159', 'lr': '0.001000'} 
2023-06-06 21:16:27 [DEBUG] Epoch 15 - {'loss': '0.025309', 'mae': '0.122837', 'lr': '0.001000'} 
2023-06-06 21:16:29 [DEBUG] Epoch 16 - {'loss': '0.025205', 'mae': '0.122599', 'lr': '0.001000'} 
2023-06-06 21:16:32 [DEBUG] Epoch 17 - {'loss': '0.025119', 'mae': '0.122400', 'lr': '0.001000'} 
2023-06-06 21:16:35 [DEBUG] Epoch 18 - {'loss': '0.025088', 'mae': '0.122347', 'lr': '0.001000'} 
2023-06-06 21:16:37 [DEBUG] Epoch 19 - {'loss': '0.024992', 'mae': '0.122124', 'lr': '0.001000'} 
2023-06-06 21:16:40 [DEBUG] Epoch 20 - {'loss': '0.024938', 'mae': '0.122010', 'lr': '0.001000'} 
2023-06-06 21:16:43 [DEBUG] Epoch 21 - {'loss': '0.024857', 'mae': '0.121835', 'lr': '0.001000'} 
2023-06-06 21:16:45 [DEBUG] Epoch 22 - {'loss': '0.024815', 'mae': '0.121741', 'lr': '0.001000'} 
2023-06-06 21:16:48 [DEBUG] Epoch 23 - {'loss': '0.024756', 'mae': '0.121614', 'lr': '0.001000'} 
2023-06-06 21:16:51 [DEBUG] Epoch 24 - {'loss': '0.024711', 'mae': '0.121527', 'lr': '0.001000'} 
2023-06-06 21:16:53 [DEBUG] Epoch 25 - {'loss': '0.024667', 'mae': '0.121430', 'lr': '0.001000'} 
2023-06-06 21:16:56 [DEBUG] Epoch 26 - {'loss': '0.024630', 'mae': '0.121349', 'lr': '0.001000'} 
2023-06-06 21:16:59 [DEBUG] Epoch 27 - {'loss': '0.024589', 'mae': '0.121255', 'lr': '0.001000'} 
2023-06-06 21:17:02 [DEBUG] Epoch 28 - {'loss': '0.024543', 'mae': '0.121145', 'lr': '0.001000'} 
2023-06-06 21:17:04 [DEBUG] Epoch 29 - {'loss': '0.024547', 'mae': '0.121167', 'lr': '0.001000'} 
2023-06-06 21:17:07 [DEBUG] Epoch 30 - {'loss': '0.024510', 'mae': '0.121086', 'lr': '0.001000'} 
2023-06-06 21:17:10 [DEBUG] Epoch 31 - {'loss': '0.024423', 'mae': '0.120859', 'lr': '0.000500'} 
2023-06-06 21:17:12 [DEBUG] Epoch 32 - {'loss': '0.024420', 'mae': '0.120864', 'lr': '0.000500'} 
2023-06-06 21:17:15 [DEBUG] Epoch 33 - {'loss': '0.024400', 'mae': '0.120817', 'lr': '0.000500'} 
2023-06-06 21:17:18 [DEBUG] Epoch 34 - {'loss': '0.024377', 'mae': '0.120769', 'lr': '0.000500'} 
2023-06-06 21:17:20 [DEBUG] Epoch 35 - {'loss': '0.024353', 'mae': '0.120724', 'lr': '0.000250'} 
2023-06-06 21:17:23 [DEBUG] Epoch 36 - {'loss': '0.024353', 'mae': '0.120722', 'lr': '0.000250'} 
2023-06-06 21:17:26 [DEBUG] Epoch 37 - {'loss': '0.024341', 'mae': '0.120697', 'lr': '0.000250'} 
2023-06-06 21:17:28 [DEBUG] Epoch 38 - {'loss': '0.024323', 'mae': '0.120649', 'lr': '0.000125'} 
2023-06-06 21:17:31 [DEBUG] Epoch 39 - {'loss': '0.024321', 'mae': '0.120662', 'lr': '0.000125'} 
2023-06-06 21:17:34 [DEBUG] Epoch 40 - {'loss': '0.024312', 'mae': '0.120637', 'lr': '0.000125'} 
2023-06-06 21:17:36 [DEBUG] Epoch 41 - {'loss': '0.024314', 'mae': '0.120637', 'lr': '0.000125'} 
2023-06-06 21:17:39 [DEBUG] Epoch 42 - {'loss': '0.024303', 'mae': '0.120612', 'lr': '0.000125'} 
2023-06-06 21:17:42 [DEBUG] Epoch 43 - {'loss': '0.024301', 'mae': '0.120599', 'lr': '0.000063'} 
2023-06-06 21:17:44 [DEBUG] Epoch 44 - {'loss': '0.024297', 'mae': '0.120585', 'lr': '0.000063'} 
2023-06-06 21:17:47 [DEBUG] Epoch 45 - {'loss': '0.024291', 'mae': '0.120565', 'lr': '0.000063'} 
2023-06-06 21:17:50 [DEBUG] Epoch 46 - {'loss': '0.024281', 'mae': '0.120545', 'lr': '0.000031'} 
2023-06-06 21:17:53 [DEBUG] Epoch 47 - {'loss': '0.024285', 'mae': '0.120556', 'lr': '0.000031'} 
2023-06-06 21:17:55 [DEBUG] Epoch 48 - {'loss': '0.024280', 'mae': '0.120547', 'lr': '0.000031'} 
2023-06-06 21:17:58 [DEBUG] Epoch 49 - {'loss': '0.024280', 'mae': '0.120553', 'lr': '0.000016'} 
2023-06-06 21:18:01 [DEBUG] Epoch 50 - {'loss': '0.024276', 'mae': '0.120547', 'lr': '0.000016'} 
2023-06-06 21:18:03 [DEBUG] Epoch 51 - {'loss': '0.024276', 'mae': '0.120554', 'lr': '0.000016'} 
2023-06-06 21:18:06 [DEBUG] Epoch 52 - {'loss': '0.024280', 'mae': '0.120555', 'lr': '0.000008'} 
2023-06-06 21:18:09 [DEBUG] Epoch 53 - {'loss': '0.024283', 'mae': '0.120565', 'lr': '0.000008'} 
2023-06-06 21:18:11 [DEBUG] Epoch 54 - {'loss': '0.024281', 'mae': '0.120549', 'lr': '0.000008'} 
2023-06-06 21:18:14 [DEBUG] Epoch 55 - {'loss': '0.024279', 'mae': '0.120557', 'lr': '0.000004'} 
2023-06-06 21:18:17 [DEBUG] Epoch 56 - {'loss': '0.024268', 'mae': '0.120527', 'lr': '0.000004'} 
2023-06-06 21:18:19 [DEBUG] Epoch 57 - {'loss': '0.024277', 'mae': '0.120545', 'lr': '0.000004'} 
2023-06-06 21:18:22 [DEBUG] Epoch 58 - {'loss': '0.024276', 'mae': '0.120542', 'lr': '0.000002'} 
2023-06-06 21:18:25 [DEBUG] Epoch 59 - {'loss': '0.024269', 'mae': '0.120530', 'lr': '0.000002'} 
2023-06-06 21:18:27 [DEBUG] Epoch 60 - {'loss': '0.024273', 'mae': '0.120547', 'lr': '0.000002'} 
2023-06-06 21:18:30 [DEBUG] Epoch 61 - {'loss': '0.024272', 'mae': '0.120528', 'lr': '0.000001'} 
2023-06-06 21:18:33 [DEBUG] Epoch 62 - {'loss': '0.024274', 'mae': '0.120541', 'lr': '0.000001'} 
2023-06-06 21:18:35 [DEBUG] Epoch 63 - {'loss': '0.024274', 'mae': '0.120535', 'lr': '0.000001'} 
2023-06-06 21:18:38 [DEBUG] Epoch 64 - {'loss': '0.024271', 'mae': '0.120542', 'lr': '0.000000'} 
2023-06-06 21:18:41 [DEBUG] Epoch 65 - {'loss': '0.024279', 'mae': '0.120555', 'lr': '0.000000'} 
2023-06-06 21:18:43 [DEBUG] Epoch 66 - {'loss': '0.024273', 'mae': '0.120542', 'lr': '0.000000'} 
2023-06-06 21:18:46 [DEBUG] Epoch 67 - {'loss': '0.024269', 'mae': '0.120523', 'lr': '0.000000'} 
2023-06-06 21:18:49 [DEBUG] Epoch 68 - {'loss': '0.024277', 'mae': '0.120556', 'lr': '0.000000'} 
2023-06-06 21:18:51 [DEBUG] Epoch 69 - {'loss': '0.024268', 'mae': '0.120531', 'lr': '0.000000'} 
2023-06-06 21:18:54 [DEBUG] Epoch 70 - {'loss': '0.024274', 'mae': '0.120549', 'lr': '0.000000'} 
2023-06-06 21:18:57 [DEBUG] Epoch 71 - {'loss': '0.024274', 'mae': '0.120543', 'lr': '0.000000'} 
2023-06-06 21:18:59 [DEBUG] Epoch 72 - {'loss': '0.024263', 'mae': '0.120512', 'lr': '0.000000'} 
2023-06-06 21:19:02 [DEBUG] Epoch 73 - {'loss': '0.024268', 'mae': '0.120536', 'lr': '0.000000'} 
2023-06-06 21:19:05 [DEBUG] Epoch 74 - {'loss': '0.024278', 'mae': '0.120554', 'lr': '0.000000'} 
2023-06-06 21:19:07 [DEBUG] Epoch 75 - {'loss': '0.024277', 'mae': '0.120547', 'lr': '0.000000'} 
2023-06-06 21:19:10 [DEBUG] Epoch 76 - {'loss': '0.024269', 'mae': '0.120529', 'lr': '0.000000'} 
2023-06-06 21:19:13 [DEBUG] Epoch 77 - {'loss': '0.024281', 'mae': '0.120557', 'lr': '0.000000'} 
2023-06-06 21:19:15 [DEBUG] Epoch 78 - {'loss': '0.024272', 'mae': '0.120536', 'lr': '0.000000'} 
2023-06-06 21:19:18 [DEBUG] Epoch 79 - {'loss': '0.024273', 'mae': '0.120537', 'lr': '0.000000'} 
2023-06-06 21:19:20 [DEBUG] Epoch 80 - {'loss': '0.024273', 'mae': '0.120538', 'lr': '0.000000'} 
2023-06-06 21:19:23 [DEBUG] Epoch 81 - {'loss': '0.024272', 'mae': '0.120539', 'lr': '0.000000'} 
2023-06-06 21:19:26 [DEBUG] Epoch 82 - {'loss': '0.024276', 'mae': '0.120544', 'lr': '0.000000'} 
2023-06-06 21:19:28 [DEBUG] Epoch 83 - {'loss': '0.024273', 'mae': '0.120536', 'lr': '0.000000'} 
2023-06-06 21:19:31 [DEBUG] Epoch 84 - {'loss': '0.024274', 'mae': '0.120537', 'lr': '0.000000'} 
2023-06-06 21:19:34 [DEBUG] Epoch 85 - {'loss': '0.024269', 'mae': '0.120527', 'lr': '0.000000'} 
2023-06-06 21:19:36 [DEBUG] Epoch 86 - {'loss': '0.024273', 'mae': '0.120540', 'lr': '0.000000'} 
2023-06-06 21:19:39 [DEBUG] Epoch 87 - {'loss': '0.024276', 'mae': '0.120549', 'lr': '0.000000'} 
2023-06-06 21:19:39 [INFO] Training finished, elapsed time: 233.66 seconds
2023-06-06 21:19:39 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

2023-06-11 18:35:42 [INFO] Experiment for subject: WF_T_21 begins at 2023/06/11 18:35:42
2023-06-11 18:35:42 [INFO] Model configurations:
2023-06-11 18:35:42 [INFO] name: WF_T_21
2023-06-11 18:35:42 [INFO] model_structure: {'encoder': {'name': 'encoder model', 'input_shape': 24, 'latent_shape': 3, 'range_t2_my': [0.003, 0.015], 'range_t2_ie': [0.045, 0.07], 'range_t2_fr': [0.2, 0.3], 'base_nn_t2s': {'name': 'mlp'}, 'base_nn_amps': {'name': 'mlp'}, 'base_mlp_t2': {'hidden_layers': [256, 128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_mlp_amps': {'hidden_layers': [256, 256], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_t2': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 1, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}, 'base_resnet_amps': {'hidden_layers_head': [128], 'num_res_blocks': 2, 'res_block_size': [256, 256], 'hidden_layers_tail': [128], 'num_classes': 3, 'activation': 'sigmoid', 'activation_last_layer': 'sigmoid'}}, 'decoder': {'name': 'decoder_exp', 'num_classes': 3, 'nte': 24, 'delta_te': 0.002, 'snr_range': [50, 300]}}
2023-06-11 18:35:42 [INFO] loss_function: mse
2023-06-11 18:35:42 [INFO] optimizer: {'name': 'adamax', 'lr': 0.001, 'clipnorm': None, 'clipvalue': None}
2023-06-11 18:35:42 [INFO] metric: mae
2023-06-11 18:35:42 [INFO] shuffle: True
2023-06-11 18:35:42 [INFO] epochs: 200
2023-06-11 18:35:42 [INFO] batch_size: 512
2023-06-11 18:35:42 [INFO] verbose: 1
2023-06-11 18:35:42 [INFO] TensorBoard_log_path: logs
2023-06-11 18:35:42 [INFO] TensorBoard_hist_freq: 1
2023-06-11 18:35:42 [INFO] EarlyStopping_monitor: loss
2023-06-11 18:35:42 [INFO] EarlyStopping_patience: 15
2023-06-11 18:35:42 [INFO] ReduceLROnPlateau_monitor: loss
2023-06-11 18:35:42 [INFO] ReduceLROnPlateau_factor: 0.5
2023-06-11 18:35:42 [INFO] ReduceLROnPlateau_patience: 3
2023-06-11 18:35:42 [INFO] Checkpoint_monitor: loss
2023-06-11 18:35:42 [INFO] save_best_only: True
2023-06-11 18:35:42 [INFO] io: {'subject_id': 'WF_T_21', 'data_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mgre_mag_mi_phs_corrected.nii', 'mask_path': '/export01/data/Hanwen/data/mgre_data/WT_F_21/niftis/mgre/mask.nii', 'save_path': 'results/sled/WT_F_21_mgre_', 'save_model_path': 'models/best_epoch_WT_F_21.h5'}
2023-06-11 18:35:42 [INFO] log_path: logs/training.log
2023-06-11 18:35:42 [INFO] save_model_path: models/best_epoch_WT_F_21.h5
2023-06-11 18:35:43 [INFO] Training in progress
2023-06-11 18:35:43 [WARNING] Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
2023-06-11 18:35:46 [DEBUG] Epoch 1 - {'loss': '0.090974', 'mae': '0.213384', 'lr': '0.001000'} 
2023-06-11 18:35:49 [DEBUG] Epoch 2 - {'loss': '0.040574', 'mae': '0.151177', 'lr': '0.001000'} 
2023-06-11 18:35:51 [DEBUG] Epoch 3 - {'loss': '0.032611', 'mae': '0.137770', 'lr': '0.001000'} 
2023-06-11 18:35:54 [DEBUG] Epoch 4 - {'loss': '0.029712', 'mae': '0.132221', 'lr': '0.001000'} 
2023-06-11 18:35:57 [DEBUG] Epoch 5 - {'loss': '0.028323', 'mae': '0.129424', 'lr': '0.001000'} 
2023-06-11 18:36:00 [DEBUG] Epoch 6 - {'loss': '0.027550', 'mae': '0.127885', 'lr': '0.001000'} 
2023-06-11 18:36:02 [DEBUG] Epoch 7 - {'loss': '0.027108', 'mae': '0.126950', 'lr': '0.001000'} 
2023-06-11 18:36:05 [DEBUG] Epoch 8 - {'loss': '0.026768', 'mae': '0.126228', 'lr': '0.001000'} 
2023-06-11 18:36:08 [DEBUG] Epoch 9 - {'loss': '0.026453', 'mae': '0.125511', 'lr': '0.001000'} 
2023-06-11 18:36:10 [DEBUG] Epoch 10 - {'loss': '0.026269', 'mae': '0.125100', 'lr': '0.001000'} 
2023-06-11 18:36:13 [DEBUG] Epoch 11 - {'loss': '0.026037', 'mae': '0.124564', 'lr': '0.001000'} 
2023-06-11 18:36:16 [DEBUG] Epoch 12 - {'loss': '0.025880', 'mae': '0.124206', 'lr': '0.001000'} 
2023-06-11 18:36:18 [DEBUG] Epoch 13 - {'loss': '0.025715', 'mae': '0.123821', 'lr': '0.001000'} 
2023-06-11 18:36:21 [DEBUG] Epoch 14 - {'loss': '0.025590', 'mae': '0.123525', 'lr': '0.001000'} 
2023-06-11 18:36:24 [DEBUG] Epoch 15 - {'loss': '0.025498', 'mae': '0.123328', 'lr': '0.001000'} 
2023-06-11 18:36:26 [DEBUG] Epoch 16 - {'loss': '0.025378', 'mae': '0.123031', 'lr': '0.001000'} 
2023-06-11 18:36:29 [DEBUG] Epoch 17 - {'loss': '0.025314', 'mae': '0.122903', 'lr': '0.001000'} 
2023-06-11 18:36:32 [DEBUG] Epoch 18 - {'loss': '0.025259', 'mae': '0.122783', 'lr': '0.001000'} 
2023-06-11 18:36:35 [DEBUG] Epoch 19 - {'loss': '0.025198', 'mae': '0.122648', 'lr': '0.001000'} 
2023-06-11 18:36:37 [DEBUG] Epoch 20 - {'loss': '0.025120', 'mae': '0.122453', 'lr': '0.001000'} 
2023-06-11 18:36:40 [DEBUG] Epoch 21 - {'loss': '0.025066', 'mae': '0.122347', 'lr': '0.001000'} 
2023-06-11 18:36:43 [DEBUG] Epoch 22 - {'loss': '0.025003', 'mae': '0.122210', 'lr': '0.001000'} 
2023-06-11 18:36:45 [DEBUG] Epoch 23 - {'loss': '0.024893', 'mae': '0.121917', 'lr': '0.001000'} 
2023-06-11 18:36:48 [DEBUG] Epoch 24 - {'loss': '0.024907', 'mae': '0.121987', 'lr': '0.001000'} 
2023-06-11 18:36:51 [DEBUG] Epoch 25 - {'loss': '0.024855', 'mae': '0.121858', 'lr': '0.001000'} 
2023-06-11 18:36:53 [DEBUG] Epoch 26 - {'loss': '0.024804', 'mae': '0.121738', 'lr': '0.001000'} 
2023-06-11 18:36:56 [DEBUG] Epoch 27 - {'loss': '0.024705', 'mae': '0.121480', 'lr': '0.000500'} 
2023-06-11 18:36:59 [DEBUG] Epoch 28 - {'loss': '0.024689', 'mae': '0.121452', 'lr': '0.000500'} 
2023-06-11 18:37:02 [DEBUG] Epoch 29 - {'loss': '0.024676', 'mae': '0.121436', 'lr': '0.000500'} 
2023-06-11 18:37:04 [DEBUG] Epoch 30 - {'loss': '0.024631', 'mae': '0.121326', 'lr': '0.000500'} 
2023-06-11 18:37:07 [DEBUG] Epoch 31 - {'loss': '0.024605', 'mae': '0.121264', 'lr': '0.000250'} 
2023-06-11 18:37:10 [DEBUG] Epoch 32 - {'loss': '0.024582', 'mae': '0.121205', 'lr': '0.000250'} 
2023-06-11 18:37:12 [DEBUG] Epoch 33 - {'loss': '0.024569', 'mae': '0.121179', 'lr': '0.000250'} 
2023-06-11 18:37:15 [DEBUG] Epoch 34 - {'loss': '0.024551', 'mae': '0.121131', 'lr': '0.000250'} 
2023-06-11 18:37:18 [DEBUG] Epoch 35 - {'loss': '0.024532', 'mae': '0.121106', 'lr': '0.000125'} 
2023-06-11 18:37:20 [DEBUG] Epoch 36 - {'loss': '0.024523', 'mae': '0.121098', 'lr': '0.000125'} 
2023-06-11 18:37:23 [DEBUG] Epoch 37 - {'loss': '0.024509', 'mae': '0.121047', 'lr': '0.000125'} 
2023-06-11 18:37:26 [DEBUG] Epoch 38 - {'loss': '0.024497', 'mae': '0.121025', 'lr': '0.000063'} 
2023-06-11 18:37:28 [DEBUG] Epoch 39 - {'loss': '0.024489', 'mae': '0.120997', 'lr': '0.000063'} 
2023-06-11 18:37:31 [DEBUG] Epoch 40 - {'loss': '0.024480', 'mae': '0.120981', 'lr': '0.000063'} 
2023-06-11 18:37:34 [DEBUG] Epoch 41 - {'loss': '0.024470', 'mae': '0.120965', 'lr': '0.000063'} 
2023-06-11 18:37:36 [DEBUG] Epoch 42 - {'loss': '0.024477', 'mae': '0.120980', 'lr': '0.000031'} 
2023-06-11 18:37:39 [DEBUG] Epoch 43 - {'loss': '0.024477', 'mae': '0.120976', 'lr': '0.000031'} 
2023-06-11 18:37:42 [DEBUG] Epoch 44 - {'loss': '0.024465', 'mae': '0.120957', 'lr': '0.000031'} 
2023-06-11 18:37:45 [DEBUG] Epoch 45 - {'loss': '0.024462', 'mae': '0.120945', 'lr': '0.000016'} 
2023-06-11 18:37:47 [DEBUG] Epoch 46 - {'loss': '0.024463', 'mae': '0.120945', 'lr': '0.000016'} 
2023-06-11 18:37:50 [DEBUG] Epoch 47 - {'loss': '0.024451', 'mae': '0.120929', 'lr': '0.000016'} 
2023-06-11 18:37:53 [DEBUG] Epoch 48 - {'loss': '0.024459', 'mae': '0.120949', 'lr': '0.000008'} 
2023-06-11 18:37:55 [DEBUG] Epoch 49 - {'loss': '0.024449', 'mae': '0.120922', 'lr': '0.000008'} 
2023-06-11 18:37:58 [DEBUG] Epoch 50 - {'loss': '0.024454', 'mae': '0.120934', 'lr': '0.000008'} 
2023-06-11 18:38:01 [DEBUG] Epoch 51 - {'loss': '0.024456', 'mae': '0.120929', 'lr': '0.000004'} 
2023-06-11 18:38:03 [DEBUG] Epoch 52 - {'loss': '0.024450', 'mae': '0.120924', 'lr': '0.000004'} 
2023-06-11 18:38:06 [DEBUG] Epoch 53 - {'loss': '0.024449', 'mae': '0.120913', 'lr': '0.000004'} 
2023-06-11 18:38:09 [DEBUG] Epoch 54 - {'loss': '0.024452', 'mae': '0.120923', 'lr': '0.000002'} 
2023-06-11 18:38:11 [DEBUG] Epoch 55 - {'loss': '0.024455', 'mae': '0.120936', 'lr': '0.000002'} 
2023-06-11 18:38:14 [DEBUG] Epoch 56 - {'loss': '0.024449', 'mae': '0.120920', 'lr': '0.000002'} 
2023-06-11 18:38:17 [DEBUG] Epoch 57 - {'loss': '0.024461', 'mae': '0.120947', 'lr': '0.000001'} 
2023-06-11 18:38:19 [DEBUG] Epoch 58 - {'loss': '0.024446', 'mae': '0.120908', 'lr': '0.000001'} 
2023-06-11 18:38:22 [DEBUG] Epoch 59 - {'loss': '0.024447', 'mae': '0.120901', 'lr': '0.000001'} 
2023-06-11 18:38:25 [DEBUG] Epoch 60 - {'loss': '0.024447', 'mae': '0.120910', 'lr': '0.000000'} 
2023-06-11 18:38:27 [DEBUG] Epoch 61 - {'loss': '0.024448', 'mae': '0.120906', 'lr': '0.000000'} 
2023-06-11 18:38:30 [DEBUG] Epoch 62 - {'loss': '0.024450', 'mae': '0.120917', 'lr': '0.000000'} 
2023-06-11 18:38:33 [DEBUG] Epoch 63 - {'loss': '0.024444', 'mae': '0.120908', 'lr': '0.000000'} 
2023-06-11 18:38:35 [DEBUG] Epoch 64 - {'loss': '0.024448', 'mae': '0.120907', 'lr': '0.000000'} 
2023-06-11 18:38:38 [DEBUG] Epoch 65 - {'loss': '0.024450', 'mae': '0.120910', 'lr': '0.000000'} 
2023-06-11 18:38:41 [DEBUG] Epoch 66 - {'loss': '0.024448', 'mae': '0.120918', 'lr': '0.000000'} 
2023-06-11 18:38:44 [DEBUG] Epoch 67 - {'loss': '0.024442', 'mae': '0.120897', 'lr': '0.000000'} 
2023-06-11 18:38:46 [DEBUG] Epoch 68 - {'loss': '0.024446', 'mae': '0.120904', 'lr': '0.000000'} 
2023-06-11 18:38:49 [DEBUG] Epoch 69 - {'loss': '0.024444', 'mae': '0.120899', 'lr': '0.000000'} 
2023-06-11 18:38:52 [DEBUG] Epoch 70 - {'loss': '0.024451', 'mae': '0.120919', 'lr': '0.000000'} 
2023-06-11 18:38:54 [DEBUG] Epoch 71 - {'loss': '0.024448', 'mae': '0.120908', 'lr': '0.000000'} 
2023-06-11 18:38:57 [DEBUG] Epoch 72 - {'loss': '0.024445', 'mae': '0.120908', 'lr': '0.000000'} 
2023-06-11 18:39:00 [DEBUG] Epoch 73 - {'loss': '0.024450', 'mae': '0.120923', 'lr': '0.000000'} 
2023-06-11 18:39:02 [DEBUG] Epoch 74 - {'loss': '0.024443', 'mae': '0.120907', 'lr': '0.000000'} 
2023-06-11 18:39:05 [DEBUG] Epoch 75 - {'loss': '0.024450', 'mae': '0.120924', 'lr': '0.000000'} 
2023-06-11 18:39:08 [DEBUG] Epoch 76 - {'loss': '0.024441', 'mae': '0.120906', 'lr': '0.000000'} 
2023-06-11 18:39:10 [DEBUG] Epoch 77 - {'loss': '0.024452', 'mae': '0.120916', 'lr': '0.000000'} 
2023-06-11 18:39:13 [DEBUG] Epoch 78 - {'loss': '0.024454', 'mae': '0.120923', 'lr': '0.000000'} 
2023-06-11 18:39:16 [DEBUG] Epoch 79 - {'loss': '0.024449', 'mae': '0.120914', 'lr': '0.000000'} 
2023-06-11 18:39:18 [DEBUG] Epoch 80 - {'loss': '0.024450', 'mae': '0.120920', 'lr': '0.000000'} 
2023-06-11 18:39:21 [DEBUG] Epoch 81 - {'loss': '0.024451', 'mae': '0.120931', 'lr': '0.000000'} 
2023-06-11 18:39:24 [DEBUG] Epoch 82 - {'loss': '0.024450', 'mae': '0.120918', 'lr': '0.000000'} 
2023-06-11 18:39:26 [DEBUG] Epoch 83 - {'loss': '0.024457', 'mae': '0.120940', 'lr': '0.000000'} 
2023-06-11 18:39:29 [DEBUG] Epoch 84 - {'loss': '0.024446', 'mae': '0.120910', 'lr': '0.000000'} 
2023-06-11 18:39:32 [DEBUG] Epoch 85 - {'loss': '0.024449', 'mae': '0.120915', 'lr': '0.000000'} 
2023-06-11 18:39:34 [DEBUG] Epoch 86 - {'loss': '0.024445', 'mae': '0.120899', 'lr': '0.000000'} 
2023-06-11 18:39:37 [DEBUG] Epoch 87 - {'loss': '0.024443', 'mae': '0.120902', 'lr': '0.000000'} 
2023-06-11 18:39:40 [DEBUG] Epoch 88 - {'loss': '0.024453', 'mae': '0.120933', 'lr': '0.000000'} 
2023-06-11 18:39:42 [DEBUG] Epoch 89 - {'loss': '0.024446', 'mae': '0.120902', 'lr': '0.000000'} 
2023-06-11 18:39:45 [DEBUG] Epoch 90 - {'loss': '0.024448', 'mae': '0.120906', 'lr': '0.000000'} 
2023-06-11 18:39:48 [DEBUG] Epoch 91 - {'loss': '0.024443', 'mae': '0.120893', 'lr': '0.000000'} 
2023-06-11 18:39:48 [INFO] Training finished, elapsed time: 245.05 seconds
2023-06-11 18:39:48 [INFO] model is saved in 'models/best_epoch_WT_F_21.h5'

